{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/Users/susieguo/Desktop/parquet_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca29f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktok__order_items = pd.read_parquet(f\"{BASE_PATH}stg_tiktok_shop__order_items.parquet\")\n",
    "tiktok__order_item_metrics = pd.read_parquet(f\"{BASE_PATH}stg_tiktok_shop__order_metrics.parquet\")\n",
    "shopify__order_items = pd.read_parquet(f\"{BASE_PATH}stg_shopify__order_items.parquet\")\n",
    "shopify__order_item_metrics = pd.read_parquet(f\"{BASE_PATH}stg_shopify__order_metrics.parquet\")\n",
    "amazon_order_item_metrics = pd.read_parquet(f\"{BASE_PATH}stg_amazon__order_item_metrics.parquet\")\n",
    "amazon_order_metrics = pd.read_parquet(f\"{BASE_PATH}stg_amazon__order_metrics.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktok_merged_df = pd.merge(\n",
    "    tiktok__order_item_metrics,\n",
    "    tiktok__order_items[[\"order_id\", \"local_order_ts\",\"product_name\"]],\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Convert timestamp to datetime and create a date column\n",
    "tiktok_merged_df[\"local_order_ts_x\"] = pd.to_datetime(tiktok_merged_df[\"local_order_ts_x\"])\n",
    "tiktok_merged_df[\"order_date\"] = tiktok_merged_df[\"local_order_ts_x\"].dt.date\n",
    "\n",
    "# Sort by customer and time\n",
    "tiktok_merged_df = tiktok_merged_df.sort_values(by=[\"customer_id\", \"local_order_ts_x\"])\n",
    "\n",
    "# Determine first order date per customer\n",
    "tiktok_merged_df[\"is_new_customer\"] = tiktok_merged_df[\"customer_id\"].duplicated()\n",
    "\n",
    "# Group to get per-date-per-sku stats\n",
    "tiktok_daily_sku_metrics = tiktok_merged_df.groupby([\"order_date\", \"product_name\"]).agg(\n",
    "    total_orders=(\"order_id\", \"count\"),\n",
    "    num_customers=(\"customer_id\", \"nunique\"),\n",
    "    num_new_customers=(\"is_new_customer\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Optionally: calculate % new customers\n",
    "tiktok_daily_sku_metrics[\"pct_new_customers\"] = (\n",
    "    tiktok_daily_sku_metrics[\"num_new_customers\"] / tiktok_daily_sku_metrics[\"num_customers\"]\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopify_merged_df = pd.merge(\n",
    "    shopify__order_item_metrics,\n",
    "    shopify__order_items[[\"order_id\", \"local_order_ts\",\"product_name\"]],\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Convert timestamp to datetime and create a date column\n",
    "shopify_merged_df[\"local_order_ts_x\"] = pd.to_datetime(shopify_merged_df[\"local_order_ts_x\"])\n",
    "shopify_merged_df[\"order_date\"] = shopify_merged_df[\"local_order_ts_x\"].dt.date\n",
    "\n",
    "# Sort by customer and time\n",
    "shopify_merged_df = shopify_merged_df.sort_values(by=[\"customer_id\", \"local_order_ts_x\"])\n",
    "\n",
    "# Determine first order date per customer\n",
    "shopify_merged_df[\"is_new_customer\"] = shopify_merged_df[\"customer_id\"].duplicated()\n",
    "\n",
    "# Group to get per-date-per-sku stats\n",
    "shopify_daily_sku_metrics = shopify_merged_df.groupby([\"order_date\", \"product_name\"]).agg(\n",
    "    total_orders=(\"order_id\", \"count\"),\n",
    "    num_customers=(\"customer_id\", \"nunique\"),\n",
    "    num_new_customers=(\"is_new_customer\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Optionally: calculate % new customers\n",
    "shopify_daily_sku_metrics[\"pct_new_customers\"] = (\n",
    "    shopify_daily_sku_metrics[\"num_new_customers\"] / shopify_daily_sku_metrics[\"num_customers\"]\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7e321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_date</th>\n",
       "      <th>product_name</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>num_customers</th>\n",
       "      <th>num_new_customers</th>\n",
       "      <th>pct_new_customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>Javy Coffee Cold Brew Coffee Concentrate, Perf...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>Javy Coffee Concentrate - Cold Brew Coffee, Pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>Javy Coffee Concentrate - Cold Brew Coffee, Pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>Javy Coffee Cold Brew Coffee Concentrate, Perf...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>Javy Coffee Concentrate - Cold Brew Coffee, Pe...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24685</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Javvy French Vanilla Protein Iced Coffee - Pre...</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>27</td>\n",
       "      <td>0.350649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24686</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Javvy Hazelnut Protein Iced Coffee - Premium W...</td>\n",
       "      <td>230</td>\n",
       "      <td>62</td>\n",
       "      <td>186</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24687</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Javvy Mocha Protein Coffee - Premium Whey Prot...</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24688</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Mocha Protein Iced Coffee - Premium Whey Prote...</td>\n",
       "      <td>599</td>\n",
       "      <td>173</td>\n",
       "      <td>478</td>\n",
       "      <td>2.763006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24689</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Original Protein Iced Coffee - Premium Whey Pr...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24690 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_date                                       product_name  \\\n",
       "0      2022-04-30  Javy Coffee Cold Brew Coffee Concentrate, Perf...   \n",
       "1      2022-04-30  Javy Coffee Concentrate - Cold Brew Coffee, Pe...   \n",
       "2      2022-04-30  Javy Coffee Concentrate - Cold Brew Coffee, Pe...   \n",
       "3      2022-05-01  Javy Coffee Cold Brew Coffee Concentrate, Perf...   \n",
       "4      2022-05-01  Javy Coffee Concentrate - Cold Brew Coffee, Pe...   \n",
       "...           ...                                                ...   \n",
       "24685  2025-06-15  Javvy French Vanilla Protein Iced Coffee - Pre...   \n",
       "24686  2025-06-15  Javvy Hazelnut Protein Iced Coffee - Premium W...   \n",
       "24687  2025-06-15  Javvy Mocha Protein Coffee - Premium Whey Prot...   \n",
       "24688  2025-06-15  Mocha Protein Iced Coffee - Premium Whey Prote...   \n",
       "24689  2025-06-15  Original Protein Iced Coffee - Premium Whey Pr...   \n",
       "\n",
       "       total_orders  num_customers  num_new_customers  pct_new_customers  \n",
       "0                 3              3                  0           0.000000  \n",
       "1                 1              1                  0           0.000000  \n",
       "2                 1              0                  0           0.000000  \n",
       "3                 5              4                  1           0.250000  \n",
       "4                 3              3                  0           0.000000  \n",
       "...             ...            ...                ...                ...  \n",
       "24685            79             77                 27           0.350649  \n",
       "24686           230             62                186           3.000000  \n",
       "24687            15             13                  2           0.153846  \n",
       "24688           599            173                478           2.763006  \n",
       "24689            14              0                 14                inf  \n",
       "\n",
       "[24690 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_merged_df = pd.merge(\n",
    "    amazon_order_item_metrics,\n",
    "    amazon_order_metrics[[\"order_id\", \"customer_id\", \"local_order_ts\"]],\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Convert timestamp to datetime and create a date column\n",
    "amazon_merged_df[\"local_order_ts_x\"] = pd.to_datetime(amazon_merged_df[\"local_order_ts_x\"])\n",
    "amazon_merged_df[\"order_date\"] = amazon_merged_df[\"local_order_ts_x\"].dt.date\n",
    "\n",
    "# Sort by customer and time\n",
    "amazon_merged_df = amazon_merged_df.sort_values(by=[\"customer_id\", \"local_order_ts_x\"])\n",
    "\n",
    "# Determine first order date per customer\n",
    "amazon_merged_df[\"is_new_customer\"] = amazon_merged_df[\"customer_id\"].duplicated()\n",
    "\n",
    "# Group to get per-date-per-sku stats\n",
    "amazon_daily_sku_metrics = amazon_merged_df.groupby([\"order_date\", \"product_name\"]).agg(\n",
    "    total_orders=(\"order_id\", \"count\"),\n",
    "    num_customers=(\"customer_id\", \"nunique\"),\n",
    "    num_new_customers=(\"is_new_customer\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Optionally: calculate % new customers\n",
    "amazon_daily_sku_metrics[\"pct_new_customers\"] = (\n",
    "    amazon_daily_sku_metrics[\"num_new_customers\"] / amazon_daily_sku_metrics[\"num_customers\"]\n",
    ").fillna(0)\n",
    "\n",
    "amazon_daily_sku_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a47185",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_daily_spend = pd.read_parquet(f\"{BASE_PATH}final_unified__ad_spend_daily.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FastAggregateBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Combine all data at once\n",
    "        self.daily_data = pd.concat([\n",
    "            tiktok_daily_sku_metrics.assign(channel='tiktok'),\n",
    "            amazon_daily_sku_metrics.assign(channel='amazon'),\n",
    "            shopify_daily_sku_metrics.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        self.order_items = pd.concat([\n",
    "            tiktok__order_items.assign(channel='tiktok'),\n",
    "            amazon_order_item_metrics.assign(channel='amazon'),\n",
    "            shopify__order_items.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Clean dates once\n",
    "        self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "        self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "        self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "        self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "        \n",
    "        \n",
    "    def calculate_aggregate_falloff_rates(self):\n",
    "       \n",
    "        # Aggregate by month across ALL SKUs\n",
    "        monthly_totals = self.daily_data.groupby(self.daily_data['order_date'].dt.to_period('M')).agg({\n",
    "            'num_customers': 'sum',\n",
    "            'num_new_customers': 'sum',\n",
    "            'total_orders': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        monthly_totals['existing_customers'] = monthly_totals['num_customers'] - monthly_totals['num_new_customers']\n",
    "        monthly_totals = monthly_totals.sort_values('order_date')\n",
    "                \n",
    "    \n",
    "        falloff_rates = {}\n",
    "        \n",
    "        # Standard e-commerce retention patterns (based on industry data)\n",
    "        base_monthly_retention = 0.75 \n",
    "        \n",
    "        for age_months in range(1, 13):\n",
    "            monthly_retention = base_monthly_retention ** age_months\n",
    "            monthly_falloff = 1 - monthly_retention\n",
    "            \n",
    "            falloff_rates[f'falloff_rate_{age_months}m'] = monthly_falloff\n",
    "            falloff_rates[f'returning_rate_{age_months}m'] = monthly_retention\n",
    "        \n",
    "        return falloff_rates\n",
    "    \n",
    "    def calculate_fast_benchmark(self):\n",
    "                \n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Calculate existing customers (vectorized)\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        \n",
    "        # Calculate daily AOV (aggregate across all SKUs per day)\n",
    "        daily_aov = self.order_items.groupby('order_date').agg({\n",
    "            'sku_gross_sales': 'sum',\n",
    "            'quantity': 'sum'\n",
    "        }).reset_index()\n",
    "        daily_aov['daily_aov'] = daily_aov['sku_gross_sales'] / np.maximum(daily_aov['quantity'], 1)\n",
    "        \n",
    "        # Merge AOV data\n",
    "        benchmark_df = benchmark_df.merge(daily_aov[['order_date', 'daily_aov']], \n",
    "                                        on='order_date', how='left')\n",
    "        benchmark_df['aov_used'] = benchmark_df['daily_aov'].fillna(self.aov_avg)\n",
    "        \n",
    "        # Calculate benchmark demand (vectorized)\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['existing_customer_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['bm_demand'] = benchmark_df['new_customer_demand'] + benchmark_df['existing_customer_demand']\n",
    "        \n",
    "        # Calculate actual demand (vectorized)\n",
    "        actual_revenue = self.order_items.groupby(['product_name', 'order_date']).agg({\n",
    "            'sku_gross_sales': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                        left_on=['product_name', 'order_date'],\n",
    "                                        right_on=['product_name', 'order_date'], \n",
    "                                        how='left')\n",
    "        \n",
    "        # Fill missing actual demand with estimate\n",
    "        benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(\n",
    "            benchmark_df['num_customers'] * benchmark_df['aov_used'])\n",
    "        \n",
    "        # Calculate error metrics (vectorized)\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add fall-off rates (same for all rows - very fast)\n",
    "        falloff_rates = self.calculate_aggregate_falloff_rates()\n",
    "        for rate_name, rate_value in falloff_rates.items():\n",
    "            benchmark_df[rate_name] = rate_value\n",
    "        \n",
    "        # Clean up columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'num_customers',\n",
    "            'aov_used', 'new_customer_demand', 'existing_customer_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + list(falloff_rates.keys())\n",
    "        \n",
    "        benchmark_df = benchmark_df[final_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku', 'num_customers': 'total_customers'}, inplace=True)\n",
    "        \n",
    "        # Summary stats\n",
    "        print(f\"\\nðŸ“Š FAST BENCHMARK RESULTS:\")\n",
    "        print(f\"  âš¡ Processed {len(benchmark_df):,} records in seconds!\")\n",
    "        print(f\"  ðŸ“Š SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  ðŸ“… Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        print(f\"  ðŸ’° Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  ðŸ’° Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  ðŸ“ˆ Avg Error: ${benchmark_df['error_metric'].mean():.2f}\")\n",
    "        print(f\"  ðŸ“Š MAPE: {benchmark_df['error_percentage'].abs().mean():.1f}%\")\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_fast_benchmark(self):\n",
    "    \n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        # Calculate benchmark (vectorized - very fast)\n",
    "        benchmark_df = self.calculate_fast_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\nâš¡ COMPLETED IN {duration:.1f} SECONDS!\")\n",
    "        print(f\"ðŸŽ‰ Generated {len(benchmark_df):,} benchmark predictions\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_fast_benchmark_model(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                           shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                           tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "    \n",
    "    # Initialize fast benchmark\n",
    "    benchmark = FastAggregateBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    # Run fast benchmark\n",
    "    benchmark_df = benchmark.run_fast_benchmark()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š FAST BENCHMARK DATAFRAME:\")\n",
    "    print(f\"   Columns: {list(benchmark_df.columns)}\")\n",
    "    print(f\"   Key metrics:\")\n",
    "    print(f\"     - bm_demand: Benchmark demand\")\n",
    "    print(f\"     - actual_demand: Actual revenue\") \n",
    "    print(f\"     - error_metric: Actual - BM_Demand\")\n",
    "    print(f\"     - falloff_rate_1m to falloff_rate_12m: Universal rates\")\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "def super_simple_benchmark(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                          shopify_daily_sku_metrics, aov_avg=43):\n",
    "   \n",
    "    # Combine data\n",
    "    df = pd.concat([\n",
    "        tiktok_daily_sku_metrics.assign(channel='tiktok'),\n",
    "        amazon_daily_sku_metrics.assign(channel='amazon'),\n",
    "        shopify_daily_sku_metrics.assign(channel='shopify')\n",
    "    ])\n",
    "    \n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    \n",
    "    # Simple calculations (all vectorized)\n",
    "    df['new_customers'] = df['num_new_customers'].fillna(0)\n",
    "    df['existing_customers'] = np.maximum(0, df['num_customers'] - df['new_customers'])\n",
    "    df['aov_used'] = aov_avg\n",
    "    \n",
    "    # BM_Demand = (new + existing) * AOV\n",
    "    df['bm_demand'] = (df['new_customers'] + df['existing_customers']) * aov_avg\n",
    "    df['actual_demand'] = df['num_customers'] * aov_avg  # Simple proxy\n",
    "    df['error_metric'] = df['actual_demand'] - df['bm_demand']\n",
    "    df['error_percentage'] = (df['error_metric'] / np.maximum(df['actual_demand'], 1)) * 100\n",
    "    \n",
    "    # Add standard fall-off rates (same for all)\n",
    "    for age in range(1, 13):\n",
    "        retention = 0.75 ** age  # Standard decay\n",
    "        df[f'returning_rate_{age}m'] = retention\n",
    "        df[f'falloff_rate_{age}m'] = 1 - retention\n",
    "    \n",
    "    # Clean columns\n",
    "    result_df = df[['product_name', 'order_date', 'channel', 'new_customers', \n",
    "                   'existing_customers', 'num_customers', 'aov_used', 'bm_demand', \n",
    "                   'actual_demand', 'error_metric', 'error_percentage'] + \n",
    "                  [f'falloff_rate_{i}m' for i in range(1, 13)] +\n",
    "                  [f'returning_rate_{i}m' for i in range(1, 13)]].copy()\n",
    "    \n",
    "    result_df.rename(columns={'product_name': 'sku', 'num_customers': 'total_customers'}, inplace=True)\n",
    "\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# USAGE OPTIONS:\n",
    "\n",
    "# Option 1: Fast vectorized approach\n",
    "benchmark_model, benchmark_df = run_fast_benchmark_model(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43\n",
    ")\n",
    "\n",
    "# Option 2: Ultra simple (fastest)\n",
    "# benchmark_df = super_simple_benchmark(\n",
    "#     tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics, aov_avg=43\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FixedBalancedBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "        \"\"\"\n",
    "        FIXED BALANCED benchmark with proper AOV calculation\n",
    "        \"\"\"\n",
    "        print(\"âš¡ðŸŽ¯ FIXED Balanced Benchmark Model\")\n",
    "        print(\"ðŸ”§ Fixed AOV calculation issue\")\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Combine data efficiently\n",
    "        self.daily_data = pd.concat([\n",
    "            tiktok_daily_sku_metrics.assign(channel='tiktok'),\n",
    "            amazon_daily_sku_metrics.assign(channel='amazon'),\n",
    "            shopify_daily_sku_metrics.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        self.order_items = pd.concat([\n",
    "            tiktok__order_items.assign(channel='tiktok'),\n",
    "            amazon_order_item_metrics.assign(channel='amazon'),\n",
    "            shopify__order_items.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Clean dates\n",
    "        self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "        self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "        self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "        self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "        \n",
    "        # Add temporal columns\n",
    "        self.daily_data['month'] = self.daily_data['order_date'].dt.month\n",
    "        self.daily_data['dayofweek'] = self.daily_data['order_date'].dt.dayofweek\n",
    "        \n",
    "        print(f\"âœ… Data loaded: {len(self.daily_data):,} records\")\n",
    "        \n",
    "        # Debug the AOV issue\n",
    "        self.debug_aov_calculation()\n",
    "        \n",
    "    def debug_aov_calculation(self):\n",
    "        \"\"\"\n",
    "        Debug the AOV calculation to see what's going wrong\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ” DEBUGGING AOV CALCULATION:\")\n",
    "        \n",
    "        # Check order items data structure\n",
    "        print(f\"ðŸ“Š Order items columns: {list(self.order_items.columns)}\")\n",
    "        print(f\"ðŸ“Š Sample order items data:\")\n",
    "        print(self.order_items[['product_name', 'sku_gross_sales', 'quantity']].head())\n",
    "        \n",
    "        # Check for any obvious issues\n",
    "        print(f\"\\nðŸ“Š AOV Debug Stats:\")\n",
    "        print(f\"   Total records: {len(self.order_items):,}\")\n",
    "        print(f\"   Revenue range: ${self.order_items['sku_gross_sales'].min():.2f} - ${self.order_items['sku_gross_sales'].max():.2f}\")\n",
    "        print(f\"   Quantity range: {self.order_items['quantity'].min()} - {self.order_items['quantity'].max()}\")\n",
    "        print(f\"   Zero quantities: {(self.order_items['quantity'] == 0).sum():,}\")\n",
    "        \n",
    "        # Calculate simple AOV to see what's happening\n",
    "        if 'quantity' in self.order_items.columns:\n",
    "            # Filter out problematic records\n",
    "            clean_orders = self.order_items[\n",
    "                (self.order_items['quantity'] > 0) & \n",
    "                (self.order_items['sku_gross_sales'] > 0) &\n",
    "                (self.order_items['sku_gross_sales'] < 10000)  # Remove extreme outliers\n",
    "            ].copy()\n",
    "            \n",
    "            if len(clean_orders) > 0:\n",
    "                clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "                \n",
    "                print(f\"   Clean records: {len(clean_orders):,}\")\n",
    "                print(f\"   Clean unit price range: ${clean_orders['unit_price'].min():.2f} - ${clean_orders['unit_price'].max():.2f}\")\n",
    "                print(f\"   Clean unit price mean: ${clean_orders['unit_price'].mean():.2f}\")\n",
    "                print(f\"   Clean unit price median: ${clean_orders['unit_price'].median():.2f}\")\n",
    "            else:\n",
    "                print(\"   âŒ No clean records found!\")\n",
    "        \n",
    "    def calculate_safe_aov_lookup(self):\n",
    "        \"\"\"\n",
    "        Calculate AOV lookup tables with safety checks and outlier removal\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ’° Calculating SAFE AOV lookup tables...\")\n",
    "        \n",
    "        # Clean the order items data first\n",
    "        clean_orders = self.order_items.copy()\n",
    "        \n",
    "        # Remove problematic records\n",
    "        initial_count = len(clean_orders)\n",
    "        \n",
    "        # Remove zero/negative quantities and revenues\n",
    "        clean_orders = clean_orders[\n",
    "            (clean_orders['quantity'] > 0) & \n",
    "            (clean_orders['sku_gross_sales'] > 0)\n",
    "        ]\n",
    "        \n",
    "        # Remove extreme outliers (likely data errors)\n",
    "        clean_orders = clean_orders[\n",
    "            (clean_orders['sku_gross_sales'] < clean_orders['sku_gross_sales'].quantile(0.99)) &\n",
    "            (clean_orders['quantity'] < clean_orders['quantity'].quantile(0.99))\n",
    "        ]\n",
    "        \n",
    "        print(f\"  ðŸ§¹ Cleaned data: {initial_count:,} â†’ {len(clean_orders):,} records\")\n",
    "        \n",
    "        if len(clean_orders) == 0:\n",
    "            print(\"  âŒ No clean order data available, using default AOV for all\")\n",
    "            return {\n",
    "                'sku_aov_lookup': {},\n",
    "                'channel_aov_lookup': {},\n",
    "                'monthly_aov_lookup': {}\n",
    "            }\n",
    "        \n",
    "        # Calculate unit price\n",
    "        clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "        \n",
    "        # Remove unit price outliers\n",
    "        unit_price_q99 = clean_orders['unit_price'].quantile(0.99)\n",
    "        unit_price_q01 = clean_orders['unit_price'].quantile(0.01)\n",
    "        \n",
    "        clean_orders = clean_orders[\n",
    "            (clean_orders['unit_price'] >= unit_price_q01) &\n",
    "            (clean_orders['unit_price'] <= unit_price_q99)\n",
    "        ]\n",
    "        \n",
    "        print(f\"  ðŸ“Š Final clean data: {len(clean_orders):,} records\")\n",
    "        print(f\"  ðŸ“Š Unit price range: ${clean_orders['unit_price'].min():.2f} - ${clean_orders['unit_price'].max():.2f}\")\n",
    "        print(f\"  ðŸ“Š Unit price mean: ${clean_orders['unit_price'].mean():.2f}\")\n",
    "        \n",
    "        # Calculate AOV lookups using MEDIAN (more robust than mean)\n",
    "        \n",
    "        # 1. SKU-level AOV (using median unit price)\n",
    "        sku_aov = clean_orders.groupby('product_name')['unit_price'].agg(['median', 'count']).reset_index()\n",
    "        sku_aov = sku_aov[sku_aov['count'] >= 3]  # Need at least 3 transactions\n",
    "        sku_aov_lookup = sku_aov.set_index('product_name')['median'].to_dict()\n",
    "        \n",
    "        # 2. Channel-level AOV\n",
    "        channel_aov = clean_orders.groupby('channel')['unit_price'].agg(['median', 'count']).reset_index()\n",
    "        channel_aov = channel_aov[channel_aov['count'] >= 10]  # Need at least 10 transactions\n",
    "        channel_aov_lookup = channel_aov.set_index('channel')['median'].to_dict()\n",
    "        \n",
    "        # 3. Monthly AOV\n",
    "        clean_orders['year_month'] = clean_orders['order_date'].dt.to_period('M')\n",
    "        monthly_aov = clean_orders.groupby('year_month')['unit_price'].agg(['median', 'count']).reset_index()\n",
    "        monthly_aov = monthly_aov[monthly_aov['count'] >= 5]  # Need at least 5 transactions\n",
    "        monthly_aov_lookup = monthly_aov.set_index('year_month')['median'].to_dict()\n",
    "        \n",
    "        print(f\"  âœ… AOV Lookups created:\")\n",
    "        print(f\"     - SKU AOV: {len(sku_aov_lookup)} SKUs\")\n",
    "        print(f\"     - Channel AOV: {len(channel_aov_lookup)} channels\")\n",
    "        print(f\"     - Monthly AOV: {len(monthly_aov_lookup)} months\")\n",
    "        \n",
    "        # Show sample AOVs to verify they're reasonable\n",
    "        if sku_aov_lookup:\n",
    "            sample_skus = list(sku_aov_lookup.keys())[:3]\n",
    "            print(f\"  ðŸ“Š Sample SKU AOVs: {[(sku, f'${aov:.2f}') for sku, aov in [(s, sku_aov_lookup[s]) for s in sample_skus]]}\")\n",
    "        \n",
    "        if channel_aov_lookup:\n",
    "            print(f\"  ðŸ“Š Channel AOVs: {[(ch, f'${aov:.2f}') for ch, aov in channel_aov_lookup.items()]}\")\n",
    "        \n",
    "        return {\n",
    "            'sku_aov_lookup': sku_aov_lookup,\n",
    "            'channel_aov_lookup': channel_aov_lookup,\n",
    "            'monthly_aov_lookup': monthly_aov_lookup\n",
    "        }\n",
    "    \n",
    "    def calculate_seasonal_patterns(self):\n",
    "        \"\"\"\n",
    "        Calculate seasonal patterns safely\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ—“ï¸ Calculating seasonal patterns...\")\n",
    "        \n",
    "        # Monthly patterns\n",
    "        monthly_customers = self.daily_data.groupby('month')['num_customers'].mean()\n",
    "        overall_avg = self.daily_data['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            monthly_factors = (monthly_customers / overall_avg).fillna(1.0).to_dict()\n",
    "        else:\n",
    "            monthly_factors = {i: 1.0 for i in range(1, 13)}\n",
    "        \n",
    "        # Day-of-week patterns\n",
    "        dow_customers = self.daily_data.groupby('dayofweek')['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            dow_factors = (dow_customers / overall_avg).fillna(1.0).to_dict()\n",
    "        else:\n",
    "            dow_factors = {i: 1.0 for i in range(7)}\n",
    "        \n",
    "        # Reasonable bounds on seasonal factors\n",
    "        for month in monthly_factors:\n",
    "            monthly_factors[month] = max(0.5, min(2.0, monthly_factors[month]))\n",
    "        \n",
    "        for dow in dow_factors:\n",
    "            dow_factors[dow] = max(0.5, min(2.0, dow_factors[dow]))\n",
    "        \n",
    "        print(f\"  âœ… Seasonal patterns calculated\")\n",
    "        print(f\"     Monthly variation: {max(monthly_factors.values()) - min(monthly_factors.values()):.1%}\")\n",
    "        print(f\"     Weekly variation: {max(dow_factors.values()) - min(dow_factors.values()):.1%}\")\n",
    "        \n",
    "        return monthly_factors, dow_factors\n",
    "    \n",
    "    def calculate_retention_patterns(self):\n",
    "        \"\"\"\n",
    "        Calculate customer retention patterns\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ‘¥ Calculating retention patterns...\")\n",
    "        \n",
    "        # Simple but effective approach\n",
    "        retention_lookup = {}\n",
    "        \n",
    "        # Base retention pattern (industry standard e-commerce)\n",
    "        base_monthly_retention = 0.75  # 75% monthly retention\n",
    "        \n",
    "        for age_months in range(1, 13):\n",
    "            # Exponential decay with some randomness to reflect real patterns\n",
    "            retention = base_monthly_retention ** age_months\n",
    "            \n",
    "            # Add slight variation based on actual data if available\n",
    "            if len(self.daily_data) > 30:\n",
    "                # Look at new vs existing customer ratio trends\n",
    "                avg_new_pct = self.daily_data['pct_new_customers'].mean()\n",
    "                if avg_new_pct > 0.5:  # High churn environment\n",
    "                    retention *= 0.9  # Slightly lower retention\n",
    "                elif avg_new_pct < 0.3:  # Low churn environment\n",
    "                    retention *= 1.1  # Slightly higher retention\n",
    "            \n",
    "            # Ensure reasonable bounds\n",
    "            retention = max(0.05, min(0.95, retention))\n",
    "            retention_lookup[age_months] = retention\n",
    "        \n",
    "        print(f\"  âœ… Retention patterns calculated\")\n",
    "        for age in [1, 3, 6, 12]:\n",
    "            if age in retention_lookup:\n",
    "                print(f\"     {age:2d} months: {retention_lookup[age]:.1%} retention\")\n",
    "        \n",
    "        return retention_lookup\n",
    "    \n",
    "    def get_safe_aov(self, df, aov_lookups):\n",
    "        \"\"\"\n",
    "        Get AOV safely with multiple fallbacks\n",
    "        \"\"\"\n",
    "        # Start with default\n",
    "        aov_series = pd.Series(self.aov_avg, index=df.index)\n",
    "        \n",
    "        # Try SKU AOV\n",
    "        if aov_lookups['sku_aov_lookup']:\n",
    "            sku_aov = df['product_name'].map(aov_lookups['sku_aov_lookup'])\n",
    "            # Only use if reasonable\n",
    "            sku_aov_clean = sku_aov[(sku_aov >= 1) & (sku_aov <= 1000)]\n",
    "            aov_series.update(sku_aov_clean)\n",
    "        \n",
    "        # Try channel AOV\n",
    "        if aov_lookups['channel_aov_lookup']:\n",
    "            channel_aov = df['channel'].map(aov_lookups['channel_aov_lookup'])\n",
    "            channel_aov_clean = channel_aov[(channel_aov >= 1) & (channel_aov <= 1000)]\n",
    "            aov_series = aov_series.fillna(channel_aov_clean)\n",
    "        \n",
    "        # Try monthly AOV\n",
    "        if aov_lookups['monthly_aov_lookup']:\n",
    "            df['year_month'] = df['order_date'].dt.to_period('M')\n",
    "            monthly_aov = df['year_month'].map(aov_lookups['monthly_aov_lookup'])\n",
    "            monthly_aov_clean = monthly_aov[(monthly_aov >= 1) & (monthly_aov <= 1000)]\n",
    "            aov_series = aov_series.fillna(monthly_aov_clean)\n",
    "        \n",
    "        # Final safety check\n",
    "        aov_series = aov_series.fillna(self.aov_avg)\n",
    "        aov_series = np.clip(aov_series, 1, 1000)  # Reasonable AOV bounds\n",
    "        \n",
    "        return aov_series\n",
    "    \n",
    "    def calculate_fixed_benchmark(self):\n",
    "        \"\"\"\n",
    "        Calculate benchmark with all fixes applied\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ”§ Calculating FIXED benchmark...\")\n",
    "        \n",
    "        # Get all lookup tables\n",
    "        aov_lookups = self.calculate_safe_aov_lookup()\n",
    "        monthly_factors, dow_factors = self.calculate_seasonal_patterns()\n",
    "        retention_lookup = self.calculate_retention_patterns()\n",
    "        \n",
    "        # Start with daily data\n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Basic customer calculations\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        benchmark_df['total_customers'] = benchmark_df['num_customers']\n",
    "        \n",
    "        # Get SAFE AOV\n",
    "        print(\"  ðŸ’° Applying safe AOV...\")\n",
    "        benchmark_df['aov_used'] = self.get_safe_aov(benchmark_df, aov_lookups)\n",
    "        \n",
    "        # Seasonal adjustments\n",
    "        print(\"  ðŸ—“ï¸ Applying seasonal adjustments...\")\n",
    "        benchmark_df['monthly_factor'] = benchmark_df['month'].map(monthly_factors)\n",
    "        benchmark_df['dow_factor'] = benchmark_df['dayofweek'].map(dow_factors)\n",
    "        benchmark_df['seasonal_adjustment'] = (benchmark_df['monthly_factor'] * 0.7 + \n",
    "                                             benchmark_df['dow_factor'] * 0.3)\n",
    "        \n",
    "        # Calculate demand\n",
    "        print(\"  ðŸŽ¯ Calculating demand...\")\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['base_existing_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['adjusted_existing_demand'] = (benchmark_df['base_existing_demand'] * \n",
    "                                                   benchmark_df['seasonal_adjustment'])\n",
    "        benchmark_df['bm_demand'] = (benchmark_df['new_customer_demand'] + \n",
    "                                   benchmark_df['adjusted_existing_demand'])\n",
    "        \n",
    "        # Calculate actual demand more carefully\n",
    "        print(\"  ðŸ“Š Calculating actual demand...\")\n",
    "        \n",
    "        # Try to get actual revenue, but be more careful about aggregation\n",
    "        try:\n",
    "            actual_revenue = self.order_items.groupby(['product_name', 'order_date']).agg({\n",
    "                'sku_gross_sales': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                            left_on=['product_name', 'order_date'],\n",
    "                                            right_on=['product_name', 'order_date'], \n",
    "                                            how='left')\n",
    "            \n",
    "            # Clean actual demand\n",
    "            benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(0)\n",
    "            \n",
    "            # If actual demand seems too low, use customer-based estimate\n",
    "            customer_based_estimate = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "            \n",
    "            # Use the minimum of the two (more conservative)\n",
    "            benchmark_df['actual_demand'] = np.where(\n",
    "                benchmark_df['actual_demand'] > 0,\n",
    "                benchmark_df['actual_demand'],\n",
    "                customer_based_estimate\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ Revenue calculation failed: {e}, using customer-based estimate\")\n",
    "            benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # Calculate errors\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add retention rates\n",
    "        print(\"  ðŸ“‰ Adding retention rates...\")\n",
    "        for age_months in range(1, 13):\n",
    "            base_retention = retention_lookup[age_months]\n",
    "            seasonal_retention = base_retention * benchmark_df['seasonal_adjustment']\n",
    "            seasonal_retention = np.clip(seasonal_retention, 0.05, 0.95)\n",
    "            \n",
    "            benchmark_df[f'returning_rate_{age_months}m'] = seasonal_retention\n",
    "            benchmark_df[f'falloff_rate_{age_months}m'] = 1 - seasonal_retention\n",
    "        \n",
    "        # Clean up columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'total_customers',\n",
    "            'aov_used', 'new_customer_demand', 'adjusted_existing_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + [f'returning_rate_{i}m' for i in range(1, 13)] + [f'falloff_rate_{i}m' for i in range(1, 13)]\n",
    "        \n",
    "        benchmark_df = benchmark_df[final_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku'}, inplace=True)\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_fixed_benchmark(self):\n",
    "        \"\"\"\n",
    "        Run the fixed benchmark model\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ RUNNING FIXED BALANCED BENCHMARK\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        benchmark_df = self.calculate_fixed_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\nðŸ“Š FIXED BENCHMARK RESULTS:\")\n",
    "        print(f\"  âš¡ Processed {len(benchmark_df):,} records in {duration:.1f} seconds\")\n",
    "        print(f\"  ðŸ›ï¸ SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  ðŸ“… Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        print(f\"  ðŸ’° Avg AOV: ${benchmark_df['aov_used'].mean():.2f} (vs ${self.aov_avg} default)\")\n",
    "        print(f\"  ðŸ“Š AOV range: ${benchmark_df['aov_used'].min():.2f} - ${benchmark_df['aov_used'].max():.2f}\")\n",
    "        print(f\"  ðŸ’° Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  ðŸ’° Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  ðŸ“ˆ Avg Error: ${benchmark_df['error_metric'].mean():.2f}\")\n",
    "        print(f\"  ðŸ“Š MAPE: {benchmark_df['error_percentage'].abs().mean():.1f}%\")\n",
    "        \n",
    "        # Sanity checks\n",
    "        reasonable_aov = (benchmark_df['aov_used'] >= 1) & (benchmark_df['aov_used'] <= 1000)\n",
    "        print(f\"  âœ… Reasonable AOV %: {reasonable_aov.mean():.1%}\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_fixed_benchmark_model(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                            shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                            tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "    \"\"\"\n",
    "    Fixed benchmark model with proper AOV calculation\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”§âš¡ FIXED BALANCED BENCHMARK MODEL\")\n",
    "    print(\"ðŸ› ï¸  Fixes:\")\n",
    "    print(\"   âœ… Proper AOV calculation (median-based)\")\n",
    "    print(\"   âœ… Outlier removal\")\n",
    "    print(\"   âœ… Multiple AOV fallbacks\")\n",
    "    print(\"   âœ… Reasonable bounds checking\")\n",
    "    print(\"   âœ… Safe error handling\")\n",
    "    \n",
    "    benchmark = FixedBalancedBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    benchmark_df = benchmark.run_fixed_benchmark()\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "# USAGE:\n",
    "fixed_model, fixed_benchmark_df = run_fixed_benchmark_model(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_amazon_cohorts = pd.read_parquet(f\"{BASE_PATH}final_amazon__cohorts_monthly_monthly.parquet\")\n",
    "final_shopify_cohorts = pd.read_parquet(f\"{BASE_PATH}final_shopify__cohorts_monthly_monthly.parquet\")\n",
    "final_tiktok_cohorts = pd.read_parquet(f\"{BASE_PATH}final_tiktok_shop__cohorts_monthly_monthly.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f315b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedRobustBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, \n",
    "                 final_amazon_cohorts=None, final_tiktok_cohorts=None, final_shopify_cohorts=None,\n",
    "                 aov_avg=43):\n",
    "        \"\"\"\n",
    "        Enhanced robust benchmark that integrates real cohort data for better retention rates\n",
    "        \"\"\"\n",
    "        print(\"ðŸ›¡ï¸ ENHANCED ROBUST Benchmark Model\")\n",
    "        print(\"ðŸš€ Designed to avoid pandas groupby errors + Real cohort data\")\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Store cohort data\n",
    "        self.cohort_data = {}\n",
    "        if final_amazon_cohorts is not None:\n",
    "            self.cohort_data['amazon'] = final_amazon_cohorts.copy()\n",
    "            print(f\"   ðŸ“Š Amazon cohorts: {len(final_amazon_cohorts):,} records\")\n",
    "        if final_tiktok_cohorts is not None:\n",
    "            self.cohort_data['tiktok'] = final_tiktok_cohorts.copy()\n",
    "            print(f\"   ðŸ“Š TikTok cohorts: {len(final_tiktok_cohorts):,} records\")\n",
    "        if final_shopify_cohorts is not None:\n",
    "            self.cohort_data['shopify'] = final_shopify_cohorts.copy()\n",
    "            print(f\"   ðŸ“Š Shopify cohorts: {len(final_shopify_cohorts):,} records\")\n",
    "        \n",
    "        # Combine data safely (same as original)\n",
    "        print(\"ðŸ“Š Combining data...\")\n",
    "        daily_datasets = []\n",
    "        \n",
    "        if not tiktok_daily_sku_metrics.empty:\n",
    "            tiktok_data = tiktok_daily_sku_metrics.copy()\n",
    "            tiktok_data['channel'] = 'tiktok'\n",
    "            daily_datasets.append(tiktok_data)\n",
    "            print(f\"   TikTok: {len(tiktok_data):,} records\")\n",
    "        \n",
    "        if not amazon_daily_sku_metrics.empty:\n",
    "            amazon_data = amazon_daily_sku_metrics.copy()\n",
    "            amazon_data['channel'] = 'amazon'\n",
    "            daily_datasets.append(amazon_data)\n",
    "            print(f\"   Amazon: {len(amazon_data):,} records\")\n",
    "        \n",
    "        if not shopify_daily_sku_metrics.empty:\n",
    "            shopify_data = shopify_daily_sku_metrics.copy()\n",
    "            shopify_data['channel'] = 'shopify'\n",
    "            daily_datasets.append(shopify_data)\n",
    "            print(f\"   Shopify: {len(shopify_data):,} records\")\n",
    "        \n",
    "        if daily_datasets:\n",
    "            self.daily_data = pd.concat(daily_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"âŒ No daily data available!\")\n",
    "            self.daily_data = pd.DataFrame()\n",
    "            return\n",
    "        \n",
    "        # Combine order data safely (same as original)\n",
    "        order_datasets = []\n",
    "        \n",
    "        if not tiktok__order_items.empty:\n",
    "            tiktok_orders = tiktok__order_items.copy()\n",
    "            tiktok_orders['channel'] = 'tiktok'\n",
    "            order_datasets.append(tiktok_orders)\n",
    "        \n",
    "        if not amazon_order_item_metrics.empty:\n",
    "            amazon_orders = amazon_order_item_metrics.copy()\n",
    "            amazon_orders['channel'] = 'amazon'\n",
    "            order_datasets.append(amazon_orders)\n",
    "        \n",
    "        if not shopify__order_items.empty:\n",
    "            shopify_orders = shopify__order_items.copy()\n",
    "            shopify_orders['channel'] = 'shopify'\n",
    "            order_datasets.append(shopify_orders)\n",
    "        \n",
    "        if order_datasets:\n",
    "            self.order_items = pd.concat(order_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"âš ï¸ No order data available - will use customer-based estimates\")\n",
    "            self.order_items = pd.DataFrame()\n",
    "        \n",
    "        # Clean dates safely (same as original)\n",
    "        print(\"ðŸ“… Cleaning dates...\")\n",
    "        try:\n",
    "            self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "            print(f\"   Daily data date range: {self.daily_data['order_date'].min()} to {self.daily_data['order_date'].max()}\")\n",
    "        except:\n",
    "            print(\"âŒ Error cleaning daily data dates\")\n",
    "            return\n",
    "        \n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "                self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "                self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "                print(f\"   Order data date range: {self.order_items['order_date'].min()} to {self.order_items['order_date'].max()}\")\n",
    "            except:\n",
    "                print(\"âš ï¸ Error cleaning order dates - will use fallback AOV\")\n",
    "                self.order_items = pd.DataFrame()\n",
    "        \n",
    "        print(f\"âœ… Data loaded: {len(self.daily_data):,} daily records, {len(self.order_items):,} order items\")\n",
    "    \n",
    "    def calculate_cohort_returning_rates(self):\n",
    "        \"\"\"\n",
    "        Calculate returning rates from real cohort data using customer counts\n",
    "        \n",
    "        Returning (t+1) rate = customers at t+1 / customers at t\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“Š Calculating returning rates from real cohort data...\")\n",
    "        \n",
    "        if not self.cohort_data:\n",
    "            print(\"   No cohort data available, using default rates\")\n",
    "            return self.calculate_fallback_retention_rates()\n",
    "        \n",
    "        all_channel_rates = {}\n",
    "        \n",
    "        for channel, cohort_df in self.cohort_data.items():\n",
    "            print(f\"\\nðŸ” Processing {channel} cohorts...\")\n",
    "            \n",
    "            # Prepare cohort data\n",
    "            df = cohort_df.copy()\n",
    "            df['customer_cohort'] = pd.to_datetime(df['customer_cohort'])\n",
    "            df['order_month'] = pd.to_datetime(df['order_month'])\n",
    "            \n",
    "            # Calculate customer age in months\n",
    "            df['customer_age_months'] = (\n",
    "                (df['order_month'].dt.year - df['customer_cohort'].dt.year) * 12 +\n",
    "                (df['order_month'].dt.month - df['customer_cohort'].dt.month)\n",
    "            )\n",
    "            \n",
    "            # Filter to reasonable ages (0-12 months)\n",
    "            df = df[(df['customer_age_months'] >= 0) & (df['customer_age_months'] <= 12)]\n",
    "            \n",
    "            channel_rates = {}\n",
    "            \n",
    "            # Calculate returning rate for each age (1-12 months)\n",
    "            for age_months in range(1, 13):\n",
    "                \n",
    "                returning_rates_for_age = []\n",
    "                \n",
    "                # For each cohort, calculate returning rate from (age-1) to age\n",
    "                for cohort_date in df['customer_cohort'].unique():\n",
    "                    cohort_data = df[df['customer_cohort'] == cohort_date]\n",
    "                    \n",
    "                    # Get customers at age (age_months - 1)\n",
    "                    customers_at_prev = cohort_data[cohort_data['customer_age_months'] == (age_months - 1)]\n",
    "                    if len(customers_at_prev) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Use the best available customer count column\n",
    "                    if 'unique_customer_ct' in customers_at_prev.columns:\n",
    "                        count_prev = customers_at_prev['unique_customer_ct'].sum()\n",
    "                    elif 'cohort_size' in customers_at_prev.columns:\n",
    "                        count_prev = customers_at_prev['cohort_size'].sum()\n",
    "                    else:\n",
    "                        # Fallback: estimate from orders\n",
    "                        count_prev = customers_at_prev['total_order_ct'].sum() if 'total_order_ct' in customers_at_prev.columns else 0\n",
    "                    \n",
    "                    if count_prev <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get customers at age (age_months)\n",
    "                    customers_at_curr = cohort_data[cohort_data['customer_age_months'] == age_months]\n",
    "                    if len(customers_at_curr) == 0:\n",
    "                        count_curr = 0\n",
    "                    else:\n",
    "                        if 'unique_customer_ct' in customers_at_curr.columns:\n",
    "                            count_curr = customers_at_curr['unique_customer_ct'].sum()\n",
    "                        elif 'cohort_size' in customers_at_curr.columns:\n",
    "                            count_curr = customers_at_curr['cohort_size'].sum()\n",
    "                        else:\n",
    "                            count_curr = customers_at_curr['total_order_ct'].sum() if 'total_order_ct' in customers_at_curr.columns else 0\n",
    "                    \n",
    "                    # Calculate returning rate: customers at t+1 / customers at t\n",
    "                    returning_rate = count_curr / count_prev\n",
    "                    returning_rate = min(returning_rate, 1.0)  # Cap at 100%\n",
    "                    \n",
    "                    returning_rates_for_age.append(returning_rate)\n",
    "                \n",
    "                # Average returning rate across all cohorts for this age\n",
    "                if returning_rates_for_age:\n",
    "                    avg_returning_rate = np.mean(returning_rates_for_age)\n",
    "                    channel_rates[age_months] = avg_returning_rate\n",
    "                    print(f\"   {age_months:2d} months: {avg_returning_rate:.1%} returning rate from {len(returning_rates_for_age)} cohorts\")\n",
    "                else:\n",
    "                    # Fallback if no data\n",
    "                    fallback_rate = 0.75 ** age_months\n",
    "                    channel_rates[age_months] = fallback_rate\n",
    "                    print(f\"   {age_months:2d} months: {fallback_rate:.1%} returning rate (fallback)\")\n",
    "            \n",
    "            all_channel_rates[channel] = channel_rates\n",
    "        \n",
    "        # Calculate blended returning rates across channels\n",
    "        blended_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            rates_for_age = []\n",
    "            for channel_rates in all_channel_rates.values():\n",
    "                if age_months in channel_rates:\n",
    "                    rates_for_age.append(channel_rates[age_months])\n",
    "            \n",
    "            if rates_for_age:\n",
    "                blended_rates[age_months] = np.mean(rates_for_age)\n",
    "            else:\n",
    "                # Fallback\n",
    "                blended_rates[age_months] = 0.75 ** age_months\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Blended returning rates from cohort data:\")\n",
    "        for age in [1, 3, 6, 12]:\n",
    "            if age in blended_rates:\n",
    "                print(f\"   {age:2d} months: {blended_rates[age]:.1%}\")\n",
    "        \n",
    "        return blended_rates\n",
    "    \n",
    "    def calculate_fallback_retention_rates(self):\n",
    "        \"\"\"\n",
    "        Fallback retention calculation (same as original) if no cohort data\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ‘¥ Calculating fallback retention rates...\")\n",
    "        \n",
    "        # Check if we have customer lifecycle data\n",
    "        has_lifecycle_data = ('pct_new_customers' in self.daily_data.columns and \n",
    "                             not self.daily_data['pct_new_customers'].isna().all())\n",
    "        \n",
    "        if has_lifecycle_data:\n",
    "            avg_new_pct = self.daily_data['pct_new_customers'].mean()\n",
    "            \n",
    "            if avg_new_pct > 0.6:  # High churn environment\n",
    "                base_retention = 0.70\n",
    "                print(\"   High acquisition environment detected - using 70% base retention\")\n",
    "            elif avg_new_pct < 0.3:  # Low churn environment\n",
    "                base_retention = 0.80\n",
    "                print(\"   High retention environment detected - using 80% base retention\")\n",
    "            else:\n",
    "                base_retention = 0.75\n",
    "                print(\"   Balanced environment detected - using 75% base retention\")\n",
    "        else:\n",
    "            base_retention = 0.75\n",
    "            print(\"   Using standard 75% base retention\")\n",
    "        \n",
    "        # Calculate retention for each age month\n",
    "        retention_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            retention = base_retention ** age_months\n",
    "            retention = max(0.05, min(0.95, retention))\n",
    "            retention_rates[age_months] = retention\n",
    "        \n",
    "        print(f\"   1-month retention: {retention_rates[1]:.1%}\")\n",
    "        print(f\"   6-month retention: {retention_rates[6]:.1%}\")\n",
    "        print(f\"   12-month retention: {retention_rates[12]:.1%}\")\n",
    "        \n",
    "        return retention_rates\n",
    "    \n",
    "    # Keep all the other methods from the original SimpleRobustBenchmark\n",
    "    def calculate_simple_aov_by_channel(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\nðŸ’° Calculating simple AOV by channel...\")\n",
    "        \n",
    "        channel_aov = {}\n",
    "        \n",
    "        if self.order_items.empty:\n",
    "            print(\"   Using default AOV for all channels\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['sku_gross_sales', 'quantity', 'channel']\n",
    "        missing_cols = [col for col in required_cols if col not in self.order_items.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"   Missing columns: {missing_cols}, using default AOV\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        for channel in ['tiktok', 'amazon', 'shopify']:\n",
    "            print(f\"   Calculating {channel} AOV...\")\n",
    "            \n",
    "            # Filter channel data\n",
    "            channel_orders = self.order_items[self.order_items['channel'] == channel].copy()\n",
    "            \n",
    "            if len(channel_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Clean data simply\n",
    "            clean_orders = channel_orders[\n",
    "                (channel_orders['quantity'] > 0) & \n",
    "                (channel_orders['sku_gross_sales'] > 0) &\n",
    "                (channel_orders['sku_gross_sales'] < 10000) &  # Remove extreme outliers\n",
    "                (channel_orders['quantity'] < 1000)  # Remove bulk orders\n",
    "            ].copy()\n",
    "            \n",
    "            if len(clean_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No clean {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate unit prices\n",
    "            clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "            \n",
    "            # Remove unit price outliers\n",
    "            unit_prices = clean_orders['unit_price']\n",
    "            q1 = unit_prices.quantile(0.25)\n",
    "            q3 = unit_prices.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(1, q1 - 1.5 * iqr)\n",
    "            upper_bound = min(1000, q3 + 1.5 * iqr)\n",
    "            \n",
    "            final_prices = unit_prices[(unit_prices >= lower_bound) & (unit_prices <= upper_bound)]\n",
    "            \n",
    "            if len(final_prices) > 0:\n",
    "                channel_aov[channel] = final_prices.median()\n",
    "                print(f\"     {channel.capitalize()} AOV: ${channel_aov[channel]:.2f} (from {len(final_prices):,} clean orders)\")\n",
    "            else:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No valid {channel} prices, using default ${self.aov_avg}\")\n",
    "        \n",
    "        return channel_aov\n",
    "    \n",
    "    def calculate_simple_seasonal_factors(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\nðŸ—“ï¸ Calculating simple seasonal factors...\")\n",
    "        \n",
    "        # Add temporal columns safely\n",
    "        try:\n",
    "            self.daily_data['month'] = self.daily_data['order_date'].dt.month\n",
    "            self.daily_data['dayofweek'] = self.daily_data['order_date'].dt.dayofweek\n",
    "        except:\n",
    "            print(\"   Error adding temporal columns, using default factors\")\n",
    "            return {\n",
    "                'monthly_factors': {i: 1.0 for i in range(1, 13)},\n",
    "                'dow_factors': {i: 1.0 for i in range(7)}\n",
    "            }\n",
    "        \n",
    "        # Calculate monthly factors simply\n",
    "        monthly_factors = {}\n",
    "        overall_avg = self.daily_data['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            for month in range(1, 13):\n",
    "                month_data = self.daily_data[self.daily_data['month'] == month]\n",
    "                if len(month_data) > 0:\n",
    "                    month_avg = month_data['num_customers'].mean()\n",
    "                    factor = month_avg / overall_avg\n",
    "                    monthly_factors[month] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    monthly_factors[month] = 1.0\n",
    "        else:\n",
    "            monthly_factors = {i: 1.0 for i in range(1, 13)}\n",
    "        \n",
    "        # Calculate day-of-week factors simply\n",
    "        dow_factors = {}\n",
    "        if overall_avg > 0:\n",
    "            for dow in range(7):\n",
    "                dow_data = self.daily_data[self.daily_data['dayofweek'] == dow]\n",
    "                if len(dow_data) > 0:\n",
    "                    dow_avg = dow_data['num_customers'].mean()\n",
    "                    factor = dow_avg / overall_avg\n",
    "                    dow_factors[dow] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    dow_factors[dow] = 1.0\n",
    "        else:\n",
    "            dow_factors = {i: 1.0 for i in range(7)}\n",
    "        \n",
    "        print(f\"   Monthly variation: {max(monthly_factors.values()) - min(monthly_factors.values()):.1%}\")\n",
    "        print(f\"   Weekly variation: {max(dow_factors.values()) - min(dow_factors.values()):.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'monthly_factors': monthly_factors,\n",
    "            'dow_factors': dow_factors\n",
    "        }\n",
    "    \n",
    "    def calculate_robust_benchmark(self):\n",
    "        \"\"\"\n",
    "        Enhanced benchmark calculation using real cohort data for retention rates\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸŽ¯ Calculating enhanced robust benchmark...\")\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"âŒ No daily data available!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get patterns (using cohort data for retention rates!)\n",
    "        channel_aov = self.calculate_simple_aov_by_channel()\n",
    "        seasonal_patterns = self.calculate_simple_seasonal_factors()\n",
    "        retention_rates = self.calculate_cohort_returning_rates()  # âœ… Use real cohort data!\n",
    "        \n",
    "        # Start with daily data\n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Basic customer calculations\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        benchmark_df['total_customers'] = benchmark_df['num_customers']\n",
    "        \n",
    "        # Apply AOV by channel\n",
    "        print(\"   Applying channel-specific AOV...\")\n",
    "        benchmark_df['aov_used'] = benchmark_df['channel'].map(channel_aov).fillna(self.aov_avg)\n",
    "        \n",
    "        # Apply seasonal adjustments\n",
    "        print(\"   Applying seasonal adjustments...\")\n",
    "        benchmark_df['monthly_factor'] = benchmark_df['month'].map(seasonal_patterns['monthly_factors']).fillna(1.0)\n",
    "        benchmark_df['dow_factor'] = benchmark_df['dayofweek'].map(seasonal_patterns['dow_factors']).fillna(1.0)\n",
    "        benchmark_df['seasonal_adjustment'] = (benchmark_df['monthly_factor'] * 0.7 + \n",
    "                                             benchmark_df['dow_factor'] * 0.3)\n",
    "        \n",
    "        # Calculate demand using YOUR EXACT FORMULA\n",
    "        print(\"   Calculating benchmark demand using your formula...\")\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['existing_customer_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # âœ… YOUR EXACT FORMULA: BM_Demand = new_customers * AOV + existing_customers * AOV\n",
    "        benchmark_df['bm_demand'] = (benchmark_df['new_customer_demand'] + \n",
    "                                   benchmark_df['existing_customer_demand'])\n",
    "        \n",
    "        # Calculate actual demand\n",
    "        print(\"   Calculating actual demand...\")\n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                # Try to get actual revenue\n",
    "                actual_revenue = self.order_items.groupby(['product_name', 'order_date', 'channel']).agg({\n",
    "                    'sku_gross_sales': 'sum'\n",
    "                }).reset_index()\n",
    "                \n",
    "                benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                                left_on=['product_name', 'order_date', 'channel'],\n",
    "                                                right_on=['product_name', 'order_date', 'channel'], \n",
    "                                                how='left')\n",
    "                \n",
    "                benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(\n",
    "                    benchmark_df['total_customers'] * benchmark_df['aov_used'])\n",
    "            except:\n",
    "                print(\"     Revenue calculation failed, using customer-based estimate\")\n",
    "                benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        else:\n",
    "            benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # âœ… YOUR EXACT ERROR FORMULA: Actual_Demand - BM_Demand\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add real cohort-based retention rates\n",
    "        print(\"   Adding real cohort-based retention rates...\")\n",
    "        for age_months in range(1, 13):\n",
    "            real_retention = retention_rates[age_months]\n",
    "            # Apply seasonal adjustment\n",
    "            seasonal_retention = real_retention * benchmark_df['seasonal_adjustment']\n",
    "            seasonal_retention = np.clip(seasonal_retention, 0.05, 0.95)\n",
    "            \n",
    "            benchmark_df[f'returning_rate_{age_months}m'] = seasonal_retention\n",
    "            benchmark_df[f'falloff_rate_{age_months}m'] = 1 - seasonal_retention\n",
    "        \n",
    "        # Select final columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'total_customers',\n",
    "            'aov_used', 'new_customer_demand', 'existing_customer_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + [f'returning_rate_{i}m' for i in range(1, 13)] + [f'falloff_rate_{i}m' for i in range(1, 13)]\n",
    "        \n",
    "        # Only keep columns that exist\n",
    "        existing_columns = [col for col in final_columns if col in benchmark_df.columns]\n",
    "        benchmark_df = benchmark_df[existing_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku'}, inplace=True)\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_enhanced_benchmark(self):\n",
    "        \"\"\"\n",
    "        Run the enhanced benchmark with real cohort data\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ RUNNING ENHANCED ROBUST BENCHMARK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"âŒ No data to process!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        benchmark_df = self.calculate_robust_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if benchmark_df.empty:\n",
    "            print(\"âŒ No benchmark results generated!\")\n",
    "            return benchmark_df\n",
    "        \n",
    "        print(f\"\\nðŸ“Š ENHANCED BENCHMARK RESULTS:\")\n",
    "        print(f\"  âš¡ Processed {len(benchmark_df):,} records in {duration:.1f} seconds\")\n",
    "        print(f\"  ðŸ›ï¸ SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  ðŸ“… Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        \n",
    "        # Performance by channel\n",
    "        if 'channel' in benchmark_df.columns:\n",
    "            print(f\"\\nðŸ“º PERFORMANCE BY CHANNEL:\")\n",
    "            for channel in benchmark_df['channel'].unique():\n",
    "                channel_data = benchmark_df[benchmark_df['channel'] == channel]\n",
    "                avg_aov = channel_data['aov_used'].mean()\n",
    "                mape = channel_data['error_percentage'].abs().mean()\n",
    "                records = len(channel_data)\n",
    "                \n",
    "                print(f\"  {channel.capitalize():<8}: {records:,} records, AOV ${avg_aov:.2f}, MAPE {mape:.1f}%\")\n",
    "        \n",
    "        # Overall performance\n",
    "        overall_mape = benchmark_df['error_percentage'].abs().mean()\n",
    "        overall_aov = benchmark_df['aov_used'].mean()\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ OVERALL PERFORMANCE:\")\n",
    "        print(f\"  ðŸ“Š Total MAPE: {overall_mape:.1f}%\")\n",
    "        print(f\"  ðŸ’° Overall Avg AOV: ${overall_aov:.2f}\")\n",
    "        print(f\"  ðŸ“ˆ Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  ðŸ“ˆ Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  âœ… Using REAL cohort data for retention rates!\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_enhanced_robust_benchmark(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                                shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                                tiktok__order_items, shopify__order_items,\n",
    "                                final_amazon_cohorts=None, final_tiktok_cohorts=None, \n",
    "                                final_shopify_cohorts=None, aov_avg=43):\n",
    "    \"\"\"\n",
    "    Enhanced robust benchmark model with real cohort data integration\n",
    "    \"\"\"\n",
    "    print(\"ðŸ›¡ï¸ ENHANCED ROBUST BENCHMARK MODEL\")\n",
    "    print(\"ðŸ”§ Features:\")\n",
    "    print(\"   âœ… Safe data handling\")\n",
    "    print(\"   âœ… Simple operations (no complex groupby)\")\n",
    "    print(\"   âœ… Multiple fallbacks\")\n",
    "    print(\"   âœ… Channel-specific AOV\")\n",
    "    print(\"   âœ… Seasonal adjustments\")\n",
    "    print(\"   âœ… REAL cohort-based retention rates\")\n",
    "    print(\"   âœ… Your exact formulas: BM_Demand = new_customers * AOV + existing_customers * AOV\")\n",
    "    print(\"   âœ… Your exact error: Actual_Demand - BM_Demand\")\n",
    "    \n",
    "    benchmark = EnhancedRobustBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        final_amazon_cohorts=final_amazon_cohorts,\n",
    "        final_tiktok_cohorts=final_tiktok_cohorts,\n",
    "        final_shopify_cohorts=final_shopify_cohorts,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    benchmark_df = benchmark.run_enhanced_benchmark()\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "# USAGE:\n",
    "enhanced_model, enhanced_benchmark_df = run_enhanced_robust_benchmark(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items,\n",
    "    final_amazon_cohorts, final_tiktok_cohorts, final_shopify_cohorts, aov_avg=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedRobustBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, \n",
    "                 final_amazon_cohorts=None, final_tiktok_cohorts=None, final_shopify_cohorts=None,\n",
    "                 aov_avg=43):\n",
    "        \"\"\"\n",
    "        Enhanced robust benchmark that integrates real cohort data for better retention rates\n",
    "        \"\"\"\n",
    "        print(\"ðŸ›¡ï¸ ENHANCED ROBUST Benchmark Model\")\n",
    "        print(\"ðŸš€ Designed to avoid pandas groupby errors + Real cohort data\")\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Store cohort data\n",
    "        self.cohort_data = {}\n",
    "        if final_amazon_cohorts is not None:\n",
    "            self.cohort_data['amazon'] = final_amazon_cohorts.copy()\n",
    "            print(f\"   ðŸ“Š Amazon cohorts: {len(final_amazon_cohorts):,} records\")\n",
    "        if final_tiktok_cohorts is not None:\n",
    "            self.cohort_data['tiktok'] = final_tiktok_cohorts.copy()\n",
    "            print(f\"   ðŸ“Š TikTok cohorts: {len(final_tiktok_cohorts):,} records\")\n",
    "        if final_shopify_cohorts is not None:\n",
    "            self.cohort_data['shopify'] = final_shopify_cohorts.copy()\n",
    "            print(f\"   ðŸ“Š Shopify cohorts: {len(final_shopify_cohorts):,} records\")\n",
    "        \n",
    "        # Combine data safely (same as original)\n",
    "        print(\"ðŸ“Š Combining data...\")\n",
    "        daily_datasets = []\n",
    "        \n",
    "        if not tiktok_daily_sku_metrics.empty:\n",
    "            tiktok_data = tiktok_daily_sku_metrics.copy()\n",
    "            tiktok_data['channel'] = 'tiktok'\n",
    "            daily_datasets.append(tiktok_data)\n",
    "            print(f\"   TikTok: {len(tiktok_data):,} records\")\n",
    "        \n",
    "        if not amazon_daily_sku_metrics.empty:\n",
    "            amazon_data = amazon_daily_sku_metrics.copy()\n",
    "            amazon_data['channel'] = 'amazon'\n",
    "            daily_datasets.append(amazon_data)\n",
    "            print(f\"   Amazon: {len(amazon_data):,} records\")\n",
    "        \n",
    "        if not shopify_daily_sku_metrics.empty:\n",
    "            shopify_data = shopify_daily_sku_metrics.copy()\n",
    "            shopify_data['channel'] = 'shopify'\n",
    "            daily_datasets.append(shopify_data)\n",
    "            print(f\"   Shopify: {len(shopify_data):,} records\")\n",
    "        \n",
    "        if daily_datasets:\n",
    "            self.daily_data = pd.concat(daily_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"âŒ No daily data available!\")\n",
    "            self.daily_data = pd.DataFrame()\n",
    "            return\n",
    "        \n",
    "        # Combine order data safely (same as original)\n",
    "        order_datasets = []\n",
    "        \n",
    "        if not tiktok__order_items.empty:\n",
    "            tiktok_orders = tiktok__order_items.copy()\n",
    "            tiktok_orders['channel'] = 'tiktok'\n",
    "            order_datasets.append(tiktok_orders)\n",
    "        \n",
    "        if not amazon_order_item_metrics.empty:\n",
    "            amazon_orders = amazon_order_item_metrics.copy()\n",
    "            amazon_orders['channel'] = 'amazon'\n",
    "            order_datasets.append(amazon_orders)\n",
    "        \n",
    "        if not shopify__order_items.empty:\n",
    "            shopify_orders = shopify__order_items.copy()\n",
    "            shopify_orders['channel'] = 'shopify'\n",
    "            order_datasets.append(shopify_orders)\n",
    "        \n",
    "        if order_datasets:\n",
    "            self.order_items = pd.concat(order_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"âš ï¸ No order data available - will use customer-based estimates\")\n",
    "            self.order_items = pd.DataFrame()\n",
    "        \n",
    "        # Clean dates safely (same as original)\n",
    "        print(\"ðŸ“… Cleaning dates...\")\n",
    "        try:\n",
    "            self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "            print(f\"   Daily data date range: {self.daily_data['order_date'].min()} to {self.daily_data['order_date'].max()}\")\n",
    "        except:\n",
    "            print(\"âŒ Error cleaning daily data dates\")\n",
    "            return\n",
    "        \n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "                self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "                self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "                print(f\"   Order data date range: {self.order_items['order_date'].min()} to {self.order_items['order_date'].max()}\")\n",
    "            except:\n",
    "                print(\"âš ï¸ Error cleaning order dates - will use fallback AOV\")\n",
    "                self.order_items = pd.DataFrame()\n",
    "        \n",
    "        print(f\"âœ… Data loaded: {len(self.daily_data):,} daily records, {len(self.order_items):,} order items\")\n",
    "    \n",
    "    def calculate_cohort_returning_rates(self):\n",
    "       \n",
    "        print(\"\\nðŸ“Š Calculating returning rates based on sales fall-off...\")\n",
    "        \n",
    "        if not self.cohort_data:\n",
    "            print(\"   No cohort data available, using default rates\")\n",
    "            return self.calculate_fallback_retention_rates()\n",
    "        \n",
    "        all_channel_rates = {}\n",
    "        \n",
    "        for channel, cohort_df in self.cohort_data.items():\n",
    "            print(f\"\\nðŸ” Processing {channel} cohorts...\")\n",
    "            \n",
    "            # Prepare cohort data\n",
    "            df = cohort_df.copy()\n",
    "            df['customer_cohort'] = pd.to_datetime(df['customer_cohort'])\n",
    "            df['order_month'] = pd.to_datetime(df['order_month'])\n",
    "            \n",
    "            # Calculate customer age in months\n",
    "            df['customer_age_months'] = (\n",
    "                (df['order_month'].dt.year - df['customer_cohort'].dt.year) * 12 +\n",
    "                (df['order_month'].dt.month - df['customer_cohort'].dt.month)\n",
    "            )\n",
    "            \n",
    "            # Filter to reasonable ages (0-12 months)\n",
    "            df = df[(df['customer_age_months'] >= 0) & (df['customer_age_months'] <= 12)]\n",
    "            \n",
    "            # DEBUGGING: Show sample data to understand the pattern\n",
    "            print(f\"   ðŸ“Š Sample cohort sales progression:\")\n",
    "            sample_cohort = df['customer_cohort'].unique()[0]\n",
    "            sample_data = df[df['customer_cohort'] == sample_cohort].sort_values('customer_age_months')\n",
    "            \n",
    "            for _, row in sample_data.head(8).iterrows():\n",
    "                sales_amount = row['total_gross_sales']\n",
    "                print(f\"     Age {row['customer_age_months']:2d}: ${sales_amount:8.2f}\")\n",
    "            \n",
    "            channel_rates = {}\n",
    "            \n",
    "            # Calculate sales retention rate for each age (1-12 months)\n",
    "            for age_months in range(1, 13):\n",
    "                \n",
    "                retention_rates_for_age = []\n",
    "                \n",
    "                # For each cohort, calculate sales retention from initial spend\n",
    "                for cohort_date in df['customer_cohort'].unique():\n",
    "                    cohort_data = df[df['customer_cohort'] == cohort_date]\n",
    "                    \n",
    "                    # Find initial spend (where customer_cohort = order_month, i.e., age 0)\n",
    "                    initial_data = cohort_data[cohort_data['customer_age_months'] == 0]\n",
    "                    if len(initial_data) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    initial_sales = initial_data['total_gross_sales'].sum()\n",
    "                    if initial_sales <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Find current spend at this age\n",
    "                    current_data = cohort_data[cohort_data['customer_age_months'] == age_months]\n",
    "                    if len(current_data) == 0:\n",
    "                        current_sales = 0\n",
    "                    else:\n",
    "                        current_sales = current_data['total_gross_sales'].sum()\n",
    "                    \n",
    "                    # Calculate sales retention: current_sales / initial_sales\n",
    "                    sales_retention = current_sales / initial_sales\n",
    "                    sales_retention = min(sales_retention, 1.0)  # Cap at 100%\n",
    "                    \n",
    "                    # DEBUG: Show calculation for first few cohorts and ages\n",
    "                    if age_months <= 3 and len(retention_rates_for_age) < 3:\n",
    "                        print(f\"     Debug {cohort_date.strftime('%Y-%m')} age {age_months}: ${initial_sales:.0f} â†’ ${current_sales:.0f} = {sales_retention:.1%}\")\n",
    "                    \n",
    "                    retention_rates_for_age.append(sales_retention)\n",
    "                \n",
    "                # Average sales retention rate across all cohorts for this age\n",
    "                if retention_rates_for_age:\n",
    "                    avg_retention_rate = np.mean(retention_rates_for_age)\n",
    "                    channel_rates[age_months] = avg_retention_rate\n",
    "                    print(f\"   {age_months:2d} months: {avg_retention_rate:.1%} sales retention from {len(retention_rates_for_age)} cohorts\")\n",
    "                else:\n",
    "                    # Fallback if no data - ensure it declines\n",
    "                    fallback_rate = max(0.05, 0.75 ** age_months)\n",
    "                    channel_rates[age_months] = fallback_rate\n",
    "                    print(f\"   {age_months:2d} months: {fallback_rate:.1%} sales retention (fallback - no data)\")\n",
    "            \n",
    "            all_channel_rates[channel] = channel_rates\n",
    "        \n",
    "        # Calculate blended retention rates across channels\n",
    "        blended_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            rates_for_age = []\n",
    "            for channel_rates in all_channel_rates.values():\n",
    "                if age_months in channel_rates:\n",
    "                    rates_for_age.append(channel_rates[age_months])\n",
    "            \n",
    "            if rates_for_age:\n",
    "                blended_rates[age_months] = np.mean(rates_for_age)\n",
    "            else:\n",
    "                # Fallback that ensures declining pattern\n",
    "                blended_rates[age_months] = max(0.05, 0.75 ** age_months)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Blended sales retention rates from cohort data:\")\n",
    "        for age in [1, 3, 6, 12]:\n",
    "            if age in blended_rates:\n",
    "                print(f\"   {age:2d} months: {blended_rates[age]:.1%} (vs initial spend)\")\n",
    "        \n",
    "        # SANITY CHECK: Ensure rates generally decline\n",
    "        print(f\"\\nðŸ” Sanity check - sales retention should generally decline:\")\n",
    "        declining_pattern = True\n",
    "        for age in range(1, 12):\n",
    "            curr_rate = blended_rates.get(age, 0)\n",
    "            next_rate = blended_rates.get(age + 1, 0)\n",
    "            if next_rate > curr_rate * 1.3:  # Allow some variation but flag big increases\n",
    "                print(f\"   âš ï¸ WARNING: Month {age} to {age+1} shows unexpected increase: {curr_rate:.1%} â†’ {next_rate:.1%}\")\n",
    "                declining_pattern = False\n",
    "        \n",
    "        if declining_pattern:\n",
    "            print(f\"   âœ… Sales retention pattern looks realistic (generally declining)\")\n",
    "        \n",
    "        return blended_rates\n",
    "    \n",
    "    def calculate_fallback_retention_rates(self):\n",
    "        \"\"\"\n",
    "        Fallback retention calculation (same as original) if no cohort data\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ‘¥ Calculating fallback retention rates...\")\n",
    "        \n",
    "        # Check if we have customer lifecycle data\n",
    "        has_lifecycle_data = ('pct_new_customers' in self.daily_data.columns and \n",
    "                             not self.daily_data['pct_new_customers'].isna().all())\n",
    "        \n",
    "        if has_lifecycle_data:\n",
    "            avg_new_pct = self.daily_data['pct_new_customers'].mean()\n",
    "            \n",
    "            if avg_new_pct > 0.6:  # High churn environment\n",
    "                base_retention = 0.70\n",
    "                print(\"   High acquisition environment detected - using 70% base retention\")\n",
    "            elif avg_new_pct < 0.3:  # Low churn environment\n",
    "                base_retention = 0.80\n",
    "                print(\"   High retention environment detected - using 80% base retention\")\n",
    "            else:\n",
    "                base_retention = 0.75\n",
    "                print(\"   Balanced environment detected - using 75% base retention\")\n",
    "        else:\n",
    "            base_retention = 0.75\n",
    "            print(\"   Using standard 75% base retention\")\n",
    "        \n",
    "        # Calculate retention for each age month\n",
    "        retention_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            retention = base_retention ** age_months\n",
    "            retention = max(0.05, min(0.95, retention))\n",
    "            retention_rates[age_months] = retention\n",
    "        \n",
    "        print(f\"   1-month retention: {retention_rates[1]:.1%}\")\n",
    "        print(f\"   6-month retention: {retention_rates[6]:.1%}\")\n",
    "        print(f\"   12-month retention: {retention_rates[12]:.1%}\")\n",
    "        \n",
    "        return retention_rates\n",
    "    \n",
    "    # Keep all the other methods from the original SimpleRobustBenchmark\n",
    "    def calculate_simple_aov_by_channel(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\nðŸ’° Calculating simple AOV by channel...\")\n",
    "        \n",
    "        channel_aov = {}\n",
    "        \n",
    "        if self.order_items.empty:\n",
    "            print(\"   Using default AOV for all channels\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['sku_gross_sales', 'quantity', 'channel']\n",
    "        missing_cols = [col for col in required_cols if col not in self.order_items.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"   Missing columns: {missing_cols}, using default AOV\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        for channel in ['tiktok', 'amazon', 'shopify']:\n",
    "            print(f\"   Calculating {channel} AOV...\")\n",
    "            \n",
    "            # Filter channel data\n",
    "            channel_orders = self.order_items[self.order_items['channel'] == channel].copy()\n",
    "            \n",
    "            if len(channel_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Clean data simply\n",
    "            clean_orders = channel_orders[\n",
    "                (channel_orders['quantity'] > 0) & \n",
    "                (channel_orders['sku_gross_sales'] > 0) &\n",
    "                (channel_orders['sku_gross_sales'] < 10000) &  # Remove extreme outliers\n",
    "                (channel_orders['quantity'] < 1000)  # Remove bulk orders\n",
    "            ].copy()\n",
    "            \n",
    "            if len(clean_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No clean {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate unit prices\n",
    "            clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "            \n",
    "            # Remove unit price outliers\n",
    "            unit_prices = clean_orders['unit_price']\n",
    "            q1 = unit_prices.quantile(0.25)\n",
    "            q3 = unit_prices.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(1, q1 - 1.5 * iqr)\n",
    "            upper_bound = min(1000, q3 + 1.5 * iqr)\n",
    "            \n",
    "            final_prices = unit_prices[(unit_prices >= lower_bound) & (unit_prices <= upper_bound)]\n",
    "            \n",
    "            if len(final_prices) > 0:\n",
    "                channel_aov[channel] = final_prices.median()\n",
    "                print(f\"     {channel.capitalize()} AOV: ${channel_aov[channel]:.2f} (from {len(final_prices):,} clean orders)\")\n",
    "            else:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No valid {channel} prices, using default ${self.aov_avg}\")\n",
    "        \n",
    "        return channel_aov\n",
    "    \n",
    "    def calculate_simple_seasonal_factors(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\nðŸ—“ï¸ Calculating simple seasonal factors...\")\n",
    "        \n",
    "        # Add temporal columns safely\n",
    "        try:\n",
    "            self.daily_data['month'] = self.daily_data['order_date'].dt.month\n",
    "            self.daily_data['dayofweek'] = self.daily_data['order_date'].dt.dayofweek\n",
    "        except:\n",
    "            print(\"   Error adding temporal columns, using default factors\")\n",
    "            return {\n",
    "                'monthly_factors': {i: 1.0 for i in range(1, 13)},\n",
    "                'dow_factors': {i: 1.0 for i in range(7)}\n",
    "            }\n",
    "        \n",
    "        # Calculate monthly factors simply\n",
    "        monthly_factors = {}\n",
    "        overall_avg = self.daily_data['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            for month in range(1, 13):\n",
    "                month_data = self.daily_data[self.daily_data['month'] == month]\n",
    "                if len(month_data) > 0:\n",
    "                    month_avg = month_data['num_customers'].mean()\n",
    "                    factor = month_avg / overall_avg\n",
    "                    monthly_factors[month] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    monthly_factors[month] = 1.0\n",
    "        else:\n",
    "            monthly_factors = {i: 1.0 for i in range(1, 13)}\n",
    "        \n",
    "        # Calculate day-of-week factors simply\n",
    "        dow_factors = {}\n",
    "        if overall_avg > 0:\n",
    "            for dow in range(7):\n",
    "                dow_data = self.daily_data[self.daily_data['dayofweek'] == dow]\n",
    "                if len(dow_data) > 0:\n",
    "                    dow_avg = dow_data['num_customers'].mean()\n",
    "                    factor = dow_avg / overall_avg\n",
    "                    dow_factors[dow] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    dow_factors[dow] = 1.0\n",
    "        else:\n",
    "            dow_factors = {i: 1.0 for i in range(7)}\n",
    "        \n",
    "        print(f\"   Monthly variation: {max(monthly_factors.values()) - min(monthly_factors.values()):.1%}\")\n",
    "        print(f\"   Weekly variation: {max(dow_factors.values()) - min(dow_factors.values()):.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'monthly_factors': monthly_factors,\n",
    "            'dow_factors': dow_factors\n",
    "        }\n",
    "    \n",
    "    def calculate_robust_benchmark(self):\n",
    "        \"\"\"\n",
    "        Enhanced benchmark calculation using real cohort data for retention rates\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸŽ¯ Calculating enhanced robust benchmark...\")\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"âŒ No daily data available!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get patterns (using cohort data for retention rates!)\n",
    "        channel_aov = self.calculate_simple_aov_by_channel()\n",
    "        seasonal_patterns = self.calculate_simple_seasonal_factors()\n",
    "        retention_rates = self.calculate_cohort_returning_rates()  # âœ… Use real cohort data!\n",
    "        \n",
    "        # Start with daily data\n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Basic customer calculations\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        benchmark_df['total_customers'] = benchmark_df['num_customers']\n",
    "        \n",
    "        # Apply AOV by channel\n",
    "        print(\"   Applying channel-specific AOV...\")\n",
    "        benchmark_df['aov_used'] = benchmark_df['channel'].map(channel_aov).fillna(self.aov_avg)\n",
    "        \n",
    "        # Apply seasonal adjustments\n",
    "        print(\"   Applying seasonal adjustments...\")\n",
    "        benchmark_df['monthly_factor'] = benchmark_df['month'].map(seasonal_patterns['monthly_factors']).fillna(1.0)\n",
    "        benchmark_df['dow_factor'] = benchmark_df['dayofweek'].map(seasonal_patterns['dow_factors']).fillna(1.0)\n",
    "        benchmark_df['seasonal_adjustment'] = (benchmark_df['monthly_factor'] * 0.7 + \n",
    "                                             benchmark_df['dow_factor'] * 0.3)\n",
    "        \n",
    "        # Calculate demand using YOUR EXACT FORMULA\n",
    "        print(\"   Calculating benchmark demand using your formula...\")\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['existing_customer_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # âœ… YOUR EXACT FORMULA: BM_Demand = new_customers * AOV + existing_customers * AOV\n",
    "        benchmark_df['bm_demand'] = (benchmark_df['new_customer_demand'] + \n",
    "                                   benchmark_df['existing_customer_demand'])\n",
    "        \n",
    "        # Calculate actual demand\n",
    "        print(\"   Calculating actual demand...\")\n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                # Try to get actual revenue\n",
    "                actual_revenue = self.order_items.groupby(['product_name', 'order_date', 'channel']).agg({\n",
    "                    'sku_gross_sales': 'sum'\n",
    "                }).reset_index()\n",
    "                \n",
    "                benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                                left_on=['product_name', 'order_date', 'channel'],\n",
    "                                                right_on=['product_name', 'order_date', 'channel'], \n",
    "                                                how='left')\n",
    "                \n",
    "                benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(\n",
    "                    benchmark_df['total_customers'] * benchmark_df['aov_used'])\n",
    "            except:\n",
    "                print(\"     Revenue calculation failed, using customer-based estimate\")\n",
    "                benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        else:\n",
    "            benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # âœ… YOUR EXACT ERROR FORMULA: Actual_Demand - BM_Demand\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add real cohort-based retention rates\n",
    "        print(\"   Adding real cohort-based retention rates...\")\n",
    "        for age_months in range(1, 13):\n",
    "            real_retention = retention_rates[age_months]\n",
    "            # Apply seasonal adjustment\n",
    "            seasonal_retention = real_retention * benchmark_df['seasonal_adjustment']\n",
    "            seasonal_retention = np.clip(seasonal_retention, 0.05, 0.95)\n",
    "            \n",
    "            benchmark_df[f'returning_rate_{age_months}m'] = seasonal_retention\n",
    "            benchmark_df[f'falloff_rate_{age_months}m'] = 1 - seasonal_retention\n",
    "        \n",
    "        # Select final columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'total_customers',\n",
    "            'aov_used', 'new_customer_demand', 'existing_customer_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + [f'returning_rate_{i}m' for i in range(1, 13)] + [f'falloff_rate_{i}m' for i in range(1, 13)]\n",
    "        \n",
    "        # Only keep columns that exist\n",
    "        existing_columns = [col for col in final_columns if col in benchmark_df.columns]\n",
    "        benchmark_df = benchmark_df[existing_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku'}, inplace=True)\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_enhanced_benchmark(self):\n",
    "        \"\"\"\n",
    "        Run the enhanced benchmark with real cohort data\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ RUNNING ENHANCED ROBUST BENCHMARK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"âŒ No data to process!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        benchmark_df = self.calculate_robust_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if benchmark_df.empty:\n",
    "            print(\"âŒ No benchmark results generated!\")\n",
    "            return benchmark_df\n",
    "        \n",
    "        print(f\"\\nðŸ“Š ENHANCED BENCHMARK RESULTS:\")\n",
    "        print(f\"  âš¡ Processed {len(benchmark_df):,} records in {duration:.1f} seconds\")\n",
    "        print(f\"  ðŸ›ï¸ SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  ðŸ“… Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        \n",
    "        # Performance by channel\n",
    "        if 'channel' in benchmark_df.columns:\n",
    "            print(f\"\\nðŸ“º PERFORMANCE BY CHANNEL:\")\n",
    "            for channel in benchmark_df['channel'].unique():\n",
    "                channel_data = benchmark_df[benchmark_df['channel'] == channel]\n",
    "                avg_aov = channel_data['aov_used'].mean()\n",
    "                mape = channel_data['error_percentage'].abs().mean()\n",
    "                records = len(channel_data)\n",
    "                \n",
    "                print(f\"  {channel.capitalize():<8}: {records:,} records, AOV ${avg_aov:.2f}, MAPE {mape:.1f}%\")\n",
    "        \n",
    "        # Overall performance\n",
    "        overall_mape = benchmark_df['error_percentage'].abs().mean()\n",
    "        overall_aov = benchmark_df['aov_used'].mean()\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ OVERALL PERFORMANCE:\")\n",
    "        print(f\"  ðŸ“Š Total MAPE: {overall_mape:.1f}%\")\n",
    "        print(f\"  ðŸ’° Overall Avg AOV: ${overall_aov:.2f}\")\n",
    "        print(f\"  ðŸ“ˆ Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  ðŸ“ˆ Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  âœ… Using REAL cohort data for retention rates!\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_enhanced_robust_benchmark(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                                shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                                tiktok__order_items, shopify__order_items,\n",
    "                                final_amazon_cohorts=None, final_tiktok_cohorts=None, \n",
    "                                final_shopify_cohorts=None, aov_avg=43):\n",
    "    \"\"\"\n",
    "    Enhanced robust benchmark model with real cohort data integration\n",
    "    \"\"\"\n",
    "    print(\"ðŸ›¡ï¸ ENHANCED ROBUST BENCHMARK MODEL\")\n",
    "    print(\"ðŸ”§ Features:\")\n",
    "    print(\"   âœ… Safe data handling\")\n",
    "    print(\"   âœ… Simple operations (no complex groupby)\")\n",
    "    print(\"   âœ… Multiple fallbacks\")\n",
    "    print(\"   âœ… Channel-specific AOV\")\n",
    "    print(\"   âœ… Seasonal adjustments\")\n",
    "    print(\"   âœ… REAL cohort-based retention rates\")\n",
    "    print(\"   âœ… Your exact formulas: BM_Demand = new_customers * AOV + existing_customers * AOV\")\n",
    "    print(\"   âœ… Your exact error: Actual_Demand - BM_Demand\")\n",
    "    \n",
    "    benchmark = EnhancedRobustBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        final_amazon_cohorts=final_amazon_cohorts,\n",
    "        final_tiktok_cohorts=final_tiktok_cohorts,\n",
    "        final_shopify_cohorts=final_shopify_cohorts,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    benchmark_df = benchmark.run_enhanced_benchmark()\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "# USAGE:\n",
    "enhanced_model, enhanced_benchmark_df = run_enhanced_robust_benchmark(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items,\n",
    "    final_amazon_cohorts, final_tiktok_cohorts, final_shopify_cohorts, aov_avg=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4132c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelEvaluationFramework:\n",
    "    def __init__(self, benchmark_df, baseline_model_df=None):\n",
    "        \n",
    "        print(\"ðŸ“Š COMPREHENSIVE MODEL EVALUATION FRAMEWORK\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.benchmark_df = benchmark_df.copy()\n",
    "        self.baseline_df = baseline_model_df.copy() if baseline_model_df is not None else None\n",
    "        \n",
    "        # Clean and prepare data\n",
    "        self.prepare_evaluation_data()\n",
    "        \n",
    "        print(f\"âœ… Loaded benchmark data: {len(self.benchmark_df):,} records\")\n",
    "        if self.baseline_df is not None:\n",
    "            print(f\"âœ… Loaded baseline data: {len(self.baseline_df):,} records\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No baseline model provided - will use naive benchmarks\")\n",
    "    \n",
    "    def prepare_evaluation_data(self):\n",
    "        \"\"\"\n",
    "        Clean and prepare data for evaluation\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ”§ Preparing evaluation data...\")\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['order_date', 'sku', 'actual_demand', 'bm_demand', 'error_metric']\n",
    "        missing_cols = [col for col in required_cols if col not in self.benchmark_df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"âŒ Missing required columns: {missing_cols}\")\n",
    "            return\n",
    "        \n",
    "        # Clean data\n",
    "        self.benchmark_df = self.benchmark_df.dropna(subset=['actual_demand', 'bm_demand'])\n",
    "        self.benchmark_df = self.benchmark_df[self.benchmark_df['actual_demand'] > 0]  # Remove zero demand\n",
    "        \n",
    "        # Add derived metrics\n",
    "        self.benchmark_df['absolute_error'] = np.abs(self.benchmark_df['error_metric'])\n",
    "        self.benchmark_df['percentage_error'] = np.where(\n",
    "            self.benchmark_df['actual_demand'] > 0,\n",
    "            (self.benchmark_df['error_metric'] / self.benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        self.benchmark_df['absolute_percentage_error'] = np.abs(self.benchmark_df['percentage_error'])\n",
    "        \n",
    "        # Add temporal columns\n",
    "        self.benchmark_df['order_date'] = pd.to_datetime(self.benchmark_df['order_date'])\n",
    "        self.benchmark_df['year_month'] = self.benchmark_df['order_date'].dt.to_period('M')\n",
    "        self.benchmark_df['quarter'] = self.benchmark_df['order_date'].dt.to_period('Q')\n",
    "        self.benchmark_df['year'] = self.benchmark_df['order_date'].dt.year\n",
    "        self.benchmark_df['month'] = self.benchmark_df['order_date'].dt.month\n",
    "        \n",
    "        # Calculate relative performance metrics\n",
    "        if 'channel' in self.benchmark_df.columns:\n",
    "            self.benchmark_df['channel'] = self.benchmark_df['channel'].fillna('unknown')\n",
    "        \n",
    "        print(f\"   âœ… Clean records: {len(self.benchmark_df):,}\")\n",
    "        print(f\"   ðŸ“… Date range: {self.benchmark_df['order_date'].min()} to {self.benchmark_df['order_date'].max()}\")\n",
    "        print(f\"   ðŸ›ï¸ Unique SKUs: {self.benchmark_df['sku'].nunique()}\")\n",
    "        if 'channel' in self.benchmark_df.columns:\n",
    "            print(f\"   ðŸ“º Channels: {self.benchmark_df['channel'].unique()}\")\n",
    "    \n",
    "    def calculate_overall_accuracy(self):\n",
    "        \"\"\"\n",
    "        Calculate overall model accuracy metrics\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“ˆ OVERALL ACCURACY METRICS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Basic accuracy metrics\n",
    "        mae = self.benchmark_df['absolute_error'].mean()\n",
    "        mape = self.benchmark_df['absolute_percentage_error'].mean()\n",
    "        rmse = np.sqrt((self.benchmark_df['error_metric'] ** 2).mean())\n",
    "        \n",
    "        # Relative metrics\n",
    "        total_actual = self.benchmark_df['actual_demand'].sum()\n",
    "        total_predicted = self.benchmark_df['bm_demand'].sum()\n",
    "        total_error = self.benchmark_df['error_metric'].sum()\n",
    "        \n",
    "        bias = (total_predicted - total_actual) / total_actual * 100\n",
    "        \n",
    "        # Accuracy scores\n",
    "        accuracy_within_10pct = (self.benchmark_df['absolute_percentage_error'] <= 10).mean() * 100\n",
    "        accuracy_within_20pct = (self.benchmark_df['absolute_percentage_error'] <= 20).mean() * 100\n",
    "        accuracy_within_50pct = (self.benchmark_df['absolute_percentage_error'] <= 50).mean() * 100\n",
    "        \n",
    "        # Directional accuracy\n",
    "        over_predictions = (self.benchmark_df['error_metric'] < 0).sum()\n",
    "        under_predictions = (self.benchmark_df['error_metric'] > 0).sum()\n",
    "        perfect_predictions = (self.benchmark_df['error_metric'] == 0).sum()\n",
    "        \n",
    "        print(f\"ðŸ“Š ACCURACY SUMMARY:\")\n",
    "        print(f\"   Mean Absolute Error (MAE):     ${mae:,.2f}\")\n",
    "        print(f\"   Mean Absolute % Error (MAPE):  {mape:.1f}%\")\n",
    "        print(f\"   Root Mean Square Error (RMSE): ${rmse:,.2f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"ðŸ“ˆ BIAS ANALYSIS:\")\n",
    "        print(f\"   Overall Bias:                  {bias:+.1f}% ({'Over' if bias > 0 else 'Under'}-prediction)\")\n",
    "        print(f\"   Total Actual Demand:           ${total_actual:,.2f}\")\n",
    "        print(f\"   Total Predicted Demand:        ${total_predicted:,.2f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"ðŸŽ¯ PREDICTION ACCURACY:\")\n",
    "        print(f\"   Within 10%:  {accuracy_within_10pct:.1f}% of predictions\")\n",
    "        print(f\"   Within 20%:  {accuracy_within_20pct:.1f}% of predictions\")\n",
    "        print(f\"   Within 50%:  {accuracy_within_50pct:.1f}% of predictions\")\n",
    "        print(f\"\")\n",
    "        print(f\"ðŸ“Š PREDICTION DIRECTION:\")\n",
    "        print(f\"   Over-predictions:  {over_predictions:,} ({over_predictions/len(self.benchmark_df)*100:.1f}%)\")\n",
    "        print(f\"   Under-predictions: {under_predictions:,} ({under_predictions/len(self.benchmark_df)*100:.1f}%)\")\n",
    "        print(f\"   Perfect predictions: {perfect_predictions:,} ({perfect_predictions/len(self.benchmark_df)*100:.1f}%)\")\\\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'rmse': rmse,\n",
    "            'bias': bias,\n",
    "            'accuracy_10pct': accuracy_within_10pct,\n",
    "            'accuracy_20pct': accuracy_within_20pct,\n",
    "            'accuracy_50pct': accuracy_within_50pct\n",
    "        }\n",
    "    \n",
    "    def calculate_weighted_accuracy(self):\n",
    "        \"\"\"\n",
    "        Calculate accuracy weighted by product importance (revenue/volume)\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ’° WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate product weights by total revenue\n",
    "        product_revenue = self.benchmark_df.groupby('sku')['actual_demand'].sum().sort_values(ascending=False)\n",
    "        total_revenue = product_revenue.sum()\n",
    "        product_weights = product_revenue / total_revenue\n",
    "        \n",
    "        # Calculate weighted errors\n",
    "        sku_errors = self.benchmark_df.groupby('sku').agg({\n",
    "            'absolute_percentage_error': 'mean',\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        sku_errors['weight'] = sku_errors['sku'].map(product_weights)\n",
    "        sku_errors = sku_errors.dropna()\n",
    "        \n",
    "        # Weighted metrics\n",
    "        weighted_mape = (sku_errors['absolute_percentage_error'] * sku_errors['weight']).sum()\n",
    "        weighted_mae = (sku_errors['absolute_error'] * sku_errors['weight']).sum()\n",
    "        \n",
    "        # Top product analysis\n",
    "        top_10_skus = product_revenue.head(10)\n",
    "        top_10_performance = self.benchmark_df[self.benchmark_df['sku'].isin(top_10_skus.index)]\n",
    "        top_10_mape = top_10_performance['absolute_percentage_error'].mean()\n",
    "        \n",
    "        bottom_skus = product_revenue.tail(len(product_revenue)//2)  # Bottom 50%\n",
    "        bottom_performance = self.benchmark_df[self.benchmark_df['sku'].isin(bottom_skus.index)]\n",
    "        bottom_mape = bottom_performance['absolute_percentage_error'].mean()\n",
    "        \n",
    "        print(f\"ðŸ’Ž WEIGHTED PERFORMANCE:\")\n",
    "        print(f\"   Revenue-Weighted MAPE:    {weighted_mape:.1f}%\")\n",
    "        print(f\"   Revenue-Weighted MAE:     ${weighted_mae:,.2f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"ðŸ† TOP 10 PRODUCTS (by revenue):\")\n",
    "        print(f\"   MAPE:                     {top_10_mape:.1f}%\")\n",
    "        print(f\"   Revenue Share:            {top_10_skus.sum()/total_revenue*100:.1f}%\")\n",
    "        print(f\"\")\n",
    "        print(f\"ðŸ“‰ BOTTOM 50% PRODUCTS:\")\n",
    "        print(f\"   MAPE:                     {bottom_mape:.1f}%\")\n",
    "        print(f\"   Revenue Share:            {bottom_skus.sum()/total_revenue*100:.1f}%\")\n",
    "        print(f\"\")\n",
    "        print(f\"ðŸ“Š PERFORMANCE SPREAD:\")\n",
    "        print(f\"   Top vs Bottom Difference: {top_10_mape - bottom_mape:+.1f}% MAPE\")\\\n",
    "        \n",
    "        return {\n",
    "            'weighted_mape': weighted_mape,\n",
    "            'weighted_mae': weighted_mae,\n",
    "            'top_10_mape': top_10_mape,\n",
    "            'bottom_50_mape': bottom_mape,\n",
    "            'top_products': top_10_skus\n",
    "        }\n",
    "    \n",
    "    def analyze_channel_performance(self):\n",
    "        \"\"\"\n",
    "        Analyze model performance across different channels\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ“º CHANNEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'channel' not in self.benchmark_df.columns:\n",
    "            print(\"   âŒ No channel data available\")\n",
    "            return {}\n",
    "        \n",
    "        channel_performance = self.benchmark_df.groupby('channel').agg({\n",
    "            'absolute_percentage_error': ['mean', 'std', 'count'],\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': 'sum',\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        channel_performance.columns = ['mape', 'mape_std', 'record_count', 'mae', 'actual_total', 'predicted_total', 'total_error']\n",
    "        channel_performance = channel_performance.reset_index()\n",
    "        \n",
    "        # Calculate channel bias\n",
    "        channel_performance['bias_pct'] = (\n",
    "            (channel_performance['predicted_total'] - channel_performance['actual_total']) / \n",
    "            channel_performance['actual_total'] * 100\n",
    "        )\n",
    "        \n",
    "        # Revenue share\n",
    "        total_revenue = channel_performance['actual_total'].sum()\n",
    "        channel_performance['revenue_share'] = channel_performance['actual_total'] / total_revenue * 100\n",
    "        \n",
    "        # Sort by performance\n",
    "        channel_performance = channel_performance.sort_values('mape')\n",
    "        \n",
    "        print(\"ðŸ“Š CHANNEL RANKINGS (by MAPE):\")\n",
    "        for _, row in channel_performance.iterrows():\n",
    "            print(f\"   {row['channel'].capitalize():<10}: {row['mape']:.1f}% MAPE, {row['bias_pct']:+.1f}% bias, {row['revenue_share']:.1f}% revenue share\")\n",
    "        \n",
    "        best_channel = channel_performance.iloc[0]['channel']\n",
    "        worst_channel = channel_performance.iloc[-1]['channel']\n",
    "        performance_gap = channel_performance.iloc[-1]['mape'] - channel_performance.iloc[0]['mape']\n",
    "        \n",
    "        print(f\"\")\n",
    "        print(f\"ðŸ† CHANNEL INSIGHTS:\")\n",
    "        print(f\"   Best Performing:  {best_channel.capitalize()} ({channel_performance.iloc[0]['mape']:.1f}% MAPE)\")\n",
    "        print(f\"   Worst Performing: {worst_channel.capitalize()} ({channel_performance.iloc[-1]['mape']:.1f}% MAPE)\")\n",
    "        print(f\"   Performance Gap:  {performance_gap:.1f}% MAPE difference\")\\\n",
    "        \n",
    "        return channel_performance.to_dict('records')\n",
    "    \n",
    "    def analyze_sku_performance(self):\n",
    "        \"\"\"\n",
    "        Analyze model performance for top SKUs\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ›ï¸ SKU PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Calculate SKU-level performance\n",
    "        sku_performance = self.benchmark_df.groupby('sku').agg({\n",
    "            'absolute_percentage_error': 'mean',\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': ['sum', 'count'],\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        sku_performance.columns = ['mape', 'mae', 'total_revenue', 'record_count', 'predicted_total', 'total_error']\n",
    "        sku_performance = sku_performance.reset_index()\n",
    "        \n",
    "        # Calculate bias\n",
    "        sku_performance['bias_pct'] = (\n",
    "            (sku_performance['predicted_total'] - sku_performance['total_revenue']) / \n",
    "            sku_performance['total_revenue'] * 100\n",
    "        )\n",
    "        \n",
    "        # Filter to SKUs with significant volume (top 50% by revenue)\n",
    "        revenue_threshold = sku_performance['total_revenue'].quantile(0.5)\n",
    "        significant_skus = sku_performance[sku_performance['total_revenue'] >= revenue_threshold]\n",
    "        \n",
    "        # Top and bottom performers\n",
    "        top_10_accurate = significant_skus.nsmallest(10, 'mape')\n",
    "        bottom_10_accurate = significant_skus.nlargest(10, 'mape')\n",
    "        \n",
    "        print(\"ðŸŽ¯ TOP 10 MOST ACCURATE SKUs (among significant SKUs):\")\n",
    "        for _, row in top_10_accurate.iterrows():\n",
    "            sku_short = row['sku'][:20] + \"...\" if len(row['sku']) > 20 else row['sku']\n",
    "            print(f\"   {sku_short:<25}: {row['mape']:.1f}% MAPE, ${row['total_revenue']:,.0f} revenue\")\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"âŒ BOTTOM 10 LEAST ACCURATE SKUs (among significant SKUs):\")\n",
    "        for _, row in bottom_10_accurate.iterrows():\n",
    "            sku_short = row['sku'][:20] + \"...\" if len(row['sku']) > 20 else row['sku']\n",
    "            print(f\"   {sku_short:<25}: {row['mape']:.1f}% MAPE, ${row['total_revenue']:,.0f} revenue\")\n",
    "        \n",
    "        # Summary stats\n",
    "        avg_mape_all = sku_performance['mape'].mean()\n",
    "        avg_mape_significant = significant_skus['mape'].mean()\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"ðŸ“Š SKU PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   Average MAPE (all SKUs):        {avg_mape_all:.1f}%\")\n",
    "        print(f\"   Average MAPE (significant SKUs): {avg_mape_significant:.1f}%\")\n",
    "        print(f\"   SKUs analyzed:                  {len(sku_performance):,}\")\n",
    "        print(f\"   Significant SKUs:               {len(significant_skus):,}\")\\\n",
    "        \n",
    "        return {\n",
    "            'top_accurate': top_10_accurate.to_dict('records'),\n",
    "            'bottom_accurate': bottom_10_accurate.to_dict('records'),\n",
    "            'avg_mape_all': avg_mape_all,\n",
    "            'avg_mape_significant': avg_mape_significant\n",
    "        }\n",
    "    \n",
    "    def analyze_temporal_performance(self):\n",
    "        \"\"\"\n",
    "        Analyze model performance over time\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ“… TEMPORAL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Monthly performance\n",
    "        monthly_performance = self.benchmark_df.groupby('year_month').agg({\n",
    "            'absolute_percentage_error': 'mean',\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': 'sum',\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        monthly_performance['bias_pct'] = (\n",
    "            (monthly_performance['bm_demand'] - monthly_performance['actual_demand']) / \n",
    "            monthly_performance['actual_demand'] * 100\n",
    "        )\n",
    "        \n",
    "        # Trend analysis\n",
    "        monthly_performance['month_num'] = range(len(monthly_performance))\n",
    "        mape_trend = np.corrcoef(monthly_performance['month_num'], monthly_performance['absolute_percentage_error'])[0,1]\n",
    "        \n",
    "        # Best and worst months\n",
    "        best_month = monthly_performance.loc[monthly_performance['absolute_percentage_error'].idxmin()]\n",
    "        worst_month = monthly_performance.loc[monthly_performance['absolute_percentage_error'].idxmax()]\n",
    "        \n",
    "        # Consistency metrics\n",
    "        mape_std = monthly_performance['absolute_percentage_error'].std()\n",
    "        mape_cv = mape_std / monthly_performance['absolute_percentage_error'].mean()  # Coefficient of variation\n",
    "        \n",
    "        print(\"ðŸ“ˆ TEMPORAL TRENDS:\")\n",
    "        print(f\"   MAPE Trend Correlation:  {mape_trend:+.3f} ({'Improving' if mape_trend < 0 else 'Deteriorating'} over time)\")\n",
    "        print(f\"   MAPE Standard Deviation: {mape_std:.1f}%\")\n",
    "        print(f\"   MAPE Consistency (CV):   {mape_cv:.2f} ({'Consistent' if mape_cv < 0.3 else 'Variable'})\")\n",
    "        print(\"\")\n",
    "        print(f\"ðŸ† BEST MONTH:  {best_month['year_month']} ({best_month['absolute_percentage_error']:.1f}% MAPE)\")\n",
    "        print(f\"âŒ WORST MONTH: {worst_month['year_month']} ({worst_month['absolute_percentage_error']:.1f}% MAPE)\")\n",
    "        print(\"\")\n",
    "        print(f\"ðŸ“Š PERFORMANCE RANGE: {worst_month['absolute_percentage_error'] - best_month['absolute_percentage_error']:.1f}% MAPE spread\")\\\n",
    "        \n",
    "        return {\n",
    "            'monthly_performance': monthly_performance,\n",
    "            'mape_trend': mape_trend,\n",
    "            'best_month': best_month,\n",
    "            'worst_month': worst_month,\n",
    "            'consistency': mape_cv\n",
    "        }\n",
    "    \n",
    "    def cross_validation_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform time-series cross validation analysis\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ”„ CROSS-VALIDATION ANALYSIS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Sort by date for time series CV\n",
    "        df_sorted = self.benchmark_df.sort_values('order_date')\n",
    "        \n",
    "        # Define train/test splits (expanding window)\n",
    "        months = sorted(df_sorted['year_month'].unique())\n",
    "        \n",
    "        if len(months) < 6:\n",
    "            print(\"   âŒ Need at least 6 months of data for cross-validation\")\n",
    "            return {}\n",
    "        \n",
    "        cv_results = []\n",
    "        min_train_months = 3\n",
    "        \n",
    "        for i in range(min_train_months, len(months)):\n",
    "            train_months = months[:i]\n",
    "            test_month = months[i]\n",
    "            \n",
    "            train_data = df_sorted[df_sorted['year_month'].isin(train_months)]\n",
    "            test_data = df_sorted[df_sorted['year_month'] == test_month]\n",
    "            \n",
    "            if len(test_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate baseline (simple average from training data)\n",
    "            baseline_avg = train_data['actual_demand'].mean()\n",
    "            \n",
    "            # Performance metrics for test month\n",
    "            test_mape = test_data['absolute_percentage_error'].mean()\n",
    "            test_mae = test_data['absolute_error'].mean()\n",
    "            \n",
    "            # Baseline comparison (naive forecast = training average)\n",
    "            baseline_errors = np.abs(test_data['actual_demand'] - baseline_avg)\n",
    "            baseline_mape = (baseline_errors / test_data['actual_demand']).mean() * 100\n",
    "            \n",
    "            model_beats_baseline = test_mape < baseline_mape\n",
    "            \n",
    "            cv_results.append({\n",
    "                'test_month': test_month,\n",
    "                'train_months': len(train_months),\n",
    "                'test_records': len(test_data),\n",
    "                'model_mape': test_mape,\n",
    "                'baseline_mape': baseline_mape,\n",
    "                'improvement': baseline_mape - test_mape,\n",
    "                'beats_baseline': model_beats_baseline\n",
    "            })\n",
    "        \n",
    "        cv_df = pd.DataFrame(cv_results)\n",
    "        \n",
    "        # Summary statistics\n",
    "        avg_improvement = cv_df['improvement'].mean()\n",
    "        win_rate = cv_df['beats_baseline'].mean() * 100\n",
    "        consistent_months = cv_df['beats_baseline'].sum()\n",
    "        total_months = len(cv_df)\n",
    "        \n",
    "        print(\"ðŸŽ¯ CROSS-VALIDATION RESULTS:\")\n",
    "        print(f\"   Months Tested:           {total_months}\")\n",
    "        print(f\"   Win Rate vs Baseline:    {win_rate:.1f}% ({consistent_months}/{total_months} months)\")\n",
    "        print(f\"   Average Improvement:     {avg_improvement:+.1f}% MAPE vs naive baseline\")\n",
    "        print(\"\")\n",
    "        print(\"ðŸ“Š MONTH-BY-MONTH PERFORMANCE:\")\n",
    "        \n",
    "        for _, row in cv_df.tail(12).iterrows():  # Show last 12 months\n",
    "            status = \"âœ… WIN\" if row['beats_baseline'] else \"âŒ LOSS\"\n",
    "            print(f\"   {row['test_month']}: {row['model_mape']:.1f}% vs {row['baseline_mape']:.1f}% baseline = {row['improvement']:+.1f}% {status}\")\\\n",
    "        \n",
    "        return cv_df\n",
    "    \n",
    "    def cumulative_error_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze cumulative error impact over time\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ“ˆ CUMULATIVE ERROR ANALYSIS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Monthly aggregated data\n",
    "        monthly_data = self.benchmark_df.groupby('year_month').agg({\n",
    "            'actual_demand': 'sum',\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Calculate cumulative metrics\n",
    "        monthly_data['cumulative_actual'] = monthly_data['actual_demand'].cumsum()\n",
    "        monthly_data['cumulative_predicted'] = monthly_data['bm_demand'].cumsum()\n",
    "        monthly_data['cumulative_error'] = monthly_data['error_metric'].cumsum()\n",
    "        monthly_data['cumulative_error_pct'] = (monthly_data['cumulative_error'] / monthly_data['cumulative_actual']) * 100\n",
    "        \n",
    "        # If following model for full year projection\n",
    "        last_month_error_rate = monthly_data['error_metric'].iloc[-1] / monthly_data['actual_demand'].iloc[-1]\n",
    "        avg_monthly_demand = monthly_data['actual_demand'].mean()\n",
    "        \n",
    "        # Project annual impact\n",
    "        annual_error_projection = last_month_error_rate * avg_monthly_demand * 12\n",
    "        annual_demand_projection = avg_monthly_demand * 12\n",
    "        annual_error_pct = (annual_error_projection / annual_demand_projection) * 100\n",
    "        \n",
    "        # Final cumulative error\n",
    "        final_cumulative_error = monthly_data['cumulative_error_pct'].iloc[-1]\n",
    "        \n",
    "        print(\"ðŸ’° CUMULATIVE IMPACT ANALYSIS:\")\n",
    "        print(\"\")\n",
    "        print(\"ðŸ“Š CURRENT CUMULATIVE PERFORMANCE:\")\n",
    "        print(f\"   Total Actual Demand:     ${monthly_data['cumulative_actual'].iloc[-1]:,.0f}\")\n",
    "        print(f\"   Total Predicted Demand:  ${monthly_data['cumulative_predicted'].iloc[-1]:,.0f}\")\n",
    "        print(f\"   Cumulative Error:        ${monthly_data['cumulative_error'].iloc[-1]:,.0f}\")\n",
    "        print(f\"   Cumulative Error %:      {final_cumulative_error:+.1f}%\")\n",
    "        print(\"\")\n",
    "        print(\"ðŸ”® ANNUAL PROJECTION (if following model):\")\n",
    "        print(f\"   Projected Annual Demand: ${annual_demand_projection:,.0f}\")\n",
    "        print(f\"   Projected Annual Error:  ${annual_error_projection:,.0f}\")\n",
    "        print(f\"   Projected Error Rate:    {annual_error_pct:+.1f}%\")\n",
    "        print(\"\")\n",
    "        print(\"âš ï¸ RISK ASSESSMENT:\")\n",
    "        \n",
    "        if abs(annual_error_pct) < 5:\n",
    "            risk_level = \"ðŸŸ¢ LOW RISK\"\n",
    "        elif abs(annual_error_pct) < 15:\n",
    "            risk_level = \"ðŸŸ¡ MODERATE RISK\"\n",
    "        else:\n",
    "            risk_level = \"ðŸ”´ HIGH RISK\"\n",
    "        \n",
    "        print(f\"   {risk_level} - {abs(annual_error_pct):.1f}% projected annual error\")\n",
    "        \n",
    "        # Show monthly progression\n",
    "        print(\"\")\n",
    "        print(\"ðŸ“… MONTHLY ERROR PROGRESSION:\")\n",
    "        for _, row in monthly_data.tail(6).iterrows():\n",
    "            print(f\"   {row['year_month']}: {row['cumulative_error_pct']:+.1f}% cumulative error\")\\\n",
    "        \n",
    "        return {\n",
    "            'monthly_data': monthly_data,\n",
    "            'annual_projection': annual_error_pct,\n",
    "            'cumulative_error': final_cumulative_error,\n",
    "            'risk_level': risk_level\n",
    "        }\n",
    "    \n",
    "    def run_comprehensive_evaluation(self):\n",
    "        \"\"\"\n",
    "        Run all evaluation analyses\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ RUNNING COMPREHENSIVE MODEL EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            results['overall'] = self.calculate_overall_accuracy()\n",
    "            results['weighted'] = self.calculate_weighted_accuracy()\n",
    "            results['channels'] = self.analyze_channel_performance()\n",
    "            results['skus'] = self.analyze_sku_performance()\n",
    "            results['temporal'] = self.analyze_temporal_performance()\n",
    "            results['cross_validation'] = self.cross_validation_analysis()\n",
    "            results['cumulative'] = self.cumulative_error_analysis()\n",
    "            \n",
    "            print(f\"\\nðŸŽ‰ EVALUATION COMPLETE!\")\n",
    "            print(f\"=\"*60)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during evaluation: {str(e)}\")\n",
    "            return results\n",
    "\n",
    "# Usage function\n",
    "def evaluate_model_performance(benchmark_df, baseline_df=None):\n",
    "    \"\"\"\n",
    "    Main function to evaluate model performance comprehensively\n",
    "    \n",
    "    Args:\n",
    "        benchmark_df: Your enhanced benchmark model results\n",
    "        baseline_df: Optional baseline model for comparison\n",
    "    \n",
    "    Returns:\n",
    "        ModelEvaluationFramework instance with all results\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluator = ModelEvaluationFramework(benchmark_df, baseline_df)\n",
    "    results = evaluator.run_comprehensive_evaluation()\n",
    "    \n",
    "    return evaluator, results\n",
    "\n",
    "# To use:\n",
    "evaluator, evaluation_results = evaluate_model_performance(enhanced_benchmark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2056b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MLModelEvaluationAnalysis:\n",
    "    def __init__(self, test_data, model_results, benchmark_col='bm_demand', actual_col='actual_demand'):\n",
    "        \n",
    "        print(\"ðŸ” COMPREHENSIVE ML MODEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.test_data = test_data.copy()\n",
    "        self.model_results = model_results\n",
    "        self.benchmark_col = benchmark_col\n",
    "        self.actual_col = actual_col\n",
    "        \n",
    "        self.add_ml_predictions_to_test_data()\n",
    "        \n",
    "        self.prepare_analysis_data()\n",
    "        \n",
    "    def add_ml_predictions_to_test_data(self):\n",
    "        \n",
    "        print(\"\\nðŸ“Š Adding ML predictions to test data...\")\n",
    "        \n",
    "        for model_name, results in self.model_results.items():\n",
    "            if 'predictions' in results:\n",
    "                # Ensure we have the right number of predictions\n",
    "                predictions = results['predictions']\n",
    "                if len(predictions) == len(self.test_data):\n",
    "                    self.test_data[f'{model_name}_pred'] = predictions\n",
    "                    print(f\"   âœ… Added {model_name} predictions\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ {model_name} prediction length mismatch: {len(predictions)} vs {len(self.test_data)}\")\n",
    "    \n",
    "    def prepare_analysis_data(self):\n",
    "        \"\"\"\n",
    "        Prepare data for comprehensive analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ”§ Preparing analysis data...\")\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        if self.actual_col not in self.test_data.columns:\n",
    "            print(f\"âŒ Missing actual demand column: {self.actual_col}\")\n",
    "            return\n",
    "        \n",
    "        if self.benchmark_col not in self.test_data.columns:\n",
    "            print(f\"âŒ Missing benchmark column: {self.benchmark_col}\")\n",
    "            return\n",
    "        \n",
    "        # Add temporal columns\n",
    "        if 'order_date' in self.test_data.columns:\n",
    "            self.test_data['order_date'] = pd.to_datetime(self.test_data['order_date'])\n",
    "            self.test_data['year_month'] = self.test_data['order_date'].dt.to_period('M')\n",
    "            self.test_data['month'] = self.test_data['order_date'].dt.month\n",
    "            self.test_data['quarter'] = self.test_data['order_date'].dt.quarter\n",
    "        \n",
    "        # Calculate benchmark errors\n",
    "        self.test_data['benchmark_error'] = self.test_data[self.actual_col] - self.test_data[self.benchmark_col]\n",
    "        self.test_data['benchmark_abs_error'] = np.abs(self.test_data['benchmark_error'])\n",
    "        self.test_data['benchmark_pct_error'] = np.where(\n",
    "            self.test_data[self.actual_col] > 0,\n",
    "            (self.test_data['benchmark_error'] / self.test_data[self.actual_col]) * 100,\n",
    "            0\n",
    "        )\n",
    "        self.test_data['benchmark_abs_pct_error'] = np.abs(self.test_data['benchmark_pct_error'])\n",
    "        \n",
    "        # Calculate ML model errors\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            self.test_data[f'{model_name}_error'] = self.test_data[self.actual_col] - self.test_data[pred_col]\n",
    "            self.test_data[f'{model_name}_abs_error'] = np.abs(self.test_data[f'{model_name}_error'])\n",
    "            self.test_data[f'{model_name}_pct_error'] = np.where(\n",
    "                self.test_data[self.actual_col] > 0,\n",
    "                (self.test_data[f'{model_name}_error'] / self.test_data[self.actual_col]) * 100,\n",
    "                0\n",
    "            )\n",
    "            self.test_data[f'{model_name}_abs_pct_error'] = np.abs(self.test_data[f'{model_name}_pct_error'])\n",
    "        \n",
    "        print(f\"   âœ… Analysis data prepared: {len(self.test_data):,} records\")\n",
    "        print(f\"   ðŸ“Š ML models found: {len(ml_pred_cols)}\")\n",
    "        \n",
    "    def analyze_overall_accuracy(self):\n",
    "        \"\"\"\n",
    "        1. Overall Accuracy Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“ˆ 1. OVERALL ACCURACY ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Get ML model prediction columns\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        if not ml_pred_cols:\n",
    "            print(\"âŒ No ML model predictions found\")\n",
    "            return {}\n",
    "        \n",
    "        accuracy_results = {}\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        benchmark_mape = self.test_data['benchmark_abs_pct_error'].mean()\n",
    "        \n",
    "        print(f\"ðŸŽ¯ BENCHMARK PERFORMANCE:\")\n",
    "        print(f\"   MAE:  ${benchmark_mae:,.2f}\")\n",
    "        print(f\"   MAPE: {benchmark_mape:.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        accuracy_results['benchmark'] = {'mae': benchmark_mae, 'mape': benchmark_mape}\n",
    "        \n",
    "        # ML model performance\n",
    "        print(f\"ðŸ¤– ML MODEL PERFORMANCE:\")\n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            ml_mae = self.test_data[f'{model_name}_abs_error'].mean()\n",
    "            ml_mape = self.test_data[f'{model_name}_abs_pct_error'].mean()\n",
    "            \n",
    "            # Improvement calculations\n",
    "            mae_improvement = benchmark_mae - ml_mae\n",
    "            mae_improvement_pct = (mae_improvement / benchmark_mae) * 100\n",
    "            \n",
    "            mape_improvement = benchmark_mape - ml_mape\n",
    "            \n",
    "            status = \"âœ… BETTER\" if mae_improvement > 0 else \"âŒ WORSE\"\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            print(f\"     MAE:  ${ml_mae:,.2f} (${mae_improvement:+,.2f}, {mae_improvement_pct:+.1f}%) {status}\")\n",
    "            print(f\"     MAPE: {ml_mape:.1f}% ({mape_improvement:+.1f}%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            accuracy_results[model_name] = {\n",
    "                'mae': ml_mae,\n",
    "                'mape': ml_mape,\n",
    "                'mae_improvement': mae_improvement,\n",
    "                'mae_improvement_pct': mae_improvement_pct,\n",
    "                'mape_improvement': mape_improvement\n",
    "            }\n",
    "        \n",
    "        return accuracy_results\n",
    "    \n",
    "    def analyze_weighted_accuracy(self):\n",
    "        \"\"\"\n",
    "        2. Weighted Accuracy by Product Revenue\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ’° 2. WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        if 'sku' not in self.test_data.columns:\n",
    "            print(\"âŒ No SKU column found for product analysis\")\n",
    "            return {}\n",
    "        \n",
    "        # Calculate product weights by revenue\n",
    "        product_revenue = self.test_data.groupby('sku')[self.actual_col].sum().sort_values(ascending=False)\n",
    "        total_revenue = product_revenue.sum()\n",
    "        product_weights = product_revenue / total_revenue\n",
    "        \n",
    "        # Top products analysis\n",
    "        top_20_pct_threshold = product_revenue.quantile(0.8)\n",
    "        top_products = product_revenue[product_revenue >= top_20_pct_threshold].index\n",
    "        \n",
    "        top_product_data = self.test_data[self.test_data['sku'].isin(top_products)]\n",
    "        bottom_product_data = self.test_data[~self.test_data['sku'].isin(top_products)]\n",
    "        \n",
    "        print(f\"ðŸ“Š PRODUCT SEGMENTATION:\")\n",
    "        print(f\"   Top 20% Products: {len(top_products)} SKUs, ${product_revenue[product_revenue >= top_20_pct_threshold].sum():,.0f} revenue ({product_revenue[product_revenue >= top_20_pct_threshold].sum()/total_revenue*100:.1f}%)\")\n",
    "        print(f\"   Bottom 80% Products: {len(product_revenue) - len(top_products)} SKUs, ${product_revenue[product_revenue < top_20_pct_threshold].sum():,.0f} revenue ({product_revenue[product_revenue < top_20_pct_threshold].sum()/total_revenue*100:.1f}%)\")\n",
    "        print(\"\")\n",
    "        \n",
    "        weighted_results = {}\n",
    "        \n",
    "        # Analyze performance on top vs bottom products\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for segment_name, segment_data in [('Top 20% Products', top_product_data), ('Bottom 80% Products', bottom_product_data)]:\n",
    "            print(f\"ðŸŽ¯ {segment_name.upper()}:\")\n",
    "            \n",
    "            # Benchmark performance\n",
    "            bench_mae = segment_data['benchmark_abs_error'].mean()\n",
    "            bench_mape = segment_data['benchmark_abs_pct_error'].mean()\n",
    "            print(f\"   Benchmark: MAE ${bench_mae:.2f}, MAPE {bench_mape:.1f}%\")\n",
    "            \n",
    "            segment_results = {'benchmark': {'mae': bench_mae, 'mape': bench_mape}}\n",
    "            \n",
    "            # ML model performance\n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = segment_data[f'{model_name}_abs_error'].mean()\n",
    "                ml_mape = segment_data[f'{model_name}_abs_pct_error'].mean()\n",
    "                \n",
    "                mae_improvement = bench_mae - ml_mae\n",
    "                mae_improvement_pct = (mae_improvement / bench_mae) * 100\n",
    "                \n",
    "                status = \"âœ…\" if mae_improvement > 0 else \"âŒ\"\n",
    "                print(f\"   {model_name}: MAE ${ml_mae:.2f} ({mae_improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                segment_results[model_name] = {\n",
    "                    'mae': ml_mae,\n",
    "                    'mape': ml_mape,\n",
    "                    'improvement_pct': mae_improvement_pct\n",
    "                }\n",
    "            \n",
    "            weighted_results[segment_name] = segment_results\n",
    "            print(\"\")\n",
    "        \n",
    "        return weighted_results\n",
    "    \n",
    "    def analyze_channel_performance(self):\n",
    "        \"\"\"\n",
    "        3. Channel Performance Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“º 3. CHANNEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'channel' not in self.test_data.columns:\n",
    "            print(\"âŒ No channel column found\")\n",
    "            return {}\n",
    "        \n",
    "        channel_results = {}\n",
    "        channels = self.test_data['channel'].unique()\n",
    "        \n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for channel in channels:\n",
    "            channel_data = self.test_data[self.test_data['channel'] == channel]\n",
    "            \n",
    "            print(f\"ðŸ“» {channel.upper()} CHANNEL:\")\n",
    "            print(f\"   Records: {len(channel_data):,}\")\n",
    "            print(f\"   Revenue: ${channel_data[self.actual_col].sum():,.0f}\")\n",
    "            \n",
    "            # Benchmark performance\n",
    "            bench_mae = channel_data['benchmark_abs_error'].mean()\n",
    "            bench_mape = channel_data['benchmark_abs_pct_error'].mean()\n",
    "            print(f\"   Benchmark: MAE ${bench_mae:.2f}, MAPE {bench_mape:.1f}%\")\n",
    "            \n",
    "            channel_results[channel] = {'benchmark': {'mae': bench_mae, 'mape': bench_mape}}\n",
    "            \n",
    "            # ML model performance  \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = channel_data[f'{model_name}_abs_error'].mean()\n",
    "                ml_mape = channel_data[f'{model_name}_abs_pct_error'].mean()\n",
    "                \n",
    "                mae_improvement = bench_mae - ml_mae\n",
    "                mae_improvement_pct = (mae_improvement / bench_mae) * 100\n",
    "                \n",
    "                status = \"âœ… BETTER\" if mae_improvement > 0 else \"âŒ WORSE\"\n",
    "                print(f\"   {model_name}: MAE ${ml_mae:.2f} ({mae_improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                channel_results[channel][model_name] = {\n",
    "                    'mae': ml_mae,\n",
    "                    'mape': ml_mape,\n",
    "                    'improvement_pct': mae_improvement_pct\n",
    "                }\n",
    "            \n",
    "            print(\"\")\n",
    "        \n",
    "        return channel_results\n",
    "    \n",
    "    def analyze_temporal_performance(self):\n",
    "        \"\"\"\n",
    "        4. Temporal Performance Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“… 4. TEMPORAL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'year_month' not in self.test_data.columns:\n",
    "            print(\"âŒ No temporal data available\")\n",
    "            return {}\n",
    "        \n",
    "        # Monthly performance analysis\n",
    "        monthly_data = self.test_data.groupby('year_month').agg({\n",
    "            self.actual_col: 'sum',\n",
    "            self.benchmark_col: 'sum',\n",
    "            'benchmark_abs_error': 'mean',\n",
    "            'benchmark_abs_pct_error': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Add ML model monthly performance\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            monthly_ml = self.test_data.groupby('year_month').agg({\n",
    "                f'{model_name}_abs_error': 'mean',\n",
    "                f'{model_name}_abs_pct_error': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            monthly_data = monthly_data.join(monthly_ml)\n",
    "        \n",
    "        # Calculate month-by-month wins/losses\n",
    "        temporal_results = {}\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            # Count wins per month\n",
    "            monthly_comparison = []\n",
    "            \n",
    "            for month in monthly_data.index:\n",
    "                month_data = self.test_data[self.test_data['year_month'] == month]\n",
    "                \n",
    "                if len(month_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                bench_mae = month_data['benchmark_abs_error'].mean()\n",
    "                ml_mae = month_data[f'{model_name}_abs_error'].mean()\n",
    "                \n",
    "                wins_benchmark = ml_mae < bench_mae\n",
    "                improvement = bench_mae - ml_mae\n",
    "                improvement_pct = (improvement / bench_mae) * 100 if bench_mae > 0 else 0\n",
    "                \n",
    "                monthly_comparison.append({\n",
    "                    'month': month,\n",
    "                    'benchmark_mae': bench_mae,\n",
    "                    'ml_mae': ml_mae,\n",
    "                    'wins_benchmark': wins_benchmark,\n",
    "                    'improvement': improvement,\n",
    "                    'improvement_pct': improvement_pct,\n",
    "                    'records': len(month_data)\n",
    "                })\n",
    "            \n",
    "            comparison_df = pd.DataFrame(monthly_comparison)\n",
    "            \n",
    "            # Summary statistics\n",
    "            total_months = len(comparison_df)\n",
    "            months_won = comparison_df['wins_benchmark'].sum()\n",
    "            win_rate = (months_won / total_months) * 100 if total_months > 0 else 0\n",
    "            avg_improvement = comparison_df['improvement_pct'].mean()\n",
    "            \n",
    "            print(f\"ðŸ“Š {model_name.upper()} MONTHLY PERFORMANCE:\")\n",
    "            print(f\"   Months tested: {total_months}\")\n",
    "            print(f\"   Months won: {months_won}/{total_months} ({win_rate:.1f}%)\")\n",
    "            print(f\"   Average improvement: {avg_improvement:+.1f}%\")\n",
    "            \n",
    "            # Show recent monthly performance\n",
    "            print(f\"   Recent performance:\")\n",
    "            for _, row in comparison_df.tail(6).iterrows():\n",
    "                status = \"âœ… WIN\" if row['wins_benchmark'] else \"âŒ LOSS\"\n",
    "                print(f\"     {row['month']}: {row['improvement_pct']:+.1f}% {status}\")\n",
    "            \n",
    "            temporal_results[model_name] = {\n",
    "                'total_months': total_months,\n",
    "                'months_won': months_won,\n",
    "                'win_rate': win_rate,\n",
    "                'avg_improvement': avg_improvement,\n",
    "                'monthly_details': comparison_df\n",
    "            }\n",
    "            print(\"\")\n",
    "        \n",
    "        return temporal_results\n",
    "    \n",
    "    def analyze_cumulative_error_impact(self):\n",
    "        \"\"\"\n",
    "        5. Cumulative Error & Annual Risk Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ’° 5. CUMULATIVE ERROR & ANNUAL RISK ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if 'year_month' not in self.test_data.columns:\n",
    "            print(\"âŒ No temporal data for cumulative analysis\")\n",
    "            return {}\n",
    "        \n",
    "        # Monthly aggregated data\n",
    "        monthly_data = self.test_data.groupby('year_month').agg({\n",
    "            self.actual_col: 'sum',\n",
    "            self.benchmark_col: 'sum',\n",
    "            'benchmark_error': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add ML model data\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        cumulative_results = {}\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            # Calculate monthly ML predictions and errors\n",
    "            monthly_ml = self.test_data.groupby('year_month').agg({\n",
    "                pred_col: 'sum',\n",
    "                f'{model_name}_error': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Merge with main monthly data\n",
    "            monthly_combined = monthly_data.merge(monthly_ml, on='year_month')\n",
    "            \n",
    "            # Calculate cumulative metrics\n",
    "            monthly_combined['cumulative_actual'] = monthly_combined[self.actual_col].cumsum()\n",
    "            monthly_combined['cumulative_benchmark'] = monthly_combined[self.benchmark_col].cumsum()\n",
    "            monthly_combined['cumulative_ml'] = monthly_combined[pred_col].cumsum()\n",
    "            \n",
    "            monthly_combined['cumulative_benchmark_error'] = monthly_combined['benchmark_error'].cumsum()\n",
    "            monthly_combined['cumulative_ml_error'] = monthly_combined[f'{model_name}_error'].cumsum()\n",
    "            \n",
    "            monthly_combined['cumulative_benchmark_error_pct'] = (monthly_combined['cumulative_benchmark_error'] / monthly_combined['cumulative_actual']) * 100\n",
    "            monthly_combined['cumulative_ml_error_pct'] = (monthly_combined['cumulative_ml_error'] / monthly_combined['cumulative_actual']) * 100\n",
    "            \n",
    "            # Annual projections\n",
    "            avg_monthly_demand = monthly_combined[self.actual_col].mean()\n",
    "            last_month_ml_error_rate = monthly_combined[f'{model_name}_error'].iloc[-1] / monthly_combined[self.actual_col].iloc[-1]\n",
    "            last_month_benchmark_error_rate = monthly_combined['benchmark_error'].iloc[-1] / monthly_combined[self.actual_col].iloc[-1]\n",
    "            \n",
    "            # Project full year impact\n",
    "            annual_demand_projection = avg_monthly_demand * 12\n",
    "            annual_ml_error_projection = last_month_ml_error_rate * avg_monthly_demand * 12\n",
    "            annual_benchmark_error_projection = last_month_benchmark_error_rate * avg_monthly_demand * 12\n",
    "            \n",
    "            annual_ml_error_pct = (annual_ml_error_projection / annual_demand_projection) * 100\n",
    "            annual_benchmark_error_pct = (annual_benchmark_error_projection / annual_demand_projection) * 100\n",
    "            \n",
    "            # Final cumulative error\n",
    "            final_ml_cumulative_error = monthly_combined['cumulative_ml_error_pct'].iloc[-1]\n",
    "            final_benchmark_cumulative_error = monthly_combined['cumulative_benchmark_error_pct'].iloc[-1]\n",
    "            \n",
    "            print(f\"ðŸ“Š {model_name.upper()} CUMULATIVE ANALYSIS:\")\n",
    "            print(f\"   Current cumulative error: {final_ml_cumulative_error:+.1f}%\")\n",
    "            print(f\"   Benchmark cumulative error: {final_benchmark_cumulative_error:+.1f}%\")\n",
    "            print(f\"   Projected annual error: {annual_ml_error_pct:+.1f}%\")\n",
    "            print(f\"   Benchmark annual error: {annual_benchmark_error_pct:+.1f}%\")\n",
    "            \n",
    "            # Risk assessment\n",
    "            if abs(annual_ml_error_pct) < 5:\n",
    "                risk_level = \"ðŸŸ¢ LOW RISK\"\n",
    "            elif abs(annual_ml_error_pct) < 15:\n",
    "                risk_level = \"ðŸŸ¡ MODERATE RISK\"\n",
    "            else:\n",
    "                risk_level = \"ðŸ”´ HIGH RISK\"\n",
    "            \n",
    "            print(f\"   Risk Level: {risk_level}\")\n",
    "            \n",
    "            # Show if ML beats benchmark cumulatively\n",
    "            cumulative_improvement = final_benchmark_cumulative_error - final_ml_cumulative_error\n",
    "            annual_improvement = annual_benchmark_error_pct - annual_ml_error_pct\n",
    "            \n",
    "            print(f\"   Cumulative improvement: {cumulative_improvement:+.1f}%\")\n",
    "            print(f\"   Annual improvement: {annual_improvement:+.1f}%\")\n",
    "            \n",
    "            cumulative_results[model_name] = {\n",
    "                'final_cumulative_error': final_ml_cumulative_error,\n",
    "                'projected_annual_error': annual_ml_error_pct,\n",
    "                'risk_level': risk_level,\n",
    "                'cumulative_improvement': cumulative_improvement,\n",
    "                'annual_improvement': annual_improvement,\n",
    "                'monthly_data': monthly_combined\n",
    "            }\n",
    "            print(\"\")\n",
    "        \n",
    "        return cumulative_results\n",
    "    \n",
    "    def feature_importance_deep_dive(self):\n",
    "    \n",
    "        \n",
    "        # Analyze prediction patterns\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        if ml_pred_cols and 'demand_ma_3d' in self.test_data.columns:\n",
    "            print(\"ðŸ“Š CORRELATION ANALYSIS:\")\n",
    "            \n",
    "            for pred_col in ml_pred_cols[:2]:  # Check top 2 models\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                \n",
    "                # Correlation between ML prediction and 3-day moving average\n",
    "                if 'demand_ma_3d' in self.test_data.columns:\n",
    "                    correlation = self.test_data[pred_col].corr(self.test_data['demand_ma_3d'])\n",
    "                    print(f\"   {model_name} vs demand_ma_3d: {correlation:.3f} correlation\")\n",
    "                \n",
    "                # Correlation with actual demand\n",
    "                correlation_actual = self.test_data[pred_col].corr(self.test_data[self.actual_col])\n",
    "                print(f\"   {model_name} vs actual demand: {correlation_actual:.3f} correlation\")\n",
    "                \n",
    "                # Correlation with benchmark\n",
    "                correlation_benchmark = self.test_data[pred_col].corr(self.test_data[self.benchmark_col])\n",
    "                print(f\"   {model_name} vs benchmark: {correlation_benchmark:.3f} correlation\")\n",
    "                print(\"\")\n",
    "    \n",
    "    def run_comprehensive_analysis(self):\n",
    "        \"\"\"\n",
    "        Run all analyses and provide summary\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ RUNNING COMPREHENSIVE ML MODEL ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            results['overall_accuracy'] = self.analyze_overall_accuracy()\n",
    "            results['weighted_accuracy'] = self.analyze_weighted_accuracy()\n",
    "            results['channel_performance'] = self.analyze_channel_performance()\n",
    "            results['temporal_performance'] = self.analyze_temporal_performance()\n",
    "            results['cumulative_analysis'] = self.analyze_cumulative_error_impact()\n",
    "            self.feature_importance_deep_dive()\n",
    "            \n",
    "            # Summary conclusions\n",
    "            self.provide_final_recommendations(results)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during analysis: {str(e)}\")\n",
    "            return results\n",
    "    \n",
    "    def provide_final_recommendations(self, results):\n",
    "        \"\"\"\n",
    "        Provide final recommendations based on analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸŽ¯ FINAL RECOMMENDATIONS\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        # Find best performing ML model\n",
    "        if 'overall_accuracy' in results:\n",
    "            best_model = None\n",
    "            best_improvement = -float('inf')\n",
    "            \n",
    "            for model_name, metrics in results['overall_accuracy'].items():\n",
    "                if model_name != 'benchmark' and 'mae_improvement_pct' in metrics:\n",
    "                    if metrics['mae_improvement_pct'] > best_improvement:\n",
    "                        best_improvement = metrics['mae_improvement_pct']\n",
    "                        best_model = model_name\n",
    "            \n",
    "            if best_model and best_improvement > 0:\n",
    "                print(f\"âœ… BEST MODEL: {best_model}\")\n",
    "                print(f\"   Overall improvement: {best_improvement:+.1f}%\")\n",
    "                \n",
    "                # Check consistency\n",
    "                if 'temporal_performance' in results and best_model in results['temporal_performance']:\n",
    "                    win_rate = results['temporal_performance'][best_model]['win_rate']\n",
    "                    print(f\"   Monthly win rate: {win_rate:.1f}%\")\n",
    "                    \n",
    "                    if win_rate >= 70:\n",
    "                        print(\"   ðŸŸ¢ CONSISTENT performer - safe to deploy\")\n",
    "                    elif win_rate >= 50:\n",
    "                        print(\"   ðŸŸ¡ MODERATELY consistent - monitor closely\")\n",
    "                    else:\n",
    "                        print(\"   ðŸ”´ INCONSISTENT - risky for production\")\n",
    "                \n",
    "                print(\"\")\n",
    "                print(\"ðŸ“‹ DEPLOYMENT RECOMMENDATIONS:\")\n",
    "                if best_improvement > 10 and win_rate >= 70:\n",
    "                    print(\"   ðŸš€ DEPLOY: Strong, consistent improvement\")\n",
    "                elif best_improvement > 5:\n",
    "                    print(\"   ðŸ§ª PILOT: Test on subset before full deployment\")\n",
    "                else:\n",
    "                    print(\"   â¸ï¸  HOLD: Improvement too small, stick with benchmark\")\n",
    "                    \n",
    "            else:\n",
    "                print(\"âŒ NO ML MODEL BEATS BENCHMARK\")\n",
    "                print(\"   ðŸ’¡ Stick with your benchmark model\")\n",
    "                print(\"   ðŸ”„ Consider different features or model types\")\n",
    "\n",
    "def analyze_ml_model_performance(test_data, model_results, benchmark_col='bm_demand', actual_col='actual_demand'):\n",
    "    \"\"\"\n",
    "    Main function to run comprehensive ML model analysis\n",
    "    \n",
    "    Args:\n",
    "        test_data: Test dataset with predictions\n",
    "        model_results: Dictionary of trained models and results\n",
    "        benchmark_col: Column name for benchmark predictions\n",
    "        actual_col: Column name for actual values\n",
    "    \n",
    "    Returns:\n",
    "        MLModelEvaluationAnalysis instance with all results\n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = MLModelEvaluationAnalysis(test_data, model_results, benchmark_col, actual_col)\n",
    "    analysis_results = analyzer.run_comprehensive_analysis()\n",
    "    \n",
    "    return analyzer, analysis_results\n",
    "\n",
    "# Usage:\n",
    "analyzer, analysis_results = analyze_ml_model_performance(test_data, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7321b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler, analyzer, results =run_advanced_demand_models(\n",
    "    benchmark_df=enhanced_benchmark_df,\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items, \n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler, analyzer, results = run_advanced_demand_models(\n",
    "    benchmark_df=enhanced_benchmark_df,\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items, \n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prophet library available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Run the enhanced analysis\\nenhanced_modeler, enhanced_analyzer, enhanced_results, marketing_insights = run_enhanced_demand_analysis_with_prophet_and_marketing(\\n    benchmark_df=your_benchmark_data,\\n    amazon_order_items=your_amazon_data,\\n    # ... other channel data\\n)\\n\\n# Extract specific insights\\nmarketing_effectiveness = enhanced_results['marketing_effectiveness']\\nq4_seasonality = enhanced_results['q4_seasonality']\\nprophet_performance = enhanced_results['prophet_effectiveness']\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prophet import with error handling\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "    print(\"âœ… Prophet library available\")\n",
    "except ImportError:\n",
    "    PROPHET_AVAILABLE = False\n",
    "    print(\"âš ï¸ Prophet not available. Install with: pip install prophet\")\n",
    "\n",
    "class EnhancedDemandModels:\n",
    "    def __init__(self, benchmark_df, amazon_order_items=None, tiktok_order_items=None, shopify_order_items=None,\n",
    "                 amazon_daily_sku=None, tiktok_daily_sku=None, shopify_daily_sku=None):\n",
    "        \n",
    "        print(\"ðŸš€ ENHANCED DEMAND PREDICTION MODELS\")\n",
    "        print(\"ðŸŽ¯ New Features: Prophet forecasting + Marketing transformations + Q4 seasonality\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.benchmark_df = benchmark_df.copy()\n",
    "        \n",
    "        # Combine channel-specific data\n",
    "        self.order_items_df = self.combine_channel_data([\n",
    "            (amazon_order_items, 'amazon'),\n",
    "            (tiktok_order_items, 'tiktok'), \n",
    "            (shopify_order_items, 'shopify')\n",
    "        ], 'order_items')\n",
    "        \n",
    "        self.daily_sku_df = self.combine_channel_data([\n",
    "            (amazon_daily_sku, 'amazon'),\n",
    "            (tiktok_daily_sku, 'tiktok'),\n",
    "            (shopify_daily_sku, 'shopify')\n",
    "        ], 'daily_sku')\n",
    "        \n",
    "        self.models = {}\n",
    "        self.feature_engineered_df = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.prophet_models = {}\n",
    "        \n",
    "        # Prepare base data\n",
    "        self.prepare_base_data()\n",
    "    \n",
    "    def combine_channel_data(self, channel_data_list, data_type):\n",
    "        \"\"\"Combine data from multiple channels into single dataframe\"\"\"\n",
    "        combined_data = []\n",
    "        \n",
    "        for data, channel_name in channel_data_list:\n",
    "            if data is not None and not data.empty:\n",
    "                df = data.copy()\n",
    "                df['channel'] = channel_name\n",
    "                combined_data.append(df)\n",
    "                print(f\"   ðŸ“Š {channel_name.capitalize()} {data_type}: {len(df):,} records\")\n",
    "        \n",
    "        if combined_data:\n",
    "            result = pd.concat(combined_data, ignore_index=True)\n",
    "            print(f\"   âœ… Combined {data_type}: {len(result):,} total records\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"   âš ï¸ No {data_type} data available\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_base_data(self):\n",
    "        \"\"\"Clean and prepare base data for feature engineering\"\"\"\n",
    "        print(\"\\nðŸ”§ Preparing base data...\")\n",
    "        \n",
    "        # Clean benchmark data\n",
    "        self.benchmark_df = self.benchmark_df.dropna(subset=['actual_demand', 'bm_demand'])\n",
    "        self.benchmark_df = self.benchmark_df[self.benchmark_df['actual_demand'] > 0]\n",
    "        \n",
    "        # Ensure date column\n",
    "        self.benchmark_df['order_date'] = pd.to_datetime(self.benchmark_df['order_date'])\n",
    "        \n",
    "        print(f\"   âœ… Base data: {len(self.benchmark_df):,} records\")\n",
    "        print(f\"   ðŸ“… Date range: {self.benchmark_df['order_date'].min()} to {self.benchmark_df['order_date'].max()}\")\n",
    "    \n",
    "    def engineer_marketing_transformations(self, df):\n",
    "        \"\"\"Feature Set: Marketing Spend Transformations for Diminishing Returns\"\"\"\n",
    "        print(\"   ðŸ’° Engineering marketing spend transformations...\")\n",
    "        \n",
    "        # Identify marketing-related columns\n",
    "        marketing_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in \n",
    "                         ['marketing', 'spend', 'ad', 'advertising', 'campaign', 'promotion', 'cpc', 'cpm', 'roas'])]\n",
    "        \n",
    "        if not marketing_cols:\n",
    "            print(\"   âš ï¸ No marketing columns found, creating synthetic marketing data\")\n",
    "            # Create synthetic marketing spend based on demand patterns\n",
    "            np.random.seed(42)\n",
    "            df['marketing_spend'] = np.random.gamma(2, df['actual_demand'] * 0.1) + np.random.normal(0, 50)\n",
    "            df['marketing_spend'] = np.maximum(df['marketing_spend'], 1)  # Ensure positive\n",
    "            marketing_cols = ['marketing_spend']\n",
    "        \n",
    "        print(f\"   ðŸ“Š Found marketing columns: {marketing_cols}\")\n",
    "        \n",
    "        for col in marketing_cols:\n",
    "            # Ensure positive values for log transformations\n",
    "            df[f'{col}_positive'] = np.maximum(df[col], 1)\n",
    "            \n",
    "            # 1. Log transformation: Y ~ Î²â‚*log(marketing)\n",
    "            # Captures diminishing returns - each additional dollar has decreasing impact\n",
    "            df[f'{col}_log'] = np.log(df[f'{col}_positive'])\n",
    "            \n",
    "            # 2. Quadratic transformation: Y ~ Î²â‚*marketing + Î²â‚‚*marketingÂ²\n",
    "            # Captures non-linear relationship - can model both increasing and decreasing returns\n",
    "            df[f'{col}_squared'] = df[f'{col}_positive'] ** 2\n",
    "            \n",
    "            # 3. Square root transformation (alternative diminishing returns)\n",
    "            df[f'{col}_sqrt'] = np.sqrt(df[f'{col}_positive'])\n",
    "            \n",
    "            # 4. Marketing efficiency ratio (spend per unit of demand)\n",
    "            df[f'{col}_efficiency'] = df[f'{col}_positive'] / (df['actual_demand'] + 1)\n",
    "            \n",
    "            # 5. Marketing intensity buckets\n",
    "            marketing_quantiles = df[f'{col}_positive'].quantile([0.25, 0.5, 0.75])\n",
    "            df[f'{col}_intensity_low'] = (df[f'{col}_positive'] <= marketing_quantiles[0.25]).astype(int)\n",
    "            df[f'{col}_intensity_medium'] = ((df[f'{col}_positive'] > marketing_quantiles[0.25]) & \n",
    "                                           (df[f'{col}_positive'] <= marketing_quantiles[0.75])).astype(int)\n",
    "            df[f'{col}_intensity_high'] = (df[f'{col}_positive'] > marketing_quantiles[0.75]).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_q4_holiday_features(self, df):\n",
    "       \n",
    "        print(\"   ðŸŽ„ Engineering Q4 and holiday features...\")\n",
    "        \n",
    "        # Basic Q4 features\n",
    "        df['is_q4'] = (df['order_date'].dt.quarter == 4).astype(int)\n",
    "        df['month'] = df['order_date'].dt.month\n",
    "        \n",
    "        # Detailed Q4 breakdown\n",
    "        df['is_october'] = (df['month'] == 10).astype(int)\n",
    "        df['is_november'] = (df['month'] == 11).astype(int) \n",
    "        df['is_december'] = (df['month'] == 12).astype(int)\n",
    "        \n",
    "        # Black Friday period (last Thursday of November + 4 days)\n",
    "        df['week_of_year'] = df['order_date'].dt.isocalendar().week\n",
    "        \n",
    "        # Approximate Black Friday weeks (weeks 47-48 typically)\n",
    "        df['is_black_friday_week'] = ((df['week_of_year'] >= 47) & (df['week_of_year'] <= 48) & \n",
    "                                     (df['month'] == 11)).astype(int)\n",
    "        \n",
    "        # Cyber Monday (Monday after Black Friday)\n",
    "        df['is_cyber_monday_week'] = df['is_black_friday_week']  # Same week typically\n",
    "        \n",
    "        # Pre-holiday shopping surge (first 3 weeks of December)\n",
    "        df['is_pre_christmas'] = ((df['month'] == 12) & (df['order_date'].dt.day <= 21)).astype(int)\n",
    "        \n",
    "        # Post-holiday period (last week of December)\n",
    "        df['is_post_christmas'] = ((df['month'] == 12) & (df['order_date'].dt.day >= 26)).astype(int)\n",
    "        \n",
    "        # Holiday shopping intensity score\n",
    "        df['holiday_intensity'] = (\n",
    "            df['is_black_friday_week'] * 3 +  # Highest intensity\n",
    "            df['is_pre_christmas'] * 2 +      # High intensity\n",
    "            df['is_q4'] * 1                   # Base Q4 lift\n",
    "        )\n",
    "        \n",
    "        # Days until/since major holidays\n",
    "        # This is approximate - in real implementation you'd use exact holiday dates\n",
    "        df['days_to_black_friday'] = np.where(\n",
    "            (df['month'] == 11) & (df['order_date'].dt.day < 25),\n",
    "            25 - df['order_date'].dt.day,  # Approximate\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        df['days_to_christmas'] = np.where(\n",
    "            df['month'] == 12,\n",
    "            25 - df['order_date'].dt.day,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Year-over-year Q4 growth (if multi-year data)\n",
    "        df['year'] = df['order_date'].dt.year\n",
    "        if df['year'].nunique() > 1:\n",
    "            # Create year-over-year comparison features\n",
    "            for year in sorted(df['year'].unique())[1:]:  # Skip first year\n",
    "                prev_year = year - 1\n",
    "                df[f'is_year_{year}'] = (df['year'] == year).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_prophet_features(self, df):\n",
    "        \"\"\"Feature Set: Prophet-derived features\"\"\"\n",
    "        print(\"   ðŸ“ˆ Engineering Prophet-derived features...\")\n",
    "        \n",
    "        if not PROPHET_AVAILABLE:\n",
    "            print(\"   âš ï¸ Prophet not available, skipping Prophet features\")\n",
    "            return df\n",
    "        \n",
    "        # Prepare data for Prophet (needs 'ds' and 'y' columns)\n",
    "        prophet_data = df[['order_date', 'actual_demand']].copy()\n",
    "        prophet_data.columns = ['ds', 'y']\n",
    "        prophet_data = prophet_data.groupby('ds')['y'].sum().reset_index()  # Aggregate by date\n",
    "        \n",
    "        try:\n",
    "            # Train Prophet model for trend decomposition\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            \n",
    "            # Add custom seasonalities for Q4\n",
    "            prophet_model.add_seasonality(\n",
    "                name='quarterly',\n",
    "                period=91.25,  # ~3 months\n",
    "                fourier_order=4\n",
    "            )\n",
    "            \n",
    "            print(\"   ðŸ”„ Training Prophet model for feature extraction...\")\n",
    "            prophet_model.fit(prophet_data)\n",
    "            \n",
    "            # Generate components\n",
    "            future = prophet_model.make_future_dataframe(periods=0)\n",
    "            forecast = prophet_model.predict(future)\n",
    "            \n",
    "            # Extract Prophet components\n",
    "            prophet_components = forecast[['ds', 'trend', 'yearly', 'weekly', 'quarterly']].copy()\n",
    "            prophet_components.columns = ['order_date', 'prophet_trend', 'prophet_yearly', 'prophet_weekly', 'prophet_quarterly']\n",
    "            \n",
    "            # Merge Prophet features back to original data\n",
    "            df = df.merge(prophet_components, on='order_date', how='left')\n",
    "            \n",
    "            # Create additional Prophet-derived features\n",
    "            df['prophet_trend_change'] = df.groupby('sku')['prophet_trend'].pct_change()\n",
    "            df['prophet_seasonality_strength'] = np.abs(df['prophet_yearly']) + np.abs(df['prophet_weekly'])\n",
    "            \n",
    "            # Fill NaN values\n",
    "            prophet_cols = ['prophet_trend', 'prophet_yearly', 'prophet_weekly', 'prophet_quarterly', \n",
    "                          'prophet_trend_change', 'prophet_seasonality_strength']\n",
    "            for col in prophet_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "            \n",
    "            print(\"   âœ… Prophet features extracted successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error in Prophet feature engineering: {str(e)}\")\n",
    "            # Add dummy Prophet features\n",
    "            df['prophet_trend'] = df['actual_demand'].rolling(7).mean()\n",
    "            df['prophet_yearly'] = np.sin(2 * np.pi * df['order_date'].dt.dayofyear / 365.25)\n",
    "            df['prophet_weekly'] = np.sin(2 * np.pi * df['order_date'].dt.dayofweek / 7)\n",
    "            df['prophet_quarterly'] = 0\n",
    "            df = df.fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_log_log_transformations(self, df, target_col='actual_demand'):\n",
    "        \"\"\"Create log-log model transformations: log(Y) ~ Î²â‚*log(X)\"\"\"\n",
    "        print(\"   ðŸ“Š Creating log-log transformations...\")\n",
    "        \n",
    "        # Create log-transformed target\n",
    "        df[f'{target_col}_log'] = np.log(np.maximum(df[target_col], 1))\n",
    "        \n",
    "        # Find numeric columns for log-log transformation\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        marketing_cols = [col for col in numeric_cols if any(keyword in col.lower() for keyword in \n",
    "                         ['marketing', 'spend', 'ad', 'advertising', 'price', 'cost'])]\n",
    "        \n",
    "        # Create log-log features for marketing variables\n",
    "        for col in marketing_cols:\n",
    "            if col != target_col and not col.endswith('_log'):\n",
    "                # Ensure positive values\n",
    "                positive_col = f'{col}_positive'\n",
    "                if positive_col not in df.columns:\n",
    "                    df[positive_col] = np.maximum(df[col], 1)\n",
    "                \n",
    "                # Create log-log interaction\n",
    "                log_col = f'{col}_log'\n",
    "                if log_col in df.columns:\n",
    "                    df[f'loglog_{col}'] = df[f'{target_col}_log'] * df[log_col]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_enhanced_feature_dataset(self):\n",
    "        \"\"\"Create comprehensive feature-engineered dataset with new transformations\"\"\"\n",
    "        print(\"\\nðŸ—ï¸ ENHANCED FEATURE ENGINEERING PIPELINE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        df = self.benchmark_df.copy()\n",
    "        \n",
    "        # Apply existing feature engineering\n",
    "        df = self.engineer_seasonality_features(df)\n",
    "        df = self.engineer_product_features(df)\n",
    "        df = self.engineer_customer_lifecycle_features(df)\n",
    "        df = self.engineer_cross_sku_features(df)\n",
    "        \n",
    "        # Apply new enhanced feature engineering\n",
    "        df = self.engineer_marketing_transformations(df)\n",
    "        df = self.engineer_q4_holiday_features(df)\n",
    "        df = self.engineer_prophet_features(df)\n",
    "        df = self.create_log_log_transformations(df)\n",
    "        df = self.engineer_lag_features(df)  # Apply lag features last\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        if 'channel' in df.columns:\n",
    "            le_channel = LabelEncoder()\n",
    "            df['channel_encoded'] = le_channel.fit_transform(df['channel'].fillna('unknown'))\n",
    "        else:\n",
    "            df['channel_encoded'] = 0\n",
    "        \n",
    "        # Remove non-feature columns for modeling\n",
    "        feature_cols = [col for col in df.columns if col not in [\n",
    "            'order_date', 'sku', 'actual_demand', 'bm_demand', 'error_metric', \n",
    "            'product_name', 'first_sale', 'last_sale', 'channel', 'actual_demand_log',\n",
    "            'marketing_spend_positive'  # Remove helper columns\n",
    "        ]]\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        for col in feature_cols:\n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(0)\n",
    "        \n",
    "        self.feature_engineered_df = df\n",
    "        self.feature_columns = feature_cols\n",
    "        \n",
    "        print(f\"   âœ… Enhanced feature engineering complete\")\n",
    "        print(f\"   ðŸ“Š Total features: {len(feature_cols)}\")\n",
    "        print(f\"   ðŸ“‹ Records: {len(df):,}\")\n",
    "        \n",
    "        # Show new feature categories\n",
    "        marketing_features = [col for col in feature_cols if any(keyword in col for keyword in ['marketing', '_log', '_squared', '_efficiency'])]\n",
    "        q4_features = [col for col in feature_cols if any(keyword in col for keyword in ['q4', 'holiday', 'black_friday', 'christmas'])]\n",
    "        prophet_features = [col for col in feature_cols if 'prophet' in col]\n",
    "        \n",
    "        print(f\"   ðŸ’° Marketing transformation features: {len(marketing_features)}\")\n",
    "        print(f\"   ðŸŽ„ Q4/Holiday features: {len(q4_features)}\")\n",
    "        print(f\"   ðŸ“ˆ Prophet-derived features: {len(prophet_features)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def train_prophet_models(self, df):\n",
    "        \"\"\"Train Prophet models for each SKU\"\"\"\n",
    "        print(\"\\nðŸ“ˆ TRAINING PROPHET MODELS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        if not PROPHET_AVAILABLE:\n",
    "            print(\"âŒ Prophet not available. Skipping Prophet models.\")\n",
    "            return {}\n",
    "        \n",
    "        prophet_results = {}\n",
    "        \n",
    "        # Get unique SKUs (limit to top SKUs for computational efficiency)\n",
    "        sku_revenue = df.groupby('sku')['actual_demand'].sum().sort_values(ascending=False)\n",
    "        top_skus = sku_revenue.head(10).index  # Train Prophet on top 10 SKUs\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Training Prophet on top {len(top_skus)} SKUs by revenue\")\n",
    "        \n",
    "        for sku in top_skus:\n",
    "            sku_data = df[df['sku'] == sku].copy()\n",
    "            \n",
    "            if len(sku_data) < 30:  # Need sufficient data for Prophet\n",
    "                print(f\"   âš ï¸ Skipping {sku}: insufficient data ({len(sku_data)} records)\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare Prophet data\n",
    "                prophet_data = sku_data[['order_date', 'actual_demand']].copy()\n",
    "                prophet_data.columns = ['ds', 'y']\n",
    "                prophet_data = prophet_data.sort_values('ds')\n",
    "                \n",
    "                # Create Prophet model with enhanced seasonality\n",
    "                model = Prophet(\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False,\n",
    "                    seasonality_mode='multiplicative',\n",
    "                    changepoint_prior_scale=0.05,\n",
    "                    interval_width=0.8\n",
    "                )\n",
    "                \n",
    "                # Add Q4 seasonality\n",
    "                model.add_seasonality(\n",
    "                    name='quarterly',\n",
    "                    period=91.25,\n",
    "                    fourier_order=3\n",
    "                )\n",
    "                \n",
    "                # Add marketing spend as regressor if available\n",
    "                marketing_cols = [col for col in sku_data.columns if 'marketing' in col and not col.endswith('_log')]\n",
    "                if marketing_cols:\n",
    "                    main_marketing_col = marketing_cols[0]\n",
    "                    prophet_data[main_marketing_col] = sku_data[main_marketing_col].values\n",
    "                    model.add_regressor(main_marketing_col)\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"   ðŸ”„ Training Prophet for {sku}...\")\n",
    "                model.fit(prophet_data)\n",
    "                \n",
    "                # Make predictions\n",
    "                future = model.make_future_dataframe(periods=0)\n",
    "                if marketing_cols:\n",
    "                    future[main_marketing_col] = prophet_data[main_marketing_col]\n",
    "                \n",
    "                forecast = model.predict(future)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                actual = prophet_data['y'].values\n",
    "                predicted = forecast['yhat'].values\n",
    "                \n",
    "                mae = mean_absolute_error(actual, predicted)\n",
    "                \n",
    "                prophet_results[sku] = {\n",
    "                    'model': model,\n",
    "                    'forecast': forecast,\n",
    "                    'mae': mae,\n",
    "                    'predictions': predicted,\n",
    "                    'marketing_regressors': marketing_cols\n",
    "                }\n",
    "                \n",
    "                print(f\"   âœ… {sku}: MAE ${mae:,.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error training Prophet for {sku}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        self.prophet_models = prophet_results\n",
    "        print(f\"\\nâœ… Prophet training complete: {len(prophet_results)} models trained\")\n",
    "        \n",
    "        return prophet_results\n",
    "    \n",
    "    def train_enhanced_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train enhanced models with marketing transformations\"\"\"\n",
    "        \n",
    "        model_formulas = {\n",
    "            'Marketing Log Model': \"\"\"\n",
    "            ðŸ’° MARKETING LOG TRANSFORMATION MODEL:\n",
    "            Demand = Î²â‚€ + Î²â‚Ã—log(marketing_spend) + Î²â‚‚Ã—seasonality + Î²â‚ƒÃ—customer_features + Îµ\n",
    "            \n",
    "            Focus: Captures diminishing returns from marketing spend\n",
    "            Theory: Each additional dollar has decreasing marginal impact\n",
    "            \"\"\",\n",
    "            \n",
    "            'Marketing Quadratic Model': \"\"\"\n",
    "            ðŸ“ˆ MARKETING QUADRATIC MODEL:\n",
    "            Demand = Î²â‚€ + Î²â‚Ã—marketing + Î²â‚‚Ã—marketingÂ² + Î²â‚ƒÃ—other_features + Îµ\n",
    "            \n",
    "            Focus: Non-linear marketing response curve\n",
    "            Theory: Can model both increasing and decreasing returns\n",
    "            \"\"\",\n",
    "            \n",
    "            'Log-Log Marketing Model': \"\"\"\n",
    "            ðŸ“Š LOG-LOG MARKETING MODEL:\n",
    "            log(Demand) = Î²â‚€ + Î²â‚Ã—log(marketing) + Î²â‚‚Ã—log(other_features) + Îµ\n",
    "            \n",
    "            Focus: Elasticity interpretation - Î² coefficients are elasticities\n",
    "            Theory: Percent change in marketing â†’ percent change in demand\n",
    "            \"\"\",\n",
    "            \n",
    "            'Q4 Enhanced Model': \"\"\"\n",
    "            ðŸŽ„ Q4 ENHANCED SEASONALITY MODEL:\n",
    "            Demand = Î²â‚€ + Î²â‚Ã—base_features + Î²â‚‚Ã—is_black_fridayÃ—5 + Î²â‚ƒÃ—holiday_intensityÃ—3 + \n",
    "                     Î²â‚„Ã—days_to_christmas + Î²â‚…Ã—q4_marketing_interaction + Îµ\n",
    "            \n",
    "            Focus: Captures Q4 shopping surge and holiday patterns\n",
    "            Weights: Black Friday gets 5x multiplier, holiday intensity 3x\n",
    "            \"\"\",\n",
    "            \n",
    "            'Prophet-Enhanced Linear': \"\"\"\n",
    "            ðŸ“ˆ PROPHET-ENHANCED LINEAR MODEL:\n",
    "            Demand = Î²â‚€ + Î²â‚Ã—prophet_trend + Î²â‚‚Ã—prophet_seasonality + Î²â‚ƒÃ—marketing_log + \n",
    "                     Î²â‚„Ã—customer_features + Îµ\n",
    "            \n",
    "            Focus: Combines Prophet's time series insights with regression\n",
    "            \"\"\",\n",
    "            \n",
    "            'Hybrid Marketing Model': \"\"\"\n",
    "            ðŸ”„ HYBRID MARKETING RESPONSE MODEL:\n",
    "            Demand = Î²â‚€ + f(marketing_spend) + g(seasonality) + h(customer_mix)\n",
    "            \n",
    "            Where:\n",
    "            f(marketing) = Î²â‚Ã—log(marketing) + Î²â‚‚Ã—marketing_efficiency\n",
    "            g(seasonality) = Î²â‚ƒÃ—q4_multiplier + Î²â‚„Ã—prophet_trend\n",
    "            h(customer) = Î²â‚…Ã—loyalty_score + Î²â‚†Ã—new_customer_ratio\n",
    "            \n",
    "            Focus: Sophisticated marketing response with diminishing returns\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        models_to_train = {}\n",
    "        \n",
    "        # 1. Marketing Log Model\n",
    "        marketing_log_features = []\n",
    "        marketing_log_features.extend([col for col in X_train.columns if '_log' in col and 'marketing' in col])\n",
    "        marketing_log_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                                     ['month_sin', 'month_cos', 'customer', 'retention'])])\n",
    "        marketing_log_features = [f for f in marketing_log_features if f in X_train.columns][:15]  # Limit features\n",
    "        \n",
    "        models_to_train['Marketing Log Model'] = (LinearRegression(), marketing_log_features, False)\n",
    "        \n",
    "        # 2. Marketing Quadratic Model  \n",
    "        marketing_quad_features = []\n",
    "        marketing_quad_features.extend([col for col in X_train.columns if 'marketing' in col and ('_squared' in col or not any(trans in col for trans in ['_log', '_sqrt']))])\n",
    "        marketing_quad_features.extend([col for col in X_train.columns if 'seasonality' in col or 'month' in col])\n",
    "        marketing_quad_features = [f for f in marketing_quad_features if f in X_train.columns][:15]\n",
    "        \n",
    "        models_to_train['Marketing Quadratic Model'] = (LinearRegression(), marketing_quad_features, False)\n",
    "        \n",
    "        # 3. Q4 Enhanced Model\n",
    "        q4_features = []\n",
    "        q4_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                          ['q4', 'holiday', 'black_friday', 'christmas', 'december', 'november'])])\n",
    "        q4_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                          ['marketing', 'customer', 'seasonality'])])\n",
    "        q4_features = [f for f in q4_features if f in X_train.columns][:20]\n",
    "        \n",
    "        models_to_train['Q4 Enhanced Model'] = (LinearRegression(), q4_features, False)\n",
    "        \n",
    "        # 4. Prophet-Enhanced Linear\n",
    "        prophet_features = []\n",
    "        prophet_features.extend([col for col in X_train.columns if 'prophet' in col])\n",
    "        prophet_features.extend([col for col in X_train.columns if '_log' in col and 'marketing' in col])\n",
    "        prophet_features.extend([col for col in X_train.columns if 'customer' in col])\n",
    "        prophet_features = [f for f in prophet_features if f in X_train.columns][:15]\n",
    "        \n",
    "        models_to_train['Prophet-Enhanced Linear'] = (LinearRegression(), prophet_features, False)\n",
    "        \n",
    "        # 5. Hybrid Marketing Model (uses feature weighting)\n",
    "        hybrid_features = []\n",
    "        hybrid_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                              ['marketing_log', 'marketing_efficiency', 'q4', 'prophet_trend', 'customer_loyalty', 'new_customer_ratio'])])\n",
    "        hybrid_features = [f for f in hybrid_features if f in X_train.columns][:20]\n",
    "        \n",
    "        models_to_train['Hybrid Marketing Model'] = (LinearRegression(), hybrid_features, False)\n",
    "        \n",
    "        # 6. Enhanced Elastic Net with all marketing features\n",
    "        models_to_train['Enhanced Elastic Net'] = (ElasticNet(alpha=0.3, l1_ratio=0.5), None, True)\n",
    "        \n",
    "        # 7. Enhanced Random Forest\n",
    "        models_to_train['Enhanced Random Forest'] = (RandomForestRegressor(n_estimators=300, max_depth=20, \n",
    "                                                                          min_samples_split=5, random_state=42), None, False)\n",
    "        \n",
    "        # 8. Enhanced Gradient Boosting\n",
    "        models_to_train['Enhanced Gradient Boosting'] = (GradientBoostingRegressor(n_estimators=300, learning_rate=0.08,\n",
    "                                                                                   max_depth=8, random_state=42), None, False)\n",
    "        \n",
    "        # For log-log model, we need to transform the target variable\n",
    "        if 'actual_demand_log' in self.feature_engineered_df.columns:\n",
    "            # Get log-transformed target for the same indices as test set\n",
    "            y_train_log = None\n",
    "            y_test_log = None\n",
    "            \n",
    "            # Find the log target values corresponding to our train/test split\n",
    "            df_sorted = self.feature_engineered_df.sort_values('order_date')\n",
    "            split_point = int(len(df_sorted) * 0.8)\n",
    "            \n",
    "            if 'actual_demand_log' in df_sorted.columns:\n",
    "                y_train_log = df_sorted.iloc[:split_point]['actual_demand_log'].values\n",
    "                y_test_log = df_sorted.iloc[split_point:]['actual_demand_log'].values\n",
    "                \n",
    "                if len(y_train_log) == len(y_train) and len(y_test_log) == len(y_test):\n",
    "                    # Log-log model features\n",
    "                    loglog_features = [col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                                     ['_log', 'loglog_', 'marketing', 'prophet'])]\n",
    "                    loglog_features = [f for f in loglog_features if f in X_train.columns][:15]\n",
    "                    \n",
    "                    models_to_train['Log-Log Marketing Model'] = (LinearRegression(), loglog_features, False, y_train_log, y_test_log)\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        print(\"ðŸŽ¯ TRAINING ENHANCED MODELS WITH MARKETING TRANSFORMATIONS:\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for model_name, model_config in models_to_train.items():\n",
    "            print(f\"\\nðŸ”„ Training {model_name}...\")\n",
    "            \n",
    "            if len(model_config) == 5:  # Log-log model with custom target\n",
    "                model, features, needs_scaling, y_train_custom, y_test_custom = model_config\n",
    "            else:\n",
    "                model, features, needs_scaling = model_config\n",
    "                y_train_custom, y_test_custom = y_train, y_test\n",
    "            \n",
    "            # Print formula\n",
    "            if model_name in model_formulas:\n",
    "                print(f\"ðŸ“‹ {model_formulas[model_name]}\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                if features is None:  # Use all features\n",
    "                    X_train_model = X_train\n",
    "                    X_test_model = X_test\n",
    "                    features_used = X_train.columns.tolist()\n",
    "                else:  # Use specific features\n",
    "                    available_features = [f for f in features if f in X_train.columns]\n",
    "                    if not available_features:\n",
    "                        print(f\"   âŒ No valid features found for {model_name}\")\n",
    "                        continue\n",
    "                    X_train_model = X_train[available_features]\n",
    "                    X_test_model = X_test[available_features]\n",
    "                    features_used = available_features\n",
    "                \n",
    "                # Scale if needed\n",
    "                if needs_scaling:\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_model = scaler.fit_transform(X_train_model)\n",
    "                    X_test_model = scaler.transform(X_test_model)\n",
    "                \n",
    "                # Train model\n",
    "                model.fit(X_train_model, y_train_custom)\n",
    "                predictions = model.predict(X_test_model)\n",
    "                \n",
    "                # For log-log model, transform predictions back to original scale\n",
    "                if model_name == 'Log-Log Marketing Model' and len(model_config) == 5:\n",
    "                    predictions = np.exp(predictions)  # Transform back from log scale\n",
    "                    # Calculate metrics against original scale\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    mape = mean_absolute_percentage_error(y_test, predictions) * 100\n",
    "                else:\n",
    "                    # Standard metrics\n",
    "                    mae = mean_absolute_error(y_test_custom, predictions)\n",
    "                    mape = mean_absolute_percentage_error(y_test_custom, predictions) * 100\n",
    "                \n",
    "                model_results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': predictions,\n",
    "                    'mae': mae,\n",
    "                    'mape': mape,\n",
    "                    'features_used': features_used,\n",
    "                    'formula': model_formulas.get(model_name, \"No formula defined\"),\n",
    "                    'scaler': scaler if needs_scaling else None\n",
    "                }\n",
    "                \n",
    "                print(f\"   âœ… MAE: ${mae:,.2f}, MAPE: {mape:.1f}%\")\n",
    "                print(f\"   ðŸ“Š Features used: {len(features_used)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error training {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return model_results\n",
    "    \n",
    "    def evaluate_enhanced_models(self):\n",
    "        \"\"\"Train and evaluate all enhanced models including Prophet\"\"\"\n",
    "        print(\"\\nðŸ¤– ENHANCED MODEL TRAINING & EVALUATION\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if self.feature_engineered_df is None:\n",
    "            self.create_enhanced_feature_dataset()\n",
    "        \n",
    "        df = self.feature_engineered_df.copy()\n",
    "        \n",
    "        # Train Prophet models first\n",
    "        prophet_results = self.train_prophet_models(df)\n",
    "        \n",
    "        # Prepare features and target for ML models\n",
    "        X = df[self.feature_columns]\n",
    "        y = df['actual_demand']\n",
    "        \n",
    "        # Time series split for validation\n",
    "        df_sorted = df.sort_values('order_date')\n",
    "        split_point = int(len(df_sorted) * 0.8)\n",
    "        \n",
    "        train_data = df_sorted.iloc[:split_point]\n",
    "        test_data = df_sorted.iloc[split_point:]\n",
    "        \n",
    "        X_train = train_data[self.feature_columns]\n",
    "        y_train = train_data['actual_demand']\n",
    "        X_test = test_data[self.feature_columns]\n",
    "        y_test = test_data['actual_demand']\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = mean_absolute_error(test_data['actual_demand'], test_data['bm_demand'])\n",
    "        benchmark_mape = mean_absolute_percentage_error(test_data['actual_demand'], test_data['bm_demand']) * 100\n",
    "        \n",
    "        print(f\"ðŸ“Š Training Data: {len(train_data):,} records\")\n",
    "        print(f\"ðŸ“Š Test Data: {len(test_data):,} records\")\n",
    "        print(f\"ðŸŽ¯ Benchmark MAE: ${benchmark_mae:,.2f}\")\n",
    "        print(f\"ðŸŽ¯ Benchmark MAPE: {benchmark_mape:.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # Train enhanced ML models\n",
    "        model_results = self.train_enhanced_models(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Add Prophet results if available\n",
    "        if prophet_results:\n",
    "            print(\"\\nðŸ“ˆ PROPHET MODEL RESULTS:\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Aggregate Prophet predictions for comparison\n",
    "            prophet_mae_scores = [result['mae'] for result in prophet_results.values()]\n",
    "            avg_prophet_mae = np.mean(prophet_mae_scores)\n",
    "            \n",
    "            print(f\"Prophet Average MAE: ${avg_prophet_mae:,.2f}\")\n",
    "            print(f\"Prophet models trained: {len(prophet_results)}\")\n",
    "            \n",
    "            # Add aggregated Prophet to model results for comparison\n",
    "            model_results['Prophet Ensemble'] = {\n",
    "                'mae': avg_prophet_mae,\n",
    "                'mape': 0,  # Not calculated for ensemble\n",
    "                'features_used': ['time_series_components'],\n",
    "                'formula': 'Prophet time series decomposition with seasonality',\n",
    "                'predictions': np.full(len(y_test), avg_prophet_mae)  # Placeholder\n",
    "            }\n",
    "        \n",
    "        # Calculate improvements vs benchmark\n",
    "        print(\"\\nðŸ“ˆ ENHANCED MODEL PERFORMANCE vs BENCHMARK:\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        # Sort models by performance\n",
    "        model_performance = []\n",
    "        for model_name, results in model_results.items():\n",
    "            mae = results['mae']\n",
    "            mae_improvement = benchmark_mae - mae\n",
    "            mae_improvement_pct = (mae_improvement / benchmark_mae) * 100\n",
    "            model_performance.append((model_name, mae_improvement_pct, mae))\n",
    "        \n",
    "        model_performance.sort(key=lambda x: x[1], reverse=True)  # Sort by improvement %\n",
    "        \n",
    "        print(\"ðŸ† MODEL RANKING (by improvement %):\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for rank, (model_name, improvement_pct, mae) in enumerate(model_performance, 1):\n",
    "            mape = model_results[model_name].get('mape', 0)\n",
    "            \n",
    "            status = \"âœ… BEATS BENCHMARK\" if improvement_pct > 0 else \"âŒ UNDERPERFORMS\"\n",
    "            tier = \"ðŸ¥‡\" if rank <= 3 else \"ðŸ¥ˆ\" if rank <= 6 else \"ðŸ¥‰\"\n",
    "            \n",
    "            print(f\"{tier} #{rank:2d}. {model_name}\")\n",
    "            print(f\"      MAE: ${mae:,.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "            if mape > 0:\n",
    "                print(f\"      MAPE: {mape:.1f}%\")\n",
    "            print(\"\")\n",
    "            \n",
    "            # Add improvement metrics to results\n",
    "            model_results[model_name]['mae_improvement'] = benchmark_mae - mae\n",
    "            model_results[model_name]['mae_improvement_pct'] = improvement_pct\n",
    "        \n",
    "        # Marketing transformation analysis\n",
    "        self.analyze_marketing_transformations(model_results, X_test, y_test)\n",
    "        \n",
    "        self.models = model_results\n",
    "        \n",
    "        # Add enhanced predictions to test data for comprehensive analysis\n",
    "        test_data = test_data.copy()\n",
    "        for model_name, results in model_results.items():\n",
    "            if 'predictions' in results and len(results['predictions']) == len(test_data):\n",
    "                test_data[f'{model_name}_pred'] = results['predictions']\n",
    "        \n",
    "        return model_results, test_data\n",
    "    \n",
    "    def analyze_marketing_transformations(self, model_results, X_test, y_test):\n",
    "        \"\"\"Analyze the effectiveness of different marketing transformations\"\"\"\n",
    "        print(\"\\nðŸ’° MARKETING TRANSFORMATION EFFECTIVENESS ANALYSIS\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        marketing_models = {\n",
    "            'Marketing Log Model': 'Log transformation (diminishing returns)',\n",
    "            'Marketing Quadratic Model': 'Quadratic transformation (non-linear response)',\n",
    "            'Log-Log Marketing Model': 'Log-log transformation (elasticity interpretation)',\n",
    "            'Hybrid Marketing Model': 'Hybrid approach (multiple transformations)'\n",
    "        }\n",
    "        \n",
    "        best_marketing_model = None\n",
    "        best_improvement = -float('inf')\n",
    "        \n",
    "        print(\"ðŸ“Š MARKETING MODEL COMPARISON:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        for model_name, description in marketing_models.items():\n",
    "            if model_name in model_results:\n",
    "                improvement = model_results[model_name].get('mae_improvement_pct', 0)\n",
    "                mae = model_results[model_name]['mae']\n",
    "                \n",
    "                print(f\"{model_name}:\")\n",
    "                print(f\"   Description: {description}\")\n",
    "                print(f\"   Performance: {improvement:+.1f}% improvement (MAE: ${mae:,.2f})\")\n",
    "                \n",
    "                if improvement > best_improvement:\n",
    "                    best_improvement = improvement\n",
    "                    best_marketing_model = model_name\n",
    "                \n",
    "                # Show top marketing features if available\n",
    "                features_used = model_results[model_name].get('features_used', [])\n",
    "                marketing_features = [f for f in features_used if any(keyword in f for keyword in \n",
    "                                    ['marketing', '_log', '_squared', '_efficiency'])]\n",
    "                \n",
    "                if marketing_features:\n",
    "                    print(f\"   Key marketing features: {', '.join(marketing_features[:5])}\")\n",
    "                print(\"\")\n",
    "        \n",
    "        if best_marketing_model:\n",
    "            print(f\"ðŸ† BEST MARKETING TRANSFORMATION: {best_marketing_model}\")\n",
    "            print(f\"   Achieved {best_improvement:+.1f}% improvement over benchmark\")\n",
    "            print(f\"   This suggests {marketing_models[best_marketing_model].lower()} works best for your data\")\n",
    "        \n",
    "        # Q4 effectiveness analysis\n",
    "        print(\"\\nðŸŽ„ Q4/HOLIDAY SEASONALITY EFFECTIVENESS:\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        q4_model = 'Q4 Enhanced Model'\n",
    "        if q4_model in model_results:\n",
    "            q4_improvement = model_results[q4_model].get('mae_improvement_pct', 0)\n",
    "            q4_mae = model_results[q4_model]['mae']\n",
    "            \n",
    "            print(f\"Q4 Enhanced Model Performance: {q4_improvement:+.1f}% improvement\")\n",
    "            print(f\"MAE: ${q4_mae:,.2f}\")\n",
    "            \n",
    "            if q4_improvement > 5:\n",
    "                print(\"âœ… Strong Q4 seasonality effects detected - holiday features are valuable\")\n",
    "            elif q4_improvement > 0:\n",
    "                print(\"ðŸŸ¡ Moderate Q4 effects - some holiday impact present\")\n",
    "            else:\n",
    "                print(\"âŒ Weak Q4 effects - holiday features may not be significant for your data\")\n",
    "        \n",
    "        # Prophet effectiveness\n",
    "        print(\"\\nðŸ“ˆ PROPHET TIME SERIES EFFECTIVENESS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        prophet_models = [name for name in model_results.keys() if 'Prophet' in name]\n",
    "        if prophet_models:\n",
    "            for prophet_model in prophet_models:\n",
    "                prophet_improvement = model_results[prophet_model].get('mae_improvement_pct', 0)\n",
    "                prophet_mae = model_results[prophet_model]['mae']\n",
    "                \n",
    "                print(f\"{prophet_model}: {prophet_improvement:+.1f}% improvement (MAE: ${prophet_mae:,.2f})\")\n",
    "                \n",
    "                if prophet_improvement > 10:\n",
    "                    print(\"âœ… Prophet captures strong time series patterns\")\n",
    "                elif prophet_improvement > 0:\n",
    "                    print(\"ðŸŸ¡ Prophet provides moderate time series insights\")\n",
    "                else:\n",
    "                    print(\"âŒ Prophet may be overfitting or unsuitable for this data\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No Prophet models available for analysis\")\n",
    "    \n",
    "    # Include all the existing methods from the original class\n",
    "    def engineer_seasonality_features(self, df):\n",
    "        \"\"\"Feature Set 1: Advanced Seasonality Features\"\"\"\n",
    "        print(\"   ðŸ—“ï¸ Engineering seasonality features...\")\n",
    "        \n",
    "        # Basic temporal features\n",
    "        df['month'] = df['order_date'].dt.month\n",
    "        df['quarter'] = df['order_date'].dt.quarter\n",
    "        df['day_of_week'] = df['order_date'].dt.dayofweek\n",
    "        df['day_of_month'] = df['order_date'].dt.day\n",
    "        df['week_of_year'] = df['order_date'].dt.isocalendar().week\n",
    "        \n",
    "        # Holiday proximity\n",
    "        df['is_month_end'] = (df['day_of_month'] >= 28).astype(int)\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df['is_q4'] = (df['quarter'] == 4).astype(int)\n",
    "        \n",
    "        # Cyclical encoding for periodic features\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_product_features(self, df):\n",
    "        \"\"\"Feature Set 2: Product Characteristics & Pricing\"\"\"\n",
    "        print(\"   ðŸ›ï¸ Engineering product features...\")\n",
    "        \n",
    "        if self.order_items_df is not None:\n",
    "            # Calculate product-level metrics\n",
    "            product_stats = self.order_items_df.groupby('product_name').agg({\n",
    "                'sku_gross_sales': ['mean', 'std', 'count'],\n",
    "                'quantity': ['mean', 'sum'],\n",
    "                'local_order_ts': ['min', 'max']\n",
    "            }).round(2)\n",
    "            \n",
    "            product_stats.columns = ['avg_price', 'price_volatility', 'total_orders', \n",
    "                                   'avg_quantity', 'total_quantity', 'first_sale', 'last_sale']\n",
    "            product_stats = product_stats.reset_index()\n",
    "            \n",
    "            # Product age and maturity\n",
    "            product_stats['first_sale'] = pd.to_datetime(product_stats['first_sale'])\n",
    "            product_stats['last_sale'] = pd.to_datetime(product_stats['last_sale'])\n",
    "            \n",
    "            # Calculate product age as of each order date\n",
    "            df = df.merge(product_stats[['product_name', 'avg_price', 'price_volatility', 'first_sale']], \n",
    "                         left_on='sku', right_on='product_name', how='left')\n",
    "            \n",
    "            df['product_age_days'] = (df['order_date'] - df['first_sale']).dt.days\n",
    "            df['product_age_days'] = df['product_age_days'].fillna(0).clip(lower=0)\n",
    "            \n",
    "            # Price positioning\n",
    "            df['is_premium'] = (df['avg_price'] > df['avg_price'].quantile(0.8)).astype(int)\n",
    "            df['is_budget'] = (df['avg_price'] < df['avg_price'].quantile(0.2)).astype(int)\n",
    "            \n",
    "        else:\n",
    "            # Fallback features\n",
    "            df['avg_price'] = 43\n",
    "            df['price_volatility'] = 0\n",
    "            df['product_age_days'] = 30\n",
    "            df['is_premium'] = 0\n",
    "            df['is_budget'] = 0\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_customer_lifecycle_features(self, df):\n",
    "        \"\"\"Feature Set 3: Customer Lifecycle & Behavior Features\"\"\"\n",
    "        print(\"   ðŸ‘¥ Engineering customer lifecycle features...\")\n",
    "        \n",
    "        # Use existing retention rates from benchmark\n",
    "        retention_cols = [col for col in df.columns if 'returning_rate_' in col]\n",
    "        \n",
    "        if retention_cols:\n",
    "            # Customer lifecycle stage\n",
    "            df['avg_retention_1_3m'] = df[[col for col in retention_cols if '1m' in col or '2m' in col or '3m' in col]].mean(axis=1)\n",
    "            df['avg_retention_6_12m'] = df[[col for col in retention_cols if '6m' in col or '12m' in col]].mean(axis=1)\n",
    "            \n",
    "            # Customer loyalty score\n",
    "            df['customer_loyalty_score'] = (df['avg_retention_1_3m'] * 0.3 + df['avg_retention_6_12m'] * 0.7)\n",
    "            \n",
    "        else:\n",
    "            # Fallback\n",
    "            df['avg_retention_1_3m'] = 0.7\n",
    "            df['avg_retention_6_12m'] = 0.4\n",
    "            df['customer_loyalty_score'] = 0.5\n",
    "        \n",
    "        # Customer mix features\n",
    "        if 'new_customers' in df.columns and 'existing_customers' in df.columns:\n",
    "            df['total_customers_calc'] = df['new_customers'] + df['existing_customers']\n",
    "            df['new_customer_ratio'] = df['new_customers'] / (df['total_customers_calc'] + 1)\n",
    "            df['customer_mix_score'] = df['new_customer_ratio'] * 0.3 + (1 - df['new_customer_ratio']) * 0.7\n",
    "        else:\n",
    "            df['new_customer_ratio'] = 0.3\n",
    "            df['customer_mix_score'] = 0.6\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_cross_sku_features(self, df):\n",
    "        \"\"\"Feature Set 4: Cross-SKU Interaction Features\"\"\"\n",
    "        print(\"   ðŸ”— Engineering cross-SKU interaction features...\")\n",
    "        \n",
    "        # Group by date to create market-level features\n",
    "        date_aggregates = df.groupby('order_date').agg({\n",
    "            'actual_demand': ['sum', 'mean', 'std', 'count'],\n",
    "        }).round(2)\n",
    "        \n",
    "        date_aggregates.columns = ['market_total_demand', 'market_avg_demand', 'market_demand_volatility', 'market_sku_count']\n",
    "        date_aggregates = date_aggregates.reset_index()\n",
    "        \n",
    "        # Merge back to main data\n",
    "        df = df.merge(date_aggregates, on='order_date', how='left')\n",
    "        \n",
    "        # Market share and relative positioning\n",
    "        df['market_share'] = df['actual_demand'] / (df['market_total_demand'] + 1)\n",
    "        df['demand_vs_market_avg'] = df['actual_demand'] / (df['market_avg_demand'] + 1)\n",
    "        df['is_top_sku_today'] = (df['actual_demand'] >= df['market_avg_demand'] * 2).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_lag_features(self, df):\n",
    "        \"\"\"Feature Set 5: Historical Lag Features (NO DATA LEAKAGE)\"\"\"\n",
    "        print(\"   ðŸ“ˆ Engineering lag features...\")\n",
    "        \n",
    "        # Sort by SKU and date for lag calculations\n",
    "        df = df.sort_values(['sku', 'order_date'])\n",
    "        \n",
    "        # Create lag features for demand (shifted to avoid leakage)\n",
    "        for lag in [1, 3, 7]:\n",
    "            df[f'demand_lag_{lag}d'] = df.groupby('sku')['actual_demand'].shift(lag)\n",
    "        \n",
    "        # Rolling averages (shifted to avoid leakage)\n",
    "        for window in [3, 7, 14]:\n",
    "            df[f'demand_ma_{window}d'] = df.groupby('sku')['actual_demand'].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        # Fill NaN values with reasonable defaults\n",
    "        lag_cols = [col for col in df.columns if '_lag_' in col or '_ma_' in col]\n",
    "        for col in lag_cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_marketing_insights(self):\n",
    "        \"\"\"Extract insights about marketing effectiveness from trained models\"\"\"\n",
    "        print(\"\\nðŸ’¡ MARKETING EFFECTIVENESS INSIGHTS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        insights = {}\n",
    "        \n",
    "        # Analyze feature importance for marketing features\n",
    "        tree_models = ['Enhanced Random Forest', 'Enhanced Gradient Boosting']\n",
    "        \n",
    "        for model_name in tree_models:\n",
    "            if model_name in self.models and hasattr(self.models[model_name]['model'], 'feature_importances_'):\n",
    "                model = self.models[model_name]['model']\n",
    "                features = self.models[model_name]['features_used']\n",
    "                \n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': features,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                # Extract marketing-related features\n",
    "                marketing_features = importance_df[\n",
    "                    importance_df['feature'].str.contains('marketing|spend|ad', case=False, na=False)\n",
    "                ]\n",
    "                \n",
    "                if not marketing_features.empty:\n",
    "                    print(f\"\\nðŸŒ³ {model_name.upper()} - TOP MARKETING FEATURES:\")\n",
    "                    for i, (_, row) in enumerate(marketing_features.head(5).iterrows(), 1):\n",
    "                        feature_type = \"Log transform\" if \"_log\" in row['feature'] else \\\n",
    "                                     \"Quadratic\" if \"_squared\" in row['feature'] else \\\n",
    "                                     \"Efficiency\" if \"_efficiency\" in row['feature'] else \\\n",
    "                                     \"Raw spend\"\n",
    "                        \n",
    "                        print(f\"   {i}. {row['feature']}: {row['importance']:.4f} ({feature_type})\")\n",
    "                    \n",
    "                    insights[model_name] = marketing_features.head(10)\n",
    "        \n",
    "        # Analyze coefficients for linear models\n",
    "        linear_models = ['Marketing Log Model', 'Marketing Quadratic Model', 'Log-Log Marketing Model']\n",
    "        \n",
    "        for model_name in linear_models:\n",
    "            if model_name in self.models:\n",
    "                model = self.models[model_name]['model']\n",
    "                features = self.models[model_name]['features_used']\n",
    "                \n",
    "                if hasattr(model, 'coef_'):\n",
    "                    coef_df = pd.DataFrame({\n",
    "                        'feature': features,\n",
    "                        'coefficient': model.coef_\n",
    "                    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "                    \n",
    "                    marketing_coefs = coef_df[\n",
    "                        coef_df['feature'].str.contains('marketing|spend|ad', case=False, na=False)\n",
    "                    ]\n",
    "                    \n",
    "                    if not marketing_coefs.empty:\n",
    "                        print(f\"\\nðŸ“Š {model_name.upper()} - MARKETING COEFFICIENTS:\")\n",
    "                        for _, row in marketing_coefs.head(5).iterrows():\n",
    "                            direction = \"ðŸ“ˆ Positive\" if row['coefficient'] > 0 else \"ðŸ“‰ Negative\"\n",
    "                            print(f\"   {row['feature']}: {row['coefficient']:.4f} ({direction} impact)\")\n",
    "        \n",
    "        return insights\n",
    "\n",
    "\n",
    "# Enhanced Model Evaluation with Marketing Analysis\n",
    "class EnhancedMLModelEvaluationAnalysis:\n",
    "    def __init__(self, test_data, model_results, benchmark_col='bm_demand', actual_col='actual_demand'):\n",
    "        \"\"\"Enhanced analysis including marketing transformation effectiveness\"\"\"\n",
    "        print(\"\\nðŸ” ENHANCED ML MODEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"ðŸŽ¯ Including: Marketing transformations, Prophet analysis, Q4 seasonality\")\n",
    "        print(\"=\"*75)\n",
    "        \n",
    "        self.test_data = test_data.copy()\n",
    "        self.model_results = model_results\n",
    "        self.benchmark_col = benchmark_col\n",
    "        self.actual_col = actual_col\n",
    "        \n",
    "        # Add ML model predictions to test data\n",
    "        self.add_ml_predictions_to_test_data()\n",
    "        self.prepare_analysis_data()\n",
    "    \n",
    "    def add_ml_predictions_to_test_data(self):\n",
    "        \"\"\"Add ML model predictions to test dataset\"\"\"\n",
    "        print(\"\\nðŸ“Š Adding enhanced ML predictions to test data...\")\n",
    "        \n",
    "        for model_name, results in self.model_results.items():\n",
    "            if 'predictions' in results:\n",
    "                predictions = results['predictions']\n",
    "                if len(predictions) == len(self.test_data):\n",
    "                    self.test_data[f'{model_name}_pred'] = predictions\n",
    "                    print(f\"   âœ… Added {model_name} predictions\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ {model_name} prediction length mismatch\")\n",
    "    \n",
    "    def prepare_analysis_data(self):\n",
    "        \"\"\"Prepare data for comprehensive analysis\"\"\"\n",
    "        print(\"\\nðŸ”§ Preparing enhanced analysis data...\")\n",
    "        \n",
    "        # Add temporal columns if not present\n",
    "        if 'year_month' not in self.test_data.columns:\n",
    "            self.test_data['order_date'] = pd.to_datetime(self.test_data['order_date'])\n",
    "            self.test_data['year_month'] = self.test_data['order_date'].dt.to_period('M')\n",
    "        \n",
    "        # Calculate benchmark errors\n",
    "        self.test_data['benchmark_error'] = self.test_data[self.actual_col] - self.test_data[self.benchmark_col]\n",
    "        self.test_data['benchmark_abs_error'] = np.abs(self.test_data['benchmark_error'])\n",
    "        self.test_data['benchmark_abs_pct_error'] = np.abs(self.test_data['benchmark_error'] / self.test_data[self.actual_col] * 100)\n",
    "        \n",
    "        # Calculate ML model errors\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            self.test_data[f'{model_name}_error'] = self.test_data[self.actual_col] - self.test_data[pred_col]\n",
    "            self.test_data[f'{model_name}_abs_error'] = np.abs(self.test_data[f'{model_name}_error'])\n",
    "            self.test_data[f'{model_name}_abs_pct_error'] = np.abs(self.test_data[f'{model_name}_error'] / self.test_data[self.actual_col] * 100)\n",
    "        \n",
    "        print(f\"   âœ… Enhanced analysis data prepared: {len(self.test_data):,} records\")\n",
    "        print(f\"   ðŸ“Š Enhanced ML models found: {len(ml_pred_cols)}\")\n",
    "    \n",
    "    def analyze_marketing_model_effectiveness(self):\n",
    "        \"\"\"Analyze effectiveness of different marketing transformation approaches\"\"\"\n",
    "        print(\"\\nðŸ’° MARKETING TRANSFORMATION MODEL EFFECTIVENESS\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        marketing_models = [name for name in self.model_results.keys() if \n",
    "                          any(keyword in name for keyword in ['Marketing', 'Log-Log', 'Hybrid'])]\n",
    "        \n",
    "        if not marketing_models:\n",
    "            print(\"âŒ No marketing transformation models found\")\n",
    "            return {}\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        \n",
    "        marketing_analysis = {}\n",
    "        \n",
    "        print(\"ðŸ“Š MARKETING MODEL COMPARISON:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        for model_name in marketing_models:\n",
    "            if f'{model_name}_abs_error' in self.test_data.columns:\n",
    "                ml_mae = self.test_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement = ((benchmark_mae - ml_mae) / benchmark_mae) * 100\n",
    "                \n",
    "                # Analyze by marketing spend levels\n",
    "                if 'marketing_spend' in self.test_data.columns or any('marketing' in col for col in self.test_data.columns):\n",
    "                    # Find a marketing spend column\n",
    "                    marketing_col = None\n",
    "                    for col in self.test_data.columns:\n",
    "                        if 'marketing' in col.lower() and 'spend' in col.lower():\n",
    "                            marketing_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if marketing_col:\n",
    "                        # Analyze performance by marketing spend quintiles\n",
    "                        quintiles = pd.qcut(self.test_data[marketing_col], 5, labels=['Low', 'Low-Med', 'Medium', 'Med-High', 'High'])\n",
    "                        quintile_performance = {}\n",
    "                        \n",
    "                        for quintile in ['Low', 'Low-Med', 'Medium', 'Med-High', 'High']:\n",
    "                            quintile_data = self.test_data[quintiles == quintile]\n",
    "                            if len(quintile_data) > 0:\n",
    "                                quintile_mae = quintile_data[f'{model_name}_abs_error'].mean()\n",
    "                                quintile_benchmark = quintile_data['benchmark_abs_error'].mean()\n",
    "                                quintile_improvement = ((quintile_benchmark - quintile_mae) / quintile_benchmark) * 100\n",
    "                                quintile_performance[quintile] = quintile_improvement\n",
    "                \n",
    "                transformation_type = \"Log transformation\" if \"Log\" in model_name else \\\n",
    "                                    \"Quadratic transformation\" if \"Quadratic\" in model_name else \\\n",
    "                                    \"Log-log transformation\" if \"Log-Log\" in model_name else \\\n",
    "                                    \"Hybrid approach\"\n",
    "                \n",
    "                print(f\"{model_name}:\")\n",
    "                print(f\"   Transformation: {transformation_type}\")\n",
    "                print(f\"   Overall improvement: {improvement:+.1f}%\")\n",
    "                print(f\"   MAE: ${ml_mae:,.2f}\")\n",
    "                \n",
    "                if 'quintile_performance' in locals():\n",
    "                    print(f\"   Performance by marketing spend:\")\n",
    "                    for quintile, perf in quintile_performance.items():\n",
    "                        print(f\"     {quintile} spend: {perf:+.1f}%\")\n",
    "                \n",
    "                marketing_analysis[model_name] = {\n",
    "                    'improvement': improvement,\n",
    "                    'mae': ml_mae,\n",
    "                    'transformation_type': transformation_type,\n",
    "                    'quintile_performance': quintile_performance if 'quintile_performance' in locals() else {}\n",
    "                }\n",
    "                print(\"\")\n",
    "        \n",
    "        # Determine best marketing approach\n",
    "        if marketing_analysis:\n",
    "            best_marketing_model = max(marketing_analysis.keys(), \n",
    "                                     key=lambda x: marketing_analysis[x]['improvement'])\n",
    "            \n",
    "            print(f\"ðŸ† BEST MARKETING TRANSFORMATION:\")\n",
    "            print(f\"   Model: {best_marketing_model}\")\n",
    "            print(f\"   Improvement: {marketing_analysis[best_marketing_model]['improvement']:+.1f}%\")\n",
    "            print(f\"   Approach: {marketing_analysis[best_marketing_model]['transformation_type']}\")\n",
    "        \n",
    "        return marketing_analysis\n",
    "    \n",
    "    def analyze_q4_seasonality_effectiveness(self):\n",
    "        \"\"\"Analyze Q4 and holiday seasonality model effectiveness\"\"\"\n",
    "        print(\"\\nðŸŽ„ Q4 SEASONALITY EFFECTIVENESS ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Filter test data for Q4 months\n",
    "        q4_data = self.test_data[self.test_data['order_date'].dt.quarter == 4]\n",
    "        non_q4_data = self.test_data[self.test_data['order_date'].dt.quarter != 4]\n",
    "        \n",
    "        if len(q4_data) == 0:\n",
    "            print(\"âŒ No Q4 data available for analysis\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"ðŸ“Š Q4 Data: {len(q4_data):,} records\")\n",
    "        print(f\"ðŸ“Š Non-Q4 Data: {len(non_q4_data):,} records\")\n",
    "        print(\"\")\n",
    "        \n",
    "        q4_analysis = {}\n",
    "        \n",
    "        # Find Q4-enhanced models\n",
    "        q4_models = [name for name in self.model_results.keys() if 'Q4' in name or 'Enhanced' in name]\n",
    "        \n",
    "        benchmark_mae_q4 = q4_data['benchmark_abs_error'].mean()\n",
    "        benchmark_mae_non_q4 = non_q4_data['benchmark_abs_error'].mean()\n",
    "        \n",
    "        print(\"ðŸŽ¯ BENCHMARK PERFORMANCE:\")\n",
    "        print(f\"   Q4 MAE: ${benchmark_mae_q4:,.2f}\")\n",
    "        print(f\"   Non-Q4 MAE: ${benchmark_mae_non_q4:,.2f}\")\n",
    "        print(f\"   Q4 vs Non-Q4 difference: {((benchmark_mae_q4 - benchmark_mae_non_q4) / benchmark_mae_non_q4) * 100:+.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # Analyze all models for Q4 effectiveness\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        print(\"ðŸ¤– ML MODEL Q4 vs NON-Q4 PERFORMANCE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            # Q4 performance\n",
    "            q4_ml_mae = q4_data[f'{model_name}_abs_error'].mean()\n",
    "            q4_improvement = ((benchmark_mae_q4 - q4_ml_mae) / benchmark_mae_q4) * 100\n",
    "            \n",
    "            # Non-Q4 performance\n",
    "            non_q4_ml_mae = non_q4_data[f'{model_name}_abs_error'].mean()\n",
    "            non_q4_improvement = ((benchmark_mae_non_q4 - non_q4_ml_mae) / benchmark_mae_non_q4) * 100\n",
    "            \n",
    "            # Q4 specialization score (how much better in Q4 vs non-Q4)\n",
    "            q4_specialization = q4_improvement - non_q4_improvement\n",
    "            \n",
    "            print(f\"{model_name}:\")\n",
    "            print(f\"   Q4 improvement: {q4_improvement:+.1f}% (MAE: ${q4_ml_mae:,.2f})\")\n",
    "            print(f\"   Non-Q4 improvement: {non_q4_improvement:+.1f}% (MAE: ${non_q4_ml_mae:,.2f})\")\n",
    "            print(f\"   Q4 specialization: {q4_specialization:+.1f}% {'âœ…' if q4_specialization > 0 else 'âŒ'}\")\n",
    "            print(\"\")\n",
    "            \n",
    "            q4_analysis[model_name] = {\n",
    "                'q4_improvement': q4_improvement,\n",
    "                'non_q4_improvement': non_q4_improvement,\n",
    "                'q4_specialization': q4_specialization,\n",
    "                'q4_mae': q4_ml_mae,\n",
    "                'non_q4_mae': non_q4_ml_mae\n",
    "            }\n",
    "        \n",
    "        # Find best Q4 specialist\n",
    "        if q4_analysis:\n",
    "            best_q4_model = max(q4_analysis.keys(), \n",
    "                               key=lambda x: q4_analysis[x]['q4_specialization'])\n",
    "            \n",
    "            print(f\"ðŸ† BEST Q4 SPECIALIST MODEL:\")\n",
    "            print(f\"   Model: {best_q4_model}\")\n",
    "            print(f\"   Q4 specialization score: {q4_analysis[best_q4_model]['q4_specialization']:+.1f}%\")\n",
    "            \n",
    "            if q4_analysis[best_q4_model]['q4_specialization'] > 5:\n",
    "                print(\"   âœ… Strong Q4 seasonality captured\")\n",
    "            elif q4_analysis[best_q4_model]['q4_specialization'] > 0:\n",
    "                print(\"   ðŸŸ¡ Moderate Q4 seasonality effects\")\n",
    "            else:\n",
    "                print(\"   âŒ No significant Q4 specialization\")\n",
    "        \n",
    "        return q4_analysis\n",
    "    \n",
    "    def analyze_prophet_effectiveness(self):\n",
    "        \"\"\"Analyze Prophet model effectiveness and time series insights\"\"\"\n",
    "        print(\"\\nðŸ“ˆ PROPHET MODEL EFFECTIVENESS ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        prophet_models = [name for name in self.model_results.keys() if 'Prophet' in name]\n",
    "        \n",
    "        if not prophet_models:\n",
    "            print(\"âŒ No Prophet models found for analysis\")\n",
    "            return {}\n",
    "        \n",
    "        prophet_analysis = {}\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        \n",
    "        for prophet_model in prophet_models:\n",
    "            if f'{prophet_model}_abs_error' in self.test_data.columns:\n",
    "                prophet_mae = self.test_data[f'{prophet_model}_abs_error'].mean()\n",
    "                improvement = ((benchmark_mae - prophet_mae) / benchmark_mae) * 100\n",
    "                \n",
    "                print(f\"{prophet_model}:\")\n",
    "                print(f\"   Overall improvement: {improvement:+.1f}%\")\n",
    "                print(f\"   MAE: ${prophet_mae:,.2f}\")\n",
    "                \n",
    "                # Analyze Prophet performance by time periods\n",
    "                monthly_performance = []\n",
    "                \n",
    "                for month in sorted(self.test_data['year_month'].unique()):\n",
    "                    month_data = self.test_data[self.test_data['year_month'] == month]\n",
    "                    \n",
    "                    if len(month_data) > 0:\n",
    "                        month_benchmark = month_data['benchmark_abs_error'].mean()\n",
    "                        month_prophet = month_data[f'{prophet_model}_abs_error'].mean()\n",
    "                        month_improvement = ((month_benchmark - month_prophet) / month_benchmark) * 100\n",
    "                        \n",
    "                        monthly_performance.append({\n",
    "                            'month': month,\n",
    "                            'improvement': month_improvement,\n",
    "                            'mae': month_prophet\n",
    "                        })\n",
    "                \n",
    "                # Calculate consistency\n",
    "                improvements = [perf['improvement'] for perf in monthly_performance]\n",
    "                consistency_score = 100 - np.std(improvements)  # Lower std = higher consistency\n",
    "                \n",
    "                print(f\"   Monthly consistency: {consistency_score:.1f}/100\")\n",
    "                print(f\"   Best month: {max(monthly_performance, key=lambda x: x['improvement'])['month']} \"\n",
    "                      f\"({max(improvements):+.1f}%)\")\n",
    "                print(f\"   Worst month: {min(monthly_performance, key=lambda x: x['improvement'])['month']} \"\n",
    "                      f\"({min(improvements):+.1f}%)\")\n",
    "                print(\"\")\n",
    "                \n",
    "                prophet_analysis[prophet_model] = {\n",
    "                    'overall_improvement': improvement,\n",
    "                    'mae': prophet_mae,\n",
    "                    'monthly_performance': monthly_performance,\n",
    "                    'consistency_score': consistency_score\n",
    "                }\n",
    "        \n",
    "        return prophet_analysis\n",
    "    \n",
    "    def run_enhanced_comprehensive_analysis(self):\n",
    "        \"\"\"Run all enhanced analyses\"\"\"\n",
    "        print(\"ðŸš€ RUNNING ENHANCED COMPREHENSIVE ML MODEL ANALYSIS\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # Standard analyses\n",
    "            results['overall_accuracy'] = self.analyze_overall_accuracy()\n",
    "            results['weighted_accuracy'] = self.analyze_weighted_accuracy()\n",
    "            results['channel_performance'] = self.analyze_channel_performance()\n",
    "            results['top_sku_performance'] = self.analyze_top_sku_performance()\n",
    "            results['temporal_performance'] = self.analyze_temporal_performance()\n",
    "            \n",
    "            # Enhanced analyses\n",
    "            results['marketing_effectiveness'] = self.analyze_marketing_model_effectiveness()\n",
    "            results['q4_seasonality'] = self.analyze_q4_seasonality_effectiveness()\n",
    "            results['prophet_effectiveness'] = self.analyze_prophet_effectiveness()\n",
    "            \n",
    "            # Final enhanced recommendations\n",
    "            self.provide_enhanced_recommendations(results)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during enhanced analysis: {str(e)}\")\n",
    "            return results\n",
    "    \n",
    "    def provide_enhanced_recommendations(self, results):\n",
    "        \"\"\"Provide enhanced recommendations based on all analyses\"\"\"\n",
    "        print(\"\\nðŸŽ¯ ENHANCED MODEL RECOMMENDATIONS\")\n",
    "        print(\"=\"*45)\n",
    "        \n",
    "        print(\"ðŸ“Š MARKETING TRANSFORMATION RECOMMENDATIONS:\")\n",
    "        marketing_results = results.get('marketing_effectiveness', {})\n",
    "        if marketing_results:\n",
    "            best_marketing = max(marketing_results.keys(), \n",
    "                               key=lambda x: marketing_results[x]['improvement'])\n",
    "            print(f\"   ðŸ† Best marketing approach: {best_marketing}\")\n",
    "            print(f\"   ðŸ“ˆ Improvement: {marketing_results[best_marketing]['improvement']:+.1f}%\")\n",
    "            print(f\"   ðŸ’¡ Strategy: {marketing_results[best_marketing]['transformation_type']}\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ No marketing transformation analysis available\")\n",
    "        \n",
    "        print(\"\\nðŸŽ„ Q4 SEASONALITY RECOMMENDATIONS:\")\n",
    "        q4_results = results.get('q4_seasonality', {})\n",
    "        if q4_results:\n",
    "            best_q4_specialist = max(q4_results.keys(), \n",
    "                                   key=lambda x: q4_results[x]['q4_specialization'])\n",
    "            specialization_score = q4_results[best_q4_specialist]['q4_specialization']\n",
    "            \n",
    "            print(f\"   ðŸ† Best Q4 model: {best_q4_specialist}\")\n",
    "            print(f\"   ðŸŽ¯ Q4 specialization: {specialization_score:+.1f}%\")\n",
    "            \n",
    "            if specialization_score > 5:\n",
    "                print(\"   âœ… Strong Q4 effects - implement separate holiday models\")\n",
    "            elif specialization_score > 0:\n",
    "                print(\"   ðŸŸ¡ Moderate Q4 effects - consider Q4 feature engineering\")\n",
    "            else:\n",
    "                print(\"   âŒ Weak Q4 effects - focus on other factors\")\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ PROPHET RECOMMENDATIONS:\")\n",
    "        prophet_results = results.get('prophet_effectiveness', {})\n",
    "        if prophet_results:\n",
    "            best_prophet = max(prophet_results.keys(), \n",
    "                             key=lambda x: prophet_results[x]['overall_improvement'])\n",
    "            prophet_improvement = prophet_results[best_prophet]['overall_improvement']\n",
    "            consistency = prophet_results[best_prophet]['consistency_score']\n",
    "            \n",
    "            print(f\"   ðŸ† Best Prophet model: {best_prophet}\")\n",
    "            print(f\"   ðŸ“ˆ Improvement: {prophet_improvement:+.1f}%\")\n",
    "            print(f\"   ðŸ“Š Consistency: {consistency:.1f}/100\")\n",
    "            \n",
    "            if prophet_improvement > 10 and consistency > 70:\n",
    "                print(\"   âœ… Prophet highly effective - use for forecasting\")\n",
    "            elif prophet_improvement > 0:\n",
    "                print(\"   ðŸŸ¡ Prophet moderately effective - use as ensemble component\")\n",
    "            else:\n",
    "                print(\"   âŒ Prophet ineffective - stick to ML models\")\n",
    "        \n",
    "        print(\"\\nðŸ† OVERALL STRATEGY RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Find overall best model\n",
    "        overall_results = results.get('overall_accuracy', {})\n",
    "        if overall_results:\n",
    "            # Remove benchmark from comparison\n",
    "            ml_results = {k: v for k, v in overall_results.items() if k != 'benchmark'}\n",
    "            \n",
    "            if ml_results:\n",
    "                best_overall = max(ml_results.keys(), \n",
    "                                 key=lambda x: ml_results[x].get('mae_improvement_pct', 0))\n",
    "                best_improvement = ml_results[best_overall].get('mae_improvement_pct', 0)\n",
    "                \n",
    "                print(f\"   ðŸ¥‡ Best overall model: {best_overall}\")\n",
    "                print(f\"   ðŸ“ˆ Best improvement: {best_improvement:+.1f}%\")\n",
    "                \n",
    "                if best_improvement > 15:\n",
    "                    strategy = \"DEPLOY IMMEDIATELY\"\n",
    "                    confidence = \"HIGH\"\n",
    "                elif best_improvement > 5:\n",
    "                    strategy = \"PILOT DEPLOYMENT\"\n",
    "                    confidence = \"MEDIUM\"\n",
    "                else:\n",
    "                    strategy = \"CONTINUE DEVELOPMENT\"\n",
    "                    confidence = \"LOW\"\n",
    "                \n",
    "                print(f\"   ðŸŽ¯ Recommended strategy: {strategy}\")\n",
    "                print(f\"   ðŸŽª Confidence level: {confidence}\")\n",
    "    \n",
    "    # Include all existing methods from the original MLModelEvaluationAnalysis class\n",
    "    def analyze_overall_accuracy(self):\n",
    "        \"\"\"1. Overall Accuracy Analysis\"\"\"\n",
    "        print(\"\\nðŸ“ˆ 1. OVERALL ACCURACY ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        if not ml_pred_cols:\n",
    "            print(\"âŒ No ML model predictions found\")\n",
    "            return {}\n",
    "        \n",
    "        accuracy_results = {}\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        benchmark_mape = self.test_data['benchmark_abs_pct_error'].mean()\n",
    "        \n",
    "        print(f\"ðŸŽ¯ BENCHMARK PERFORMANCE:\")\n",
    "        print(f\"   MAE:  ${benchmark_mae:,.2f}\")\n",
    "        print(f\"   MAPE: {benchmark_mape:.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        accuracy_results['benchmark'] = {'mae': benchmark_mae, 'mape': benchmark_mape}\n",
    "        \n",
    "        # ML model performance\n",
    "        print(f\"ðŸ¤– ML MODEL PERFORMANCE:\")\n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            ml_mae = self.test_data[f'{model_name}_abs_error'].mean()\n",
    "            ml_mape = self.test_data[f'{model_name}_abs_pct_error'].mean()\n",
    "            \n",
    "            mae_improvement = benchmark_mae - ml_mae\n",
    "            mae_improvement_pct = (mae_improvement / benchmark_mae) * 100\n",
    "            mape_improvement = benchmark_mape - ml_mape\n",
    "            \n",
    "            status = \"âœ… BETTER\" if mae_improvement > 0 else \"âŒ WORSE\"\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            print(f\"     MAE:  ${ml_mae:,.2f} (${mae_improvement:+,.2f}, {mae_improvement_pct:+.1f}%) {status}\")\n",
    "            print(f\"     MAPE: {ml_mape:.1f}% ({mape_improvement:+.1f}%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            accuracy_results[model_name] = {\n",
    "                'mae': ml_mae,\n",
    "                'mape': ml_mape,\n",
    "                'mae_improvement': mae_improvement,\n",
    "                'mae_improvement_pct': mae_improvement_pct,\n",
    "                'mape_improvement': mape_improvement\n",
    "            }\n",
    "        \n",
    "        return accuracy_results\n",
    "    \n",
    "    def analyze_weighted_accuracy(self):\n",
    "        \"\"\"2. Weighted Accuracy by Product Revenue\"\"\"\n",
    "        print(\"\\nðŸ’° 2. WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        if 'sku' not in self.test_data.columns:\n",
    "            print(\"âŒ No SKU column found for product analysis\")\n",
    "            return {}\n",
    "        \n",
    "        # Calculate product weights by revenue\n",
    "        product_revenue = self.test_data.groupby('sku')[self.actual_col].sum().sort_values(ascending=False)\n",
    "        top_20_pct_threshold = product_revenue.quantile(0.8)\n",
    "        top_products = product_revenue[product_revenue >= top_20_pct_threshold].index\n",
    "        \n",
    "        top_product_data = self.test_data[self.test_data['sku'].isin(top_products)]\n",
    "        bottom_product_data = self.test_data[~self.test_data['sku'].isin(top_products)]\n",
    "        \n",
    "        print(f\"ðŸ“Š PRODUCT SEGMENTATION:\")\n",
    "        print(f\"   Top 20% Products: {len(top_products)} SKUs\")\n",
    "        print(f\"   Bottom 80% Products: {len(product_revenue) - len(top_products)} SKUs\")\n",
    "        print(\"\")\n",
    "        \n",
    "        weighted_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for segment_name, segment_data in [('Top 20% Products', top_product_data), ('Bottom 80% Products', bottom_product_data)]:\n",
    "            print(f\"ðŸŽ¯ {segment_name.upper()}:\")\n",
    "            \n",
    "            bench_mae = segment_data['benchmark_abs_error'].mean()\n",
    "            print(f\"   Benchmark MAE: ${bench_mae:.2f}\")\n",
    "            \n",
    "            segment_results = {'benchmark': {'mae': bench_mae}}\n",
    "            \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = segment_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                status = \"âœ…\" if improvement_pct > 0 else \"âŒ\"\n",
    "                print(f\"   {model_name}: ${ml_mae:.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                segment_results[model_name] = {'mae': ml_mae, 'improvement_pct': improvement_pct}\n",
    "            \n",
    "            weighted_results[segment_name] = segment_results\n",
    "            print(\"\")\n",
    "        \n",
    "        return weighted_results\n",
    "    \n",
    "    def analyze_channel_performance(self):\n",
    "        \"\"\"3. Channel Performance Analysis\"\"\"\n",
    "        print(\"\\nðŸ“º 3. CHANNEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'channel' not in self.test_data.columns:\n",
    "            print(\"âŒ No channel column found\")\n",
    "            return {}\n",
    "        \n",
    "        channel_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for channel in self.test_data['channel'].unique():\n",
    "            channel_data = self.test_data[self.test_data['channel'] == channel]\n",
    "            \n",
    "            print(f\"ðŸ“» {channel.upper()} CHANNEL:\")\n",
    "            print(f\"   Records: {len(channel_data):,}\")\n",
    "            \n",
    "            bench_mae = channel_data['benchmark_abs_error'].mean()\n",
    "            print(f\"   Benchmark MAE: ${bench_mae:.2f}\")\n",
    "            \n",
    "            channel_results[channel] = {'benchmark': {'mae': bench_mae}}\n",
    "            \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = channel_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                status = \"âœ… BETTER\" if improvement_pct > 0 else \"âŒ WORSE\"\n",
    "                print(f\"   {model_name}: ${ml_mae:.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                channel_results[channel][model_name] = {'mae': ml_mae, 'improvement_pct': improvement_pct}\n",
    "            \n",
    "            print(\"\")\n",
    "        \n",
    "        return channel_results\n",
    "    \n",
    "    def analyze_top_sku_performance(self):\n",
    "        \"\"\"4. Top SKU Performance Analysis\"\"\"\n",
    "        print(\"\\nðŸ† 4. TOP SKU PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get top 10 SKUs by revenue\n",
    "        sku_revenue = self.test_data.groupby('sku')[self.actual_col].sum().sort_values(ascending=False)\n",
    "        top_10_skus = sku_revenue.head(10).index\n",
    "        \n",
    "        print(f\"ðŸ“Š TOP 10 SKUs by Revenue:\")\n",
    "        for i, sku in enumerate(top_10_skus, 1):\n",
    "            revenue = sku_revenue[sku]\n",
    "            pct_of_total = (revenue / sku_revenue.sum()) * 100\n",
    "            print(f\"   {i:2d}. {sku}: ${revenue:,.0f} ({pct_of_total:.1f}% of total)\")\n",
    "        print(\"\")\n",
    "        \n",
    "        sku_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        # Analyze each top SKU individually\n",
    "        for rank, sku in enumerate(top_10_skus, 1):\n",
    "            sku_data = self.test_data[self.test_data['sku'] == sku]\n",
    "            \n",
    "            if len(sku_data) < 5:\n",
    "                continue\n",
    "            \n",
    "            bench_mae = sku_data['benchmark_abs_error'].mean()\n",
    "            sku_results[sku] = {\n",
    "                'rank': rank,\n",
    "                'revenue': sku_revenue[sku],\n",
    "                'benchmark': {'mae': bench_mae}\n",
    "            }\n",
    "            \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = sku_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                sku_results[sku][model_name] = {\n",
    "                    'mae': ml_mae,\n",
    "                    'improvement_pct': improvement_pct\n",
    "                }\n",
    "        \n",
    "        return sku_results\n",
    "    \n",
    "    def analyze_temporal_performance(self):\n",
    "        \"\"\"5. Temporal Performance Analysis\"\"\"\n",
    "        print(\"\\nðŸ“… 5. TEMPORAL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        temporal_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            monthly_comparison = []\n",
    "            \n",
    "            for month in sorted(self.test_data['year_month'].unique()):\n",
    "                month_data = self.test_data[self.test_data['year_month'] == month]\n",
    "                \n",
    "                if len(month_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                bench_mae = month_data['benchmark_abs_error'].mean()\n",
    "                ml_mae = month_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                monthly_comparison.append({\n",
    "                    'month': month,\n",
    "                    'improvement_pct': improvement_pct,\n",
    "                    'wins_benchmark': improvement_pct > 0\n",
    "                })\n",
    "            \n",
    "            comparison_df = pd.DataFrame(monthly_comparison)\n",
    "            \n",
    "            if len(comparison_df) > 0:\n",
    "                win_rate = (comparison_df['wins_benchmark'].sum() / len(comparison_df)) * 100\n",
    "                avg_improvement = comparison_df['improvement_pct'].mean()\n",
    "                \n",
    "                temporal_results[model_name] = {\n",
    "                    'win_rate': win_rate,\n",
    "                    'avg_improvement': avg_improvement,\n",
    "                    'monthly_details': comparison_df\n",
    "                }\n",
    "        \n",
    "        return temporal_results\n",
    "\n",
    "\n",
    "# Main Enhanced Pipeline Function\n",
    "def run_enhanced_demand_analysis_with_prophet_and_marketing(\n",
    "    benchmark_df, \n",
    "    amazon_order_items=None, tiktok_order_items=None, shopify_order_items=None,\n",
    "    amazon_daily_sku=None, tiktok_daily_sku=None, shopify_daily_sku=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete enhanced demand analysis pipeline with Prophet and marketing transformations\n",
    "    \n",
    "    New Features:\n",
    "    - Prophet time series forecasting\n",
    "    - Marketing spend transformations (log, quadratic, log-log)\n",
    "    - Enhanced Q4/holiday seasonality\n",
    "    - Diminishing returns modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ ENHANCED SKU DEMAND ANALYSIS WITH PROPHET & MARKETING\")\n",
    "    print(\"ðŸŽ¯ NEW: Prophet forecasting + Marketing transformations + Q4 analysis\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    # Step 1: Train enhanced models\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"STEP 1: ENHANCED MODEL TRAINING WITH MARKETING & PROPHET\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    enhanced_modeler = EnhancedDemandModels(\n",
    "        benchmark_df, \n",
    "        amazon_order_items, tiktok_order_items, shopify_order_items,\n",
    "        amazon_daily_sku, tiktok_daily_sku, shopify_daily_sku\n",
    "    )\n",
    "    \n",
    "    enhanced_model_results, enhanced_test_data = enhanced_modeler.evaluate_enhanced_models()\n",
    "    \n",
    "    # Step 2: Comprehensive enhanced evaluation\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"STEP 2: ENHANCED COMPREHENSIVE EVALUATION\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    enhanced_analyzer = EnhancedMLModelEvaluationAnalysis(\n",
    "        enhanced_test_data, enhanced_model_results\n",
    "    )\n",
    "    enhanced_analysis_results = enhanced_analyzer.run_enhanced_comprehensive_analysis()\n",
    "    \n",
    "    # Step 3: Marketing insights extraction\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"STEP 3: MARKETING EFFECTIVENESS INSIGHTS\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    marketing_insights = enhanced_modeler.get_marketing_insights()\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ ENHANCED ANALYSIS COMPLETE!\")\n",
    "    print(\"ðŸ“Š Enhanced features delivered:\")\n",
    "    print(\"   âœ… Prophet time series forecasting\")\n",
    "    print(\"   âœ… Marketing spend transformations (log, quadratic, log-log)\")\n",
    "    print(\"   âœ… Enhanced Q4 and holiday seasonality\")\n",
    "    print(\"   âœ… Diminishing returns modeling\")\n",
    "    print(\"   âœ… Marketing effectiveness analysis\")\n",
    "    print(\"   âœ… Q4 specialization scoring\")\n",
    "    print(\"   âœ… Prophet vs ML model comparison\")\n",
    "    \n",
    "    return enhanced_modeler, enhanced_analyzer, enhanced_analysis_results, marketing_insights\n",
    "\n",
    "\n",
    "# Quick usage example:\n",
    "\n",
    "enhanced_modeler, enhanced_analyzer, enhanced_results, marketing_insights = run_enhanced_demand_analysis_with_prophet_and_marketing(\n",
    "    benchmark_df=your_benchmark_data,\n",
    "    amazon_order_items=your_amazon_data,\n",
    "    # ... other channel data\n",
    ")\n",
    "\n",
    "# Extract specific insights\n",
    "marketing_effectiveness = enhanced_results['marketing_effectiveness']\n",
    "q4_seasonality = enhanced_results['q4_seasonality']\n",
    "prophet_performance = enhanced_results['prophet_effectiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333475f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_modeler, enhanced_analyzer, enhanced_results, marketing_insights = run_enhanced_demand_analysis_with_prophet_and_marketing(\n",
    "    benchmark_df=enhanced_benchmark_df,\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items,\n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")\n",
    "\n",
    "# Get specific insights\n",
    "marketing_effectiveness = enhanced_results['marketing_effectiveness']\n",
    "q4_performance = enhanced_results['q4_seasonality'] \n",
    "prophet_insights = enhanced_results['prophet_effectiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ComprehensiveModelAnalysis:\n",
    "    \"\"\"Analyze performance patterns across all models to identify universal insights\"\"\"\n",
    "    \n",
    "    def __init__(self, test_data, model_results, target_col='demand_pct_change'):\n",
    "        self.test_data = test_data.copy()\n",
    "        self.model_results = model_results\n",
    "        self.target_col = target_col\n",
    "        self.model_names = list(model_results.keys())\n",
    "        \n",
    "        # Add all model predictions to test data\n",
    "        for model_name, results in model_results.items():\n",
    "            if 'predictions' in results and len(results['predictions']) == len(test_data):\n",
    "                self.test_data[f'{model_name}_pred'] = results['predictions']\n",
    "                self.test_data[f'{model_name}_error'] = self.test_data[target_col] - results['predictions']\n",
    "                self.test_data[f'{model_name}_abs_error'] = np.abs(self.test_data[f'{model_name}_error'])\n",
    "    \n",
    "    def analyze_universal_sku_predictability(self):\n",
    "        \"\"\"Identify SKUs that are consistently predictable/unpredictable across ALL models\"\"\"\n",
    "        print(\"UNIVERSAL SKU PREDICTABILITY ANALYSIS\")\n",
    "        print(\"=\"*45)\n",
    "        \n",
    "        # Calculate MAE for each SKU across all models\n",
    "        sku_performance_all_models = {}\n",
    "        \n",
    "        for sku in self.test_data['sku'].unique():\n",
    "            sku_data = self.test_data[self.test_data['sku'] == sku]\n",
    "            \n",
    "            if len(sku_data) < 5:  # Need minimum observations\n",
    "                continue\n",
    "                \n",
    "            sku_performance = {}\n",
    "            for model_name in self.model_names:\n",
    "                if f'{model_name}_abs_error' in sku_data.columns:\n",
    "                    sku_performance[model_name] = sku_data[f'{model_name}_abs_error'].mean()\n",
    "            \n",
    "            if len(sku_performance) >= 3:  # Need at least 3 models\n",
    "                sku_performance_all_models[sku] = sku_performance\n",
    "        \n",
    "        # Create SKU performance matrix\n",
    "        sku_performance_df = pd.DataFrame(sku_performance_all_models).T\n",
    "        sku_performance_df['mean_mae'] = sku_performance_df.mean(axis=1)\n",
    "        sku_performance_df['mae_std'] = sku_performance_df.std(axis=1)\n",
    "        sku_performance_df['n_observations'] = [\n",
    "            len(self.test_data[self.test_data['sku'] == sku]) for sku in sku_performance_df.index\n",
    "        ]\n",
    "        \n",
    "        # Sort by mean performance across all models\n",
    "        sku_performance_df = sku_performance_df.sort_values('mean_mae')\n",
    "        \n",
    "        print(f\"Analyzed {len(sku_performance_df)} SKUs across {len(self.model_names)} models\")\n",
    "        print()\n",
    "        \n",
    "        print(\"TOP 10 UNIVERSALLY PREDICTABLE SKUs:\")\n",
    "        print(\"-\" * 40)\n",
    "        for sku, row in sku_performance_df.head(10).iterrows():\n",
    "            consistency = \"High\" if row['mae_std'] < 2 else \"Medium\" if row['mae_std'] < 5 else \"Low\"\n",
    "            print(f\"{sku}: {row['mean_mae']:.1f}% avg MAE (consistency: {consistency}, n={row['n_observations']})\")\n",
    "        \n",
    "        print(\"\\nTOP 10 UNIVERSALLY UNPREDICTABLE SKUs:\")\n",
    "        print(\"-\" * 42)\n",
    "        for sku, row in sku_performance_df.tail(10).iterrows():\n",
    "            consistency = \"High\" if row['mae_std'] < 2 else \"Medium\" if row['mae_std'] < 5 else \"Low\"\n",
    "            print(f\"{sku}: {row['mean_mae']:.1f}% avg MAE (consistency: {consistency}, n={row['n_observations']})\")\n",
    "        \n",
    "        # Analyze what makes SKUs predictable\n",
    "        self._analyze_predictability_drivers(sku_performance_df)\n",
    "        \n",
    "        return sku_performance_df\n",
    "    \n",
    "    def _analyze_predictability_drivers(self, sku_performance_df):\n",
    "        \"\"\"Analyze what characteristics drive SKU predictability\"\"\"\n",
    "        print(\"\\nPREDICTABILITY DRIVERS ANALYSIS:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Merge with SKU characteristics\n",
    "        sku_chars = self.test_data.groupby('sku').agg({\n",
    "            'actual_demand': ['mean', 'std', 'sum'],\n",
    "            self.target_col: ['mean', 'std'],\n",
    "            'order_date': 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        sku_chars.columns = ['avg_demand', 'demand_volatility', 'total_demand', \n",
    "                            'avg_pct_change', 'pct_change_volatility', 'n_days']\n",
    "        \n",
    "        # Merge performance with characteristics\n",
    "        analysis_df = sku_performance_df.merge(sku_chars, left_index=True, right_index=True, how='left')\n",
    "        \n",
    "        # Correlation analysis\n",
    "        correlations = analysis_df[['mean_mae', 'avg_demand', 'demand_volatility', \n",
    "                                   'total_demand', 'pct_change_volatility', 'n_days']].corr()['mean_mae'].drop('mean_mae')\n",
    "        \n",
    "        print(\"Correlation with Predictability (negative = more predictable):\")\n",
    "        for factor, corr in correlations.sort_values().items():\n",
    "            direction = \"Higher\" if corr > 0 else \"Lower\"\n",
    "            strength = \"Strong\" if abs(corr) > 0.3 else \"Moderate\" if abs(corr) > 0.1 else \"Weak\"\n",
    "            print(f\"  {factor}: {corr:.3f} ({strength} - {direction} = Less Predictable)\")\n",
    "        \n",
    "        # Segment analysis\n",
    "        print(\"\\nPREDICTABILITY BY SEGMENTS:\")\n",
    "        print(\"-\" * 28)\n",
    "        \n",
    "        # High vs Low volume\n",
    "        high_volume = analysis_df[analysis_df['total_demand'] > analysis_df['total_demand'].quantile(0.8)]\n",
    "        low_volume = analysis_df[analysis_df['total_demand'] < analysis_df['total_demand'].quantile(0.2)]\n",
    "        \n",
    "        print(f\"High Volume SKUs (top 20%): {high_volume['mean_mae'].mean():.1f}% avg MAE\")\n",
    "        print(f\"Low Volume SKUs (bottom 20%): {low_volume['mean_mae'].mean():.1f}% avg MAE\")\n",
    "        \n",
    "        # High vs Low volatility\n",
    "        high_volatility = analysis_df[analysis_df['pct_change_volatility'] > analysis_df['pct_change_volatility'].quantile(0.8)]\n",
    "        low_volatility = analysis_df[analysis_df['pct_change_volatility'] < analysis_df['pct_change_volatility'].quantile(0.2)]\n",
    "        \n",
    "        print(f\"High Volatility SKUs: {high_volatility['mean_mae'].mean():.1f}% avg MAE\")\n",
    "        print(f\"Low Volatility SKUs: {low_volatility['mean_mae'].mean():.1f}% avg MAE\")\n",
    "    \n",
    "    def analyze_universal_prediction_degradation(self):\n",
    "        \"\"\"Analyze when prediction accuracy starts degrading across ALL models\"\"\"\n",
    "        print(\"\\nUNIVERSAL PREDICTION DEGRADATION ANALYSIS\")\n",
    "        print(\"=\"*45)\n",
    "        \n",
    "        # Sort by date and calculate prediction horizons\n",
    "        test_sorted = self.test_data.sort_values('order_date').copy()\n",
    "        training_end = test_sorted['order_date'].min()\n",
    "        test_sorted['days_from_training'] = (test_sorted['order_date'] - training_end).dt.days\n",
    "        \n",
    "        # Define time bins\n",
    "        bins = [0, 7, 14, 30, 60, 90, float('inf')]\n",
    "        labels = ['0-7 days', '8-14 days', '15-30 days', '31-60 days', '61-90 days', '90+ days']\n",
    "        test_sorted['time_bin'] = pd.cut(test_sorted['days_from_training'], bins=bins, labels=labels, right=False)\n",
    "        \n",
    "        # Calculate performance for each model across time bins\n",
    "        degradation_results = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            error_col = f'{model_name}_abs_error'\n",
    "            if error_col in test_sorted.columns:\n",
    "                model_degradation = test_sorted.groupby('time_bin')[error_col].agg(['mean', 'count']).reset_index()\n",
    "                model_degradation.columns = ['time_bin', 'mae', 'n_predictions']\n",
    "                degradation_results[model_name] = model_degradation\n",
    "        \n",
    "        # Calculate consensus degradation pattern\n",
    "        consensus_degradation = []\n",
    "        \n",
    "        for i, time_bin in enumerate(labels):\n",
    "            all_model_maes = []\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for model_name, degradation_df in degradation_results.items():\n",
    "                if i < len(degradation_df):\n",
    "                    mae_value = degradation_df.iloc[i]['mae']\n",
    "                    n_pred = degradation_df.iloc[i]['n_predictions']\n",
    "                    if not pd.isna(mae_value) and n_pred > 10:  # Minimum sample size\n",
    "                        all_model_maes.append(mae_value)\n",
    "                        total_predictions += n_pred\n",
    "            \n",
    "            if all_model_maes:\n",
    "                consensus_mae = np.mean(all_model_maes)\n",
    "                consensus_std = np.std(all_model_maes)\n",
    "                consensus_degradation.append({\n",
    "                    'time_bin': time_bin,\n",
    "                    'consensus_mae': consensus_mae,\n",
    "                    'mae_std_across_models': consensus_std,\n",
    "                    'n_models_agreeing': len(all_model_maes),\n",
    "                    'total_predictions': total_predictions\n",
    "                })\n",
    "        \n",
    "        consensus_df = pd.DataFrame(consensus_degradation)\n",
    "        \n",
    "        print(\"CONSENSUS DEGRADATION PATTERN:\")\n",
    "        print(\"-\" * 32)\n",
    "        \n",
    "        if len(consensus_df) > 0:\n",
    "            baseline_mae = consensus_df.iloc[0]['consensus_mae']\n",
    "            \n",
    "            for _, row in consensus_df.iterrows():\n",
    "                degradation_pct = ((row['consensus_mae'] - baseline_mae) / baseline_mae) * 100\n",
    "                model_agreement = row['n_models_agreeing']\n",
    "                \n",
    "                agreement_level = \"High\" if model_agreement >= len(self.model_names) * 0.8 else \"Medium\" if model_agreement >= len(self.model_names) * 0.6 else \"Low\"\n",
    "                \n",
    "                print(f\"{row['time_bin']}: {row['consensus_mae']:.1f}% MAE \"\n",
    "                      f\"({degradation_pct:+.1f}% vs baseline) \"\n",
    "                      f\"[{model_agreement}/{len(self.model_names)} models, {agreement_level} agreement]\")\n",
    "            \n",
    "            # Calculate degradation rate\n",
    "            if len(consensus_df) >= 3:\n",
    "                early_performance = consensus_df.iloc[0]['consensus_mae']\n",
    "                month_performance = consensus_df.iloc[2]['consensus_mae']  # 15-30 days\n",
    "                degradation_rate = ((month_performance - early_performance) / early_performance) * 100\n",
    "                \n",
    "                print(f\"\\nCONSENSUS DEGRADATION RATE: {degradation_rate:+.1f}% over first month\")\n",
    "                \n",
    "                if degradation_rate > 15:\n",
    "                    print(\"  Recommendation: WEEKLY retraining (high degradation)\")\n",
    "                elif degradation_rate > 8:\n",
    "                    print(\"  Recommendation: BI-WEEKLY retraining (moderate degradation)\")\n",
    "                elif degradation_rate > 3:\n",
    "                    print(\"  Recommendation: MONTHLY retraining (low degradation)\")\n",
    "                else:\n",
    "                    print(\"  Recommendation: QUARTERLY retraining (very stable)\")\n",
    "        \n",
    "        return consensus_df\n",
    "    \n",
    "    def analyze_seasonal_pattern_capture(self):\n",
    "        \"\"\"Analyze how well models capture seasonal patterns across the board\"\"\"\n",
    "        print(\"\\nSEASONAL PATTERN CAPTURE ANALYSIS\")\n",
    "        print(\"=\"*37)\n",
    "        \n",
    "        # Add time features\n",
    "        test_data = self.test_data.copy()\n",
    "        test_data['month'] = test_data['order_date'].dt.month\n",
    "        test_data['quarter'] = test_data['order_date'].dt.quarter\n",
    "        test_data['day_of_week'] = test_data['order_date'].dt.dayofweek\n",
    "        \n",
    "        # Analyze actual seasonal patterns in target variable\n",
    "        actual_seasonality = test_data.groupby('month')[self.target_col].agg(['mean', 'count']).reset_index()\n",
    "        actual_seasonality.columns = ['month', 'actual_avg_change', 'n_observations']\n",
    "        actual_seasonality = actual_seasonality[actual_seasonality['n_observations'] >= 10]\n",
    "        \n",
    "        print(\"ACTUAL SEASONAL PATTERNS:\")\n",
    "        print(\"-\" * 27)\n",
    "        month_names = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \n",
    "                      7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "        \n",
    "        for _, row in actual_seasonality.iterrows():\n",
    "            month_name = month_names.get(row['month'], str(row['month']))\n",
    "            print(f\"{month_name}: {row['actual_avg_change']:+.1f}% avg change ({row['n_observations']} obs)\")\n",
    "        \n",
    "        # Analyze how well each model predicts these seasonal patterns\n",
    "        seasonal_capture_results = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            pred_col = f'{model_name}_pred'\n",
    "            if pred_col in test_data.columns:\n",
    "                predicted_seasonality = test_data.groupby('month')[pred_col].mean().reset_index()\n",
    "                predicted_seasonality.columns = ['month', 'predicted_avg_change']\n",
    "                \n",
    "                # Merge with actual seasonality\n",
    "                seasonal_comparison = actual_seasonality.merge(predicted_seasonality, on='month', how='inner')\n",
    "                \n",
    "                if len(seasonal_comparison) >= 6:  # Need at least 6 months\n",
    "                    # Calculate correlation between actual and predicted seasonal patterns\n",
    "                    seasonal_correlation = seasonal_comparison['actual_avg_change'].corr(\n",
    "                        seasonal_comparison['predicted_avg_change']\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate MAE of seasonal pattern prediction\n",
    "                    seasonal_mae = np.mean(np.abs(\n",
    "                        seasonal_comparison['actual_avg_change'] - seasonal_comparison['predicted_avg_change']\n",
    "                    ))\n",
    "                    \n",
    "                    seasonal_capture_results[model_name] = {\n",
    "                        'seasonal_correlation': seasonal_correlation,\n",
    "                        'seasonal_mae': seasonal_mae,\n",
    "                        'comparison_df': seasonal_comparison\n",
    "                    }\n",
    "        \n",
    "        print(f\"\\nSEASONAL PATTERN CAPTURE BY MODEL:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Sort models by seasonal capture ability\n",
    "        seasonal_ranking = sorted(seasonal_capture_results.items(), \n",
    "                                key=lambda x: x[1]['seasonal_correlation'], reverse=True)\n",
    "        \n",
    "        for rank, (model_name, metrics) in enumerate(seasonal_ranking, 1):\n",
    "            corr = metrics['seasonal_correlation']\n",
    "            mae = metrics['seasonal_mae']\n",
    "            \n",
    "            capture_quality = \"Excellent\" if corr > 0.8 else \"Good\" if corr > 0.6 else \"Fair\" if corr > 0.4 else \"Poor\"\n",
    "            \n",
    "            print(f\"{rank}. {model_name}: {corr:.3f} correlation, {mae:.1f}% seasonal MAE ({capture_quality})\")\n",
    "        \n",
    "        # Overall seasonal capture consensus\n",
    "        if seasonal_ranking:\n",
    "            avg_seasonal_corr = np.mean([metrics['seasonal_correlation'] for _, metrics in seasonal_ranking])\n",
    "            print(f\"\\nCONSENSUS SEASONAL CAPTURE: {avg_seasonal_corr:.3f} correlation\")\n",
    "            \n",
    "            if avg_seasonal_corr > 0.7:\n",
    "                print(\"  Assessment: STRONG seasonal pattern capture\")\n",
    "            elif avg_seasonal_corr > 0.5:\n",
    "                print(\"  Assessment: MODERATE seasonal pattern capture\")\n",
    "            elif avg_seasonal_corr > 0.3:\n",
    "                print(\"  Assessment: WEAK seasonal pattern capture\")\n",
    "            else:\n",
    "                print(\"  Assessment: POOR seasonal pattern capture - review seasonal features\")\n",
    "        \n",
    "        # Analyze Q4 vs non-Q4 performance specifically\n",
    "        self._analyze_q4_performance()\n",
    "        \n",
    "        return seasonal_capture_results\n",
    "    \n",
    "    def _analyze_q4_performance(self):\n",
    "        \"\"\"Analyze Q4 (holiday season) performance specifically\"\"\"\n",
    "        print(f\"\\nQ4 (HOLIDAY SEASON) PERFORMANCE:\")\n",
    "        print(\"-\" * 33)\n",
    "        \n",
    "        test_data = self.test_data.copy()\n",
    "        test_data['is_q4'] = test_data['order_date'].dt.quarter == 4\n",
    "        \n",
    "        q4_performance = {}\n",
    "        non_q4_performance = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            error_col = f'{model_name}_abs_error'\n",
    "            if error_col in test_data.columns:\n",
    "                q4_mae = test_data[test_data['is_q4']][error_col].mean()\n",
    "                non_q4_mae = test_data[~test_data['is_q4']][error_col].mean()\n",
    "                \n",
    "                if not pd.isna(q4_mae) and not pd.isna(non_q4_mae):\n",
    "                    q4_performance[model_name] = q4_mae\n",
    "                    non_q4_performance[model_name] = non_q4_mae\n",
    "        \n",
    "        if q4_performance and non_q4_performance:\n",
    "            print(\"Q4 vs Non-Q4 Performance:\")\n",
    "            \n",
    "            q4_models = []\n",
    "            for model_name in q4_performance.keys():\n",
    "                q4_mae = q4_performance[model_name]\n",
    "                non_q4_mae = non_q4_performance[model_name]\n",
    "                q4_difference = ((q4_mae - non_q4_mae) / non_q4_mae) * 100\n",
    "                \n",
    "                q4_models.append((model_name, q4_mae, non_q4_mae, q4_difference))\n",
    "                \n",
    "            # Sort by Q4 performance\n",
    "            q4_models.sort(key=lambda x: x[1])  # Sort by Q4 MAE\n",
    "            \n",
    "            for model_name, q4_mae, non_q4_mae, q4_diff in q4_models:\n",
    "                performance_change = \"worse\" if q4_diff > 0 else \"better\"\n",
    "                print(f\"  {model_name}: Q4 {q4_mae:.1f}%, Non-Q4 {non_q4_mae:.1f}% \"\n",
    "                      f\"({q4_diff:+.1f}% {performance_change} in Q4)\")\n",
    "            \n",
    "            # Overall Q4 assessment\n",
    "            avg_q4_impact = np.mean([diff for _, _, _, diff in q4_models])\n",
    "            \n",
    "            print(f\"\\nAVERAGE Q4 IMPACT: {avg_q4_impact:+.1f}% change in accuracy\")\n",
    "            \n",
    "            if avg_q4_impact > 10:\n",
    "                print(\"  Q4 Assessment: SIGNIFICANT accuracy drop during holidays\")\n",
    "            elif avg_q4_impact > 5:\n",
    "                print(\"  Q4 Assessment: MODERATE accuracy drop during holidays\")\n",
    "            elif avg_q4_impact > -5:\n",
    "                print(\"  Q4 Assessment: STABLE performance during holidays\")\n",
    "            else:\n",
    "                print(\"  Q4 Assessment: IMPROVED performance during holidays\")\n",
    "    \n",
    "    def generate_comprehensive_insights(self):\n",
    "        \"\"\"Generate overall insights across all analyses\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPREHENSIVE CROSS-MODEL INSIGHTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Run all analyses\n",
    "        sku_performance = self.analyze_universal_sku_predictability()\n",
    "        degradation_analysis = self.analyze_universal_prediction_degradation()\n",
    "        seasonal_analysis = self.analyze_seasonal_pattern_capture()\n",
    "        \n",
    "        print(\"\\nKEY INSIGHTS SUMMARY:\")\n",
    "        print(\"-\" * 22)\n",
    "        \n",
    "        # SKU Insights\n",
    "        best_skus = sku_performance.head(5)\n",
    "        worst_skus = sku_performance.tail(5)\n",
    "        \n",
    "        print(f\"1. SKU PREDICTABILITY:\")\n",
    "        print(f\"   - Best performing SKUs average: {best_skus['mean_mae'].mean():.1f}% MAE\")\n",
    "        print(f\"   - Worst performing SKUs average: {worst_skus['mean_mae'].mean():.1f}% MAE\")\n",
    "        print(f\"   - Predictability range: {worst_skus['mean_mae'].mean() - best_skus['mean_mae'].mean():.1f}% points\")\n",
    "        \n",
    "        # Degradation Insights\n",
    "        if len(degradation_analysis) >= 3:\n",
    "            early_performance = degradation_analysis.iloc[0]['consensus_mae']\n",
    "            month_performance = degradation_analysis.iloc[2]['consensus_mae']\n",
    "            degradation = ((month_performance - early_performance) / early_performance) * 100\n",
    "            \n",
    "            print(f\"\\n2. PREDICTION DEGRADATION:\")\n",
    "            print(f\"   - First week performance: {early_performance:.1f}% MAE\")\n",
    "            print(f\"   - One month performance: {month_performance:.1f}% MAE\")\n",
    "            print(f\"   - Degradation rate: {degradation:+.1f}% over 30 days\")\n",
    "        \n",
    "        # Seasonal Insights\n",
    "        if seasonal_analysis:\n",
    "            seasonal_correlations = [metrics['seasonal_correlation'] for metrics in seasonal_analysis.values()]\n",
    "            avg_seasonal_capture = np.mean(seasonal_correlations)\n",
    "            \n",
    "            print(f\"\\n3. SEASONAL PATTERN CAPTURE:\")\n",
    "            print(f\"   - Average seasonal correlation: {avg_seasonal_capture:.3f}\")\n",
    "            print(f\"   - Best seasonal model: {max(seasonal_analysis.items(), key=lambda x: x[1]['seasonal_correlation'])[0]}\")\n",
    "        \n",
    "        # Model Consensus Insights\n",
    "        print(f\"\\n4. MODEL CONSENSUS:\")\n",
    "        all_model_maes = [results['mae'] for results in self.model_results.values()]\n",
    "        mae_range = max(all_model_maes) - min(all_model_maes)\n",
    "        \n",
    "        print(f\"   - MAE range across models: {mae_range:.1f}% points\")\n",
    "        \n",
    "        if mae_range < 2:\n",
    "            consensus_level = \"HIGH - Models largely agree\"\n",
    "        elif mae_range < 5:\n",
    "            consensus_level = \"MODERATE - Some model disagreement\"\n",
    "        else:\n",
    "            consensus_level = \"LOW - Significant model disagreement\"\n",
    "        \n",
    "        print(f\"   - Model consensus level: {consensus_level}\")\n",
    "        \n",
    "        return {\n",
    "            'sku_performance': sku_performance,\n",
    "            'degradation_analysis': degradation_analysis,\n",
    "            'seasonal_analysis': seasonal_analysis,\n",
    "            'summary_insights': {\n",
    "                'best_sku_mae': best_skus['mean_mae'].mean(),\n",
    "                'worst_sku_mae': worst_skus['mean_mae'].mean(),\n",
    "                'degradation_rate': degradation if 'degradation' in locals() else None,\n",
    "                'seasonal_capture': avg_seasonal_capture if 'avg_seasonal_capture' in locals() else None,\n",
    "                'model_consensus': mae_range\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def run_comprehensive_analysis(test_data, model_results, target_col='demand_pct_change'):\n",
    "    \"\"\"\n",
    "    Run comprehensive analysis across all models to identify universal patterns\n",
    "    \n",
    "    This will tell you:\n",
    "    - Which SKUs are predictable across ALL models\n",
    "    - When prediction accuracy universally starts degrading\n",
    "    - Whether seasonal patterns are being captured effectively\n",
    "    - Model consensus and disagreement patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = ComprehensiveModelAnalysis(test_data, model_results, target_col)\n",
    "    insights = analyzer.generate_comprehensive_insights()\n",
    "    \n",
    "    return analyzer, insights\n",
    "\n",
    "\n",
    "# Usage with your existing results:\n",
    "\n",
    "# After running your enhanced demand analysis:\n",
    "enhanced_model, model_results, test_data, analyses = run_enhanced_demand_analysis(\n",
    "    enhanced_benchmark_df,  # Your benchmark dataframe\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items,\n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")\n",
    "# Run comprehensive cross-model analysis:\n",
    "comprehensive_analyzer, insights = run_comprehensive_analysis(test_data, model_results)\n",
    "\n",
    "# Access specific insights:\n",
    "best_skus = insights['sku_performance'].head(10)  # Universally predictable SKUs\n",
    "degradation_pattern = insights['degradation_analysis']  # When all models degrade\n",
    "seasonal_capture = insights['seasonal_analysis']  # How well seasonality is captured\n",
    "summary = insights['summary_insights']  # Key takeaways\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45b349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
