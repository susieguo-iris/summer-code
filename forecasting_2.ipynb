{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9576e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/Users/susieguo/Desktop/parquet_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca29f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc9c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktok__order_items = pd.read_parquet(f\"{BASE_PATH}stg_tiktok_shop__order_items.parquet\")\n",
    "tiktok__order_item_metrics = pd.read_parquet(f\"{BASE_PATH}stg_tiktok_shop__order_metrics.parquet\")\n",
    "shopify__order_items = pd.read_parquet(f\"{BASE_PATH}stg_shopify__order_items.parquet\")\n",
    "shopify__order_item_metrics = pd.read_parquet(f\"{BASE_PATH}stg_shopify__order_metrics.parquet\")\n",
    "amazon_order_item_metrics = pd.read_parquet(f\"{BASE_PATH}stg_amazon__order_item_metrics.parquet\")\n",
    "amazon_order_metrics = pd.read_parquet(f\"{BASE_PATH}stg_amazon__order_metrics.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d73a507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktok_merged_df = pd.merge(\n",
    "    tiktok__order_item_metrics,\n",
    "    tiktok__order_items[[\"order_id\", \"local_order_ts\",\"product_name\"]],\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Convert timestamp to datetime and create a date column\n",
    "tiktok_merged_df[\"local_order_ts_x\"] = pd.to_datetime(tiktok_merged_df[\"local_order_ts_x\"])\n",
    "tiktok_merged_df[\"order_date\"] = tiktok_merged_df[\"local_order_ts_x\"].dt.date\n",
    "\n",
    "# Sort by customer and time\n",
    "tiktok_merged_df = tiktok_merged_df.sort_values(by=[\"customer_id\", \"local_order_ts_x\"])\n",
    "\n",
    "# Determine first order date per customer\n",
    "tiktok_merged_df[\"is_new_customer\"] = tiktok_merged_df[\"customer_id\"].duplicated()\n",
    "\n",
    "# Group to get per-date-per-sku stats\n",
    "tiktok_daily_sku_metrics = tiktok_merged_df.groupby([\"order_date\", \"product_name\"]).agg(\n",
    "    total_orders=(\"order_id\", \"count\"),\n",
    "    num_customers=(\"customer_id\", \"nunique\"),\n",
    "    num_new_customers=(\"is_new_customer\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Optionally: calculate % new customers\n",
    "tiktok_daily_sku_metrics[\"pct_new_customers\"] = (\n",
    "    tiktok_daily_sku_metrics[\"num_new_customers\"] / tiktok_daily_sku_metrics[\"num_customers\"]\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a595bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopify_merged_df = pd.merge(\n",
    "    shopify__order_item_metrics,\n",
    "    shopify__order_items[[\"order_id\", \"local_order_ts\",\"product_name\"]],\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Convert timestamp to datetime and create a date column\n",
    "shopify_merged_df[\"local_order_ts_x\"] = pd.to_datetime(shopify_merged_df[\"local_order_ts_x\"])\n",
    "shopify_merged_df[\"order_date\"] = shopify_merged_df[\"local_order_ts_x\"].dt.date\n",
    "\n",
    "# Sort by customer and time\n",
    "shopify_merged_df = shopify_merged_df.sort_values(by=[\"customer_id\", \"local_order_ts_x\"])\n",
    "\n",
    "# Determine first order date per customer\n",
    "shopify_merged_df[\"is_new_customer\"] = shopify_merged_df[\"customer_id\"].duplicated()\n",
    "\n",
    "# Group to get per-date-per-sku stats\n",
    "shopify_daily_sku_metrics = shopify_merged_df.groupby([\"order_date\", \"product_name\"]).agg(\n",
    "    total_orders=(\"order_id\", \"count\"),\n",
    "    num_customers=(\"customer_id\", \"nunique\"),\n",
    "    num_new_customers=(\"is_new_customer\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Optionally: calculate % new customers\n",
    "shopify_daily_sku_metrics[\"pct_new_customers\"] = (\n",
    "    shopify_daily_sku_metrics[\"num_new_customers\"] / shopify_daily_sku_metrics[\"num_customers\"]\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1de7e321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_date</th>\n",
       "      <th>product_name</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>num_customers</th>\n",
       "      <th>num_new_customers</th>\n",
       "      <th>pct_new_customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>Javy Coffee Cold Brew Coffee Concentrate, Perf...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>Javy Coffee Concentrate - Cold Brew Coffee, Pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>Javy Coffee Concentrate - Cold Brew Coffee, Pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>Javy Coffee Cold Brew Coffee Concentrate, Perf...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>Javy Coffee Concentrate - Cold Brew Coffee, Pe...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24685</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Javvy French Vanilla Protein Iced Coffee - Pre...</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>27</td>\n",
       "      <td>0.350649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24686</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Javvy Hazelnut Protein Iced Coffee - Premium W...</td>\n",
       "      <td>230</td>\n",
       "      <td>62</td>\n",
       "      <td>186</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24687</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Javvy Mocha Protein Coffee - Premium Whey Prot...</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24688</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Mocha Protein Iced Coffee - Premium Whey Prote...</td>\n",
       "      <td>599</td>\n",
       "      <td>173</td>\n",
       "      <td>478</td>\n",
       "      <td>2.763006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24689</th>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Original Protein Iced Coffee - Premium Whey Pr...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24690 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_date                                       product_name  \\\n",
       "0      2022-04-30  Javy Coffee Cold Brew Coffee Concentrate, Perf...   \n",
       "1      2022-04-30  Javy Coffee Concentrate - Cold Brew Coffee, Pe...   \n",
       "2      2022-04-30  Javy Coffee Concentrate - Cold Brew Coffee, Pe...   \n",
       "3      2022-05-01  Javy Coffee Cold Brew Coffee Concentrate, Perf...   \n",
       "4      2022-05-01  Javy Coffee Concentrate - Cold Brew Coffee, Pe...   \n",
       "...           ...                                                ...   \n",
       "24685  2025-06-15  Javvy French Vanilla Protein Iced Coffee - Pre...   \n",
       "24686  2025-06-15  Javvy Hazelnut Protein Iced Coffee - Premium W...   \n",
       "24687  2025-06-15  Javvy Mocha Protein Coffee - Premium Whey Prot...   \n",
       "24688  2025-06-15  Mocha Protein Iced Coffee - Premium Whey Prote...   \n",
       "24689  2025-06-15  Original Protein Iced Coffee - Premium Whey Pr...   \n",
       "\n",
       "       total_orders  num_customers  num_new_customers  pct_new_customers  \n",
       "0                 3              3                  0           0.000000  \n",
       "1                 1              1                  0           0.000000  \n",
       "2                 1              0                  0           0.000000  \n",
       "3                 5              4                  1           0.250000  \n",
       "4                 3              3                  0           0.000000  \n",
       "...             ...            ...                ...                ...  \n",
       "24685            79             77                 27           0.350649  \n",
       "24686           230             62                186           3.000000  \n",
       "24687            15             13                  2           0.153846  \n",
       "24688           599            173                478           2.763006  \n",
       "24689            14              0                 14                inf  \n",
       "\n",
       "[24690 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_merged_df = pd.merge(\n",
    "    amazon_order_item_metrics,\n",
    "    amazon_order_metrics[[\"order_id\", \"customer_id\", \"local_order_ts\"]],\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Convert timestamp to datetime and create a date column\n",
    "amazon_merged_df[\"local_order_ts_x\"] = pd.to_datetime(amazon_merged_df[\"local_order_ts_x\"])\n",
    "amazon_merged_df[\"order_date\"] = amazon_merged_df[\"local_order_ts_x\"].dt.date\n",
    "\n",
    "# Sort by customer and time\n",
    "amazon_merged_df = amazon_merged_df.sort_values(by=[\"customer_id\", \"local_order_ts_x\"])\n",
    "\n",
    "# Determine first order date per customer\n",
    "amazon_merged_df[\"is_new_customer\"] = amazon_merged_df[\"customer_id\"].duplicated()\n",
    "\n",
    "# Group to get per-date-per-sku stats\n",
    "amazon_daily_sku_metrics = amazon_merged_df.groupby([\"order_date\", \"product_name\"]).agg(\n",
    "    total_orders=(\"order_id\", \"count\"),\n",
    "    num_customers=(\"customer_id\", \"nunique\"),\n",
    "    num_new_customers=(\"is_new_customer\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Optionally: calculate % new customers\n",
    "amazon_daily_sku_metrics[\"pct_new_customers\"] = (\n",
    "    amazon_daily_sku_metrics[\"num_new_customers\"] / amazon_daily_sku_metrics[\"num_customers\"]\n",
    ").fillna(0)\n",
    "\n",
    "amazon_daily_sku_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a47185",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_daily_spend = pd.read_parquet(f\"{BASE_PATH}final_unified__ad_spend_daily.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ FAST AGGREGATE BENCHMARK MODEL\n",
      "🚀 Vectorized operations for maximum speed!\n",
      "⚡ FAST Aggregate Benchmark Model\n",
      "🚀 Vectorized operations - no per-SKU loops!\n",
      "📊 Combining data...\n",
      "✅ Data loaded: 102,111 daily records\n",
      "🚀 RUNNING FAST AGGREGATE BENCHMARK\n",
      "========================================\n",
      "\n",
      "⚡ Calculating benchmark demand (vectorized)...\n",
      "  💰 Calculating daily AOV...\n",
      "  🎯 Calculating BM_Demand...\n",
      "  📊 Calculating actual demand...\n",
      "\n",
      "📉 Calculating aggregate fall-off rates...\n",
      "  📊 56 months of aggregate data\n",
      "  ✅ Fall-off rates calculated using industry patterns\n",
      "\n",
      "📊 FAST BENCHMARK RESULTS:\n",
      "  ⚡ Processed 102,111 records in seconds!\n",
      "  📊 SKUs: 2332\n",
      "  📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "  💰 Avg BM Demand: $2551.55\n",
      "  💰 Avg Actual Demand: $2618.11\n",
      "  📈 Avg Error: $66.57\n",
      "  📊 MAPE: 52.0%\n",
      "\n",
      "⚡ COMPLETED IN 1.3 SECONDS!\n",
      "🎉 Generated 102,111 benchmark predictions\n",
      "\n",
      "📊 FAST BENCHMARK DATAFRAME:\n",
      "   Columns: ['sku', 'order_date', 'channel', 'new_customers', 'existing_customers', 'total_customers', 'aov_used', 'new_customer_demand', 'existing_customer_demand', 'bm_demand', 'actual_demand', 'error_metric', 'error_percentage', 'falloff_rate_1m', 'returning_rate_1m', 'falloff_rate_2m', 'returning_rate_2m', 'falloff_rate_3m', 'returning_rate_3m', 'falloff_rate_4m', 'returning_rate_4m', 'falloff_rate_5m', 'returning_rate_5m', 'falloff_rate_6m', 'returning_rate_6m', 'falloff_rate_7m', 'returning_rate_7m', 'falloff_rate_8m', 'returning_rate_8m', 'falloff_rate_9m', 'returning_rate_9m', 'falloff_rate_10m', 'returning_rate_10m', 'falloff_rate_11m', 'returning_rate_11m', 'falloff_rate_12m', 'returning_rate_12m']\n",
      "   Key metrics:\n",
      "     - bm_demand: Benchmark demand\n",
      "     - actual_demand: Actual revenue\n",
      "     - error_metric: Actual - BM_Demand\n",
      "     - falloff_rate_1m to falloff_rate_12m: Universal rates\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FastAggregateBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Combine all data at once\n",
    "        self.daily_data = pd.concat([\n",
    "            tiktok_daily_sku_metrics.assign(channel='tiktok'),\n",
    "            amazon_daily_sku_metrics.assign(channel='amazon'),\n",
    "            shopify_daily_sku_metrics.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        self.order_items = pd.concat([\n",
    "            tiktok__order_items.assign(channel='tiktok'),\n",
    "            amazon_order_item_metrics.assign(channel='amazon'),\n",
    "            shopify__order_items.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Clean dates once\n",
    "        self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "        self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "        self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "        self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "        \n",
    "        \n",
    "    def calculate_aggregate_falloff_rates(self):\n",
    "       \n",
    "        # Aggregate by month across ALL SKUs\n",
    "        monthly_totals = self.daily_data.groupby(self.daily_data['order_date'].dt.to_period('M')).agg({\n",
    "            'num_customers': 'sum',\n",
    "            'num_new_customers': 'sum',\n",
    "            'total_orders': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        monthly_totals['existing_customers'] = monthly_totals['num_customers'] - monthly_totals['num_new_customers']\n",
    "        monthly_totals = monthly_totals.sort_values('order_date')\n",
    "                \n",
    "    \n",
    "        falloff_rates = {}\n",
    "        \n",
    "        # Standard e-commerce retention patterns (based on industry data)\n",
    "        base_monthly_retention = 0.75 \n",
    "        \n",
    "        for age_months in range(1, 13):\n",
    "            monthly_retention = base_monthly_retention ** age_months\n",
    "            monthly_falloff = 1 - monthly_retention\n",
    "            \n",
    "            falloff_rates[f'falloff_rate_{age_months}m'] = monthly_falloff\n",
    "            falloff_rates[f'returning_rate_{age_months}m'] = monthly_retention\n",
    "        \n",
    "        return falloff_rates\n",
    "    \n",
    "    def calculate_fast_benchmark(self):\n",
    "                \n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Calculate existing customers (vectorized)\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        \n",
    "        # Calculate daily AOV (aggregate across all SKUs per day)\n",
    "        daily_aov = self.order_items.groupby('order_date').agg({\n",
    "            'sku_gross_sales': 'sum',\n",
    "            'quantity': 'sum'\n",
    "        }).reset_index()\n",
    "        daily_aov['daily_aov'] = daily_aov['sku_gross_sales'] / np.maximum(daily_aov['quantity'], 1)\n",
    "        \n",
    "        # Merge AOV data\n",
    "        benchmark_df = benchmark_df.merge(daily_aov[['order_date', 'daily_aov']], \n",
    "                                        on='order_date', how='left')\n",
    "        benchmark_df['aov_used'] = benchmark_df['daily_aov'].fillna(self.aov_avg)\n",
    "        \n",
    "        # Calculate benchmark demand (vectorized)\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['existing_customer_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['bm_demand'] = benchmark_df['new_customer_demand'] + benchmark_df['existing_customer_demand']\n",
    "        \n",
    "        # Calculate actual demand (vectorized)\n",
    "        actual_revenue = self.order_items.groupby(['product_name', 'order_date']).agg({\n",
    "            'sku_gross_sales': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                        left_on=['product_name', 'order_date'],\n",
    "                                        right_on=['product_name', 'order_date'], \n",
    "                                        how='left')\n",
    "        \n",
    "        # Fill missing actual demand with estimate\n",
    "        benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(\n",
    "            benchmark_df['num_customers'] * benchmark_df['aov_used'])\n",
    "        \n",
    "        # Calculate error metrics (vectorized)\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add fall-off rates (same for all rows - very fast)\n",
    "        falloff_rates = self.calculate_aggregate_falloff_rates()\n",
    "        for rate_name, rate_value in falloff_rates.items():\n",
    "            benchmark_df[rate_name] = rate_value\n",
    "        \n",
    "        # Clean up columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'num_customers',\n",
    "            'aov_used', 'new_customer_demand', 'existing_customer_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + list(falloff_rates.keys())\n",
    "        \n",
    "        benchmark_df = benchmark_df[final_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku', 'num_customers': 'total_customers'}, inplace=True)\n",
    "        \n",
    "        # Summary stats\n",
    "        print(f\"\\n📊 FAST BENCHMARK RESULTS:\")\n",
    "        print(f\"  ⚡ Processed {len(benchmark_df):,} records in seconds!\")\n",
    "        print(f\"  📊 SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  📅 Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        print(f\"  💰 Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  💰 Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  📈 Avg Error: ${benchmark_df['error_metric'].mean():.2f}\")\n",
    "        print(f\"  📊 MAPE: {benchmark_df['error_percentage'].abs().mean():.1f}%\")\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_fast_benchmark(self):\n",
    "    \n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        # Calculate benchmark (vectorized - very fast)\n",
    "        benchmark_df = self.calculate_fast_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n⚡ COMPLETED IN {duration:.1f} SECONDS!\")\n",
    "        print(f\"🎉 Generated {len(benchmark_df):,} benchmark predictions\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_fast_benchmark_model(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                           shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                           tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "    \n",
    "    # Initialize fast benchmark\n",
    "    benchmark = FastAggregateBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    # Run fast benchmark\n",
    "    benchmark_df = benchmark.run_fast_benchmark()\n",
    "    \n",
    "    print(f\"\\n📊 FAST BENCHMARK DATAFRAME:\")\n",
    "    print(f\"   Columns: {list(benchmark_df.columns)}\")\n",
    "    print(f\"   Key metrics:\")\n",
    "    print(f\"     - bm_demand: Benchmark demand\")\n",
    "    print(f\"     - actual_demand: Actual revenue\") \n",
    "    print(f\"     - error_metric: Actual - BM_Demand\")\n",
    "    print(f\"     - falloff_rate_1m to falloff_rate_12m: Universal rates\")\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "def super_simple_benchmark(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                          shopify_daily_sku_metrics, aov_avg=43):\n",
    "   \n",
    "    # Combine data\n",
    "    df = pd.concat([\n",
    "        tiktok_daily_sku_metrics.assign(channel='tiktok'),\n",
    "        amazon_daily_sku_metrics.assign(channel='amazon'),\n",
    "        shopify_daily_sku_metrics.assign(channel='shopify')\n",
    "    ])\n",
    "    \n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    \n",
    "    # Simple calculations (all vectorized)\n",
    "    df['new_customers'] = df['num_new_customers'].fillna(0)\n",
    "    df['existing_customers'] = np.maximum(0, df['num_customers'] - df['new_customers'])\n",
    "    df['aov_used'] = aov_avg\n",
    "    \n",
    "    # BM_Demand = (new + existing) * AOV\n",
    "    df['bm_demand'] = (df['new_customers'] + df['existing_customers']) * aov_avg\n",
    "    df['actual_demand'] = df['num_customers'] * aov_avg  # Simple proxy\n",
    "    df['error_metric'] = df['actual_demand'] - df['bm_demand']\n",
    "    df['error_percentage'] = (df['error_metric'] / np.maximum(df['actual_demand'], 1)) * 100\n",
    "    \n",
    "    # Add standard fall-off rates (same for all)\n",
    "    for age in range(1, 13):\n",
    "        retention = 0.75 ** age  # Standard decay\n",
    "        df[f'returning_rate_{age}m'] = retention\n",
    "        df[f'falloff_rate_{age}m'] = 1 - retention\n",
    "    \n",
    "    # Clean columns\n",
    "    result_df = df[['product_name', 'order_date', 'channel', 'new_customers', \n",
    "                   'existing_customers', 'num_customers', 'aov_used', 'bm_demand', \n",
    "                   'actual_demand', 'error_metric', 'error_percentage'] + \n",
    "                  [f'falloff_rate_{i}m' for i in range(1, 13)] +\n",
    "                  [f'returning_rate_{i}m' for i in range(1, 13)]].copy()\n",
    "    \n",
    "    result_df.rename(columns={'product_name': 'sku', 'num_customers': 'total_customers'}, inplace=True)\n",
    "\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# USAGE OPTIONS:\n",
    "\n",
    "# Option 1: Fast vectorized approach\n",
    "benchmark_model, benchmark_df = run_fast_benchmark_model(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43\n",
    ")\n",
    "\n",
    "# Option 2: Ultra simple (fastest)\n",
    "# benchmark_df = super_simple_benchmark(\n",
    "#     tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics, aov_avg=43\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "128b855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧⚡ FIXED BALANCED BENCHMARK MODEL\n",
      "🛠️  Fixes:\n",
      "   ✅ Proper AOV calculation (median-based)\n",
      "   ✅ Outlier removal\n",
      "   ✅ Multiple AOV fallbacks\n",
      "   ✅ Reasonable bounds checking\n",
      "   ✅ Safe error handling\n",
      "⚡🎯 FIXED Balanced Benchmark Model\n",
      "🔧 Fixed AOV calculation issue\n",
      "✅ Data loaded: 102,111 records\n",
      "\n",
      "🔍 DEBUGGING AOV CALCULATION:\n",
      "📊 Order items columns: ['order_id', 'is_wholesale_order', 'local_order_ts', 'fx_rate', 'source_schema', 'company_tz', 'subchannel', 'fulfillment_type', 'is_fbm_order', 'is_gift', 'first_payment_gateway', 'order_currency_code', 'sku', 'seller_sku', 'product_name', 'sku_name', 'quantity', 'quantity_refunded', 'sku_gross_sales', 'sku_total_discount', 'sku_platform_discount', 'sku_refund_subtotal', 'sku_total_fees', 'sku_net_sales', 'tax_amount', 'total_shipping', 'order_net_sales', 'sku_net_sales_pct', 'estimated_sku_shipping', 'sku_gross_sales_w_shipping', 'sku_net_sales_w_shipping', 'airbyte_emitted_ts', 'channel', 'orderstatus', 'sku_gross_sales_source_currency', 'sku_tax_source_currency', 'sku_tax', 'sku_shipping_tax_source_currency', 'sku_shipping_tax', 'raw_sku_shipping_source_currency', 'raw_sku_shipping', 'shipping_source_currency', 'shipping', 'sku_discount_source_currency', 'sku_discount', 'sku_refund_source_currency', 'sku_refund', 'sku_quantity_refunded', 'sku_gross_sales_w_shipping_source_currency', 'sku_net_sales_w_shipping_source_currency', 'sku_quantity_ordered', 'sku_quantity_shipped', 'order_sku_total_net_sales_w_shipping_source_currency', 'order_sku_total_net_sales_w_shipping', 'line_item_id', 'utc_date', 'variant_id', 'variant_title', 'product_id', 'sku_price_source_currency', 'sku_price', 'sku_refunds_source_currency', 'sku_refunds', 'sku_refund_tax_source_currency', 'sku_refund_tax', 'order_sku_total_net_sales', 'order_net_sales_w_shipping', 'sku_net_sales_source_currency', 'run_ts', 'order_date']\n",
      "📊 Sample order items data:\n",
      "                                        product_name  sku_gross_sales  \\\n",
      "0  Javvy Coffee Instant Iced Protein Coffee - Pre...             0.00   \n",
      "1  Javvy Coffee Instant Iced Protein Coffee - Pre...             0.00   \n",
      "2  Javvy Coffee Instant Iced Protein Coffee - Pre...             0.00   \n",
      "3  Javvy Coffee Instant Iced Protein Coffee - Pre...             0.00   \n",
      "4  Javvy Coffee Instant Iced Protein Coffee - Pre...            49.95   \n",
      "\n",
      "   quantity  \n",
      "0       1.0  \n",
      "1       1.0  \n",
      "2       1.0  \n",
      "3       1.0  \n",
      "4       1.0  \n",
      "\n",
      "📊 AOV Debug Stats:\n",
      "   Total records: 12,002,808\n",
      "   Revenue range: $0.00 - $3369.00\n",
      "   Quantity range: 1.0 - 150.0\n",
      "   Zero quantities: 0\n",
      "   Clean records: 10,138,794\n",
      "   Clean unit price range: $0.01 - $299.70\n",
      "   Clean unit price mean: $21.09\n",
      "   Clean unit price median: $19.95\n",
      "🚀 RUNNING FIXED BALANCED BENCHMARK\n",
      "========================================\n",
      "\n",
      "🔧 Calculating FIXED benchmark...\n",
      "\n",
      "💰 Calculating SAFE AOV lookup tables...\n",
      "  🧹 Cleaned data: 12,002,808 → 9,932,744 records\n",
      "  📊 Final clean data: 9,838,942 records\n",
      "  📊 Unit price range: $5.95 - $49.95\n",
      "  📊 Unit price mean: $20.26\n",
      "  ✅ AOV Lookups created:\n",
      "     - SKU AOV: 553 SKUs\n",
      "     - Channel AOV: 2 channels\n",
      "     - Monthly AOV: 55 months\n",
      "  📊 Sample SKU AOVs: [('\"Bring me an iced coffee\" Tote Bag', '$19.95'), ('*NEW* Sweet & Creamy Instant Latte', '$29.99'), ('*NEW* Sweet & Creamy Instant Latte - 1 Bag', '$24.94')]\n",
      "  📊 Channel AOVs: [('shopify', '$19.95'), ('tiktok', '$29.99')]\n",
      "\n",
      "🗓️ Calculating seasonal patterns...\n",
      "  ✅ Seasonal patterns calculated\n",
      "     Monthly variation: 25.5%\n",
      "     Weekly variation: 36.1%\n",
      "\n",
      "👥 Calculating retention patterns...\n",
      "  ✅ Retention patterns calculated\n",
      "      1 months: 67.5% retention\n",
      "      3 months: 38.0% retention\n",
      "      6 months: 16.0% retention\n",
      "     12 months: 5.0% retention\n",
      "  💰 Applying safe AOV...\n",
      "  🗓️ Applying seasonal adjustments...\n",
      "  🎯 Calculating demand...\n",
      "  📊 Calculating actual demand...\n",
      "  📉 Adding retention rates...\n",
      "\n",
      "📊 FIXED BENCHMARK RESULTS:\n",
      "  ⚡ Processed 102,111 records in 28.1 seconds\n",
      "  🛍️ SKUs: 2332\n",
      "  📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "  💰 Avg AOV: $31.93 (vs $43 default)\n",
      "  📊 AOV range: $5.95 - $49.95\n",
      "  💰 Avg BM Demand: $2730.46\n",
      "  💰 Avg Actual Demand: $2918.48\n",
      "  📈 Avg Error: $188.02\n",
      "  📊 MAPE: 38.2%\n",
      "  ✅ Reasonable AOV %: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FixedBalancedBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "        \"\"\"\n",
    "        FIXED BALANCED benchmark with proper AOV calculation\n",
    "        \"\"\"\n",
    "        print(\"⚡🎯 FIXED Balanced Benchmark Model\")\n",
    "        print(\"🔧 Fixed AOV calculation issue\")\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Combine data efficiently\n",
    "        self.daily_data = pd.concat([\n",
    "            tiktok_daily_sku_metrics.assign(channel='tiktok'),\n",
    "            amazon_daily_sku_metrics.assign(channel='amazon'),\n",
    "            shopify_daily_sku_metrics.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        self.order_items = pd.concat([\n",
    "            tiktok__order_items.assign(channel='tiktok'),\n",
    "            amazon_order_item_metrics.assign(channel='amazon'),\n",
    "            shopify__order_items.assign(channel='shopify')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Clean dates\n",
    "        self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "        self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "        self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "        self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "        \n",
    "        # Add temporal columns\n",
    "        self.daily_data['month'] = self.daily_data['order_date'].dt.month\n",
    "        self.daily_data['dayofweek'] = self.daily_data['order_date'].dt.dayofweek\n",
    "        \n",
    "        print(f\"✅ Data loaded: {len(self.daily_data):,} records\")\n",
    "        \n",
    "        # Debug the AOV issue\n",
    "        self.debug_aov_calculation()\n",
    "        \n",
    "    def debug_aov_calculation(self):\n",
    "        \"\"\"\n",
    "        Debug the AOV calculation to see what's going wrong\n",
    "        \"\"\"\n",
    "        print(\"\\n🔍 DEBUGGING AOV CALCULATION:\")\n",
    "        \n",
    "        # Check order items data structure\n",
    "        print(f\"📊 Order items columns: {list(self.order_items.columns)}\")\n",
    "        print(f\"📊 Sample order items data:\")\n",
    "        print(self.order_items[['product_name', 'sku_gross_sales', 'quantity']].head())\n",
    "        \n",
    "        # Check for any obvious issues\n",
    "        print(f\"\\n📊 AOV Debug Stats:\")\n",
    "        print(f\"   Total records: {len(self.order_items):,}\")\n",
    "        print(f\"   Revenue range: ${self.order_items['sku_gross_sales'].min():.2f} - ${self.order_items['sku_gross_sales'].max():.2f}\")\n",
    "        print(f\"   Quantity range: {self.order_items['quantity'].min()} - {self.order_items['quantity'].max()}\")\n",
    "        print(f\"   Zero quantities: {(self.order_items['quantity'] == 0).sum():,}\")\n",
    "        \n",
    "        # Calculate simple AOV to see what's happening\n",
    "        if 'quantity' in self.order_items.columns:\n",
    "            # Filter out problematic records\n",
    "            clean_orders = self.order_items[\n",
    "                (self.order_items['quantity'] > 0) & \n",
    "                (self.order_items['sku_gross_sales'] > 0) &\n",
    "                (self.order_items['sku_gross_sales'] < 10000)  # Remove extreme outliers\n",
    "            ].copy()\n",
    "            \n",
    "            if len(clean_orders) > 0:\n",
    "                clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "                \n",
    "                print(f\"   Clean records: {len(clean_orders):,}\")\n",
    "                print(f\"   Clean unit price range: ${clean_orders['unit_price'].min():.2f} - ${clean_orders['unit_price'].max():.2f}\")\n",
    "                print(f\"   Clean unit price mean: ${clean_orders['unit_price'].mean():.2f}\")\n",
    "                print(f\"   Clean unit price median: ${clean_orders['unit_price'].median():.2f}\")\n",
    "            else:\n",
    "                print(\"   ❌ No clean records found!\")\n",
    "        \n",
    "    def calculate_safe_aov_lookup(self):\n",
    "        \"\"\"\n",
    "        Calculate AOV lookup tables with safety checks and outlier removal\n",
    "        \"\"\"\n",
    "        print(\"\\n💰 Calculating SAFE AOV lookup tables...\")\n",
    "        \n",
    "        # Clean the order items data first\n",
    "        clean_orders = self.order_items.copy()\n",
    "        \n",
    "        # Remove problematic records\n",
    "        initial_count = len(clean_orders)\n",
    "        \n",
    "        # Remove zero/negative quantities and revenues\n",
    "        clean_orders = clean_orders[\n",
    "            (clean_orders['quantity'] > 0) & \n",
    "            (clean_orders['sku_gross_sales'] > 0)\n",
    "        ]\n",
    "        \n",
    "        # Remove extreme outliers (likely data errors)\n",
    "        clean_orders = clean_orders[\n",
    "            (clean_orders['sku_gross_sales'] < clean_orders['sku_gross_sales'].quantile(0.99)) &\n",
    "            (clean_orders['quantity'] < clean_orders['quantity'].quantile(0.99))\n",
    "        ]\n",
    "        \n",
    "        print(f\"  🧹 Cleaned data: {initial_count:,} → {len(clean_orders):,} records\")\n",
    "        \n",
    "        if len(clean_orders) == 0:\n",
    "            print(\"  ❌ No clean order data available, using default AOV for all\")\n",
    "            return {\n",
    "                'sku_aov_lookup': {},\n",
    "                'channel_aov_lookup': {},\n",
    "                'monthly_aov_lookup': {}\n",
    "            }\n",
    "        \n",
    "        # Calculate unit price\n",
    "        clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "        \n",
    "        # Remove unit price outliers\n",
    "        unit_price_q99 = clean_orders['unit_price'].quantile(0.99)\n",
    "        unit_price_q01 = clean_orders['unit_price'].quantile(0.01)\n",
    "        \n",
    "        clean_orders = clean_orders[\n",
    "            (clean_orders['unit_price'] >= unit_price_q01) &\n",
    "            (clean_orders['unit_price'] <= unit_price_q99)\n",
    "        ]\n",
    "        \n",
    "        print(f\"  📊 Final clean data: {len(clean_orders):,} records\")\n",
    "        print(f\"  📊 Unit price range: ${clean_orders['unit_price'].min():.2f} - ${clean_orders['unit_price'].max():.2f}\")\n",
    "        print(f\"  📊 Unit price mean: ${clean_orders['unit_price'].mean():.2f}\")\n",
    "        \n",
    "        # Calculate AOV lookups using MEDIAN (more robust than mean)\n",
    "        \n",
    "        # 1. SKU-level AOV (using median unit price)\n",
    "        sku_aov = clean_orders.groupby('product_name')['unit_price'].agg(['median', 'count']).reset_index()\n",
    "        sku_aov = sku_aov[sku_aov['count'] >= 3]  # Need at least 3 transactions\n",
    "        sku_aov_lookup = sku_aov.set_index('product_name')['median'].to_dict()\n",
    "        \n",
    "        # 2. Channel-level AOV\n",
    "        channel_aov = clean_orders.groupby('channel')['unit_price'].agg(['median', 'count']).reset_index()\n",
    "        channel_aov = channel_aov[channel_aov['count'] >= 10]  # Need at least 10 transactions\n",
    "        channel_aov_lookup = channel_aov.set_index('channel')['median'].to_dict()\n",
    "        \n",
    "        # 3. Monthly AOV\n",
    "        clean_orders['year_month'] = clean_orders['order_date'].dt.to_period('M')\n",
    "        monthly_aov = clean_orders.groupby('year_month')['unit_price'].agg(['median', 'count']).reset_index()\n",
    "        monthly_aov = monthly_aov[monthly_aov['count'] >= 5]  # Need at least 5 transactions\n",
    "        monthly_aov_lookup = monthly_aov.set_index('year_month')['median'].to_dict()\n",
    "        \n",
    "        print(f\"  ✅ AOV Lookups created:\")\n",
    "        print(f\"     - SKU AOV: {len(sku_aov_lookup)} SKUs\")\n",
    "        print(f\"     - Channel AOV: {len(channel_aov_lookup)} channels\")\n",
    "        print(f\"     - Monthly AOV: {len(monthly_aov_lookup)} months\")\n",
    "        \n",
    "        # Show sample AOVs to verify they're reasonable\n",
    "        if sku_aov_lookup:\n",
    "            sample_skus = list(sku_aov_lookup.keys())[:3]\n",
    "            print(f\"  📊 Sample SKU AOVs: {[(sku, f'${aov:.2f}') for sku, aov in [(s, sku_aov_lookup[s]) for s in sample_skus]]}\")\n",
    "        \n",
    "        if channel_aov_lookup:\n",
    "            print(f\"  📊 Channel AOVs: {[(ch, f'${aov:.2f}') for ch, aov in channel_aov_lookup.items()]}\")\n",
    "        \n",
    "        return {\n",
    "            'sku_aov_lookup': sku_aov_lookup,\n",
    "            'channel_aov_lookup': channel_aov_lookup,\n",
    "            'monthly_aov_lookup': monthly_aov_lookup\n",
    "        }\n",
    "    \n",
    "    def calculate_seasonal_patterns(self):\n",
    "        \"\"\"\n",
    "        Calculate seasonal patterns safely\n",
    "        \"\"\"\n",
    "        print(\"\\n🗓️ Calculating seasonal patterns...\")\n",
    "        \n",
    "        # Monthly patterns\n",
    "        monthly_customers = self.daily_data.groupby('month')['num_customers'].mean()\n",
    "        overall_avg = self.daily_data['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            monthly_factors = (monthly_customers / overall_avg).fillna(1.0).to_dict()\n",
    "        else:\n",
    "            monthly_factors = {i: 1.0 for i in range(1, 13)}\n",
    "        \n",
    "        # Day-of-week patterns\n",
    "        dow_customers = self.daily_data.groupby('dayofweek')['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            dow_factors = (dow_customers / overall_avg).fillna(1.0).to_dict()\n",
    "        else:\n",
    "            dow_factors = {i: 1.0 for i in range(7)}\n",
    "        \n",
    "        # Reasonable bounds on seasonal factors\n",
    "        for month in monthly_factors:\n",
    "            monthly_factors[month] = max(0.5, min(2.0, monthly_factors[month]))\n",
    "        \n",
    "        for dow in dow_factors:\n",
    "            dow_factors[dow] = max(0.5, min(2.0, dow_factors[dow]))\n",
    "        \n",
    "        print(f\"  ✅ Seasonal patterns calculated\")\n",
    "        print(f\"     Monthly variation: {max(monthly_factors.values()) - min(monthly_factors.values()):.1%}\")\n",
    "        print(f\"     Weekly variation: {max(dow_factors.values()) - min(dow_factors.values()):.1%}\")\n",
    "        \n",
    "        return monthly_factors, dow_factors\n",
    "    \n",
    "    def calculate_retention_patterns(self):\n",
    "        \"\"\"\n",
    "        Calculate customer retention patterns\n",
    "        \"\"\"\n",
    "        print(\"\\n👥 Calculating retention patterns...\")\n",
    "        \n",
    "        # Simple but effective approach\n",
    "        retention_lookup = {}\n",
    "        \n",
    "        # Base retention pattern (industry standard e-commerce)\n",
    "        base_monthly_retention = 0.75  # 75% monthly retention\n",
    "        \n",
    "        for age_months in range(1, 13):\n",
    "            # Exponential decay with some randomness to reflect real patterns\n",
    "            retention = base_monthly_retention ** age_months\n",
    "            \n",
    "            # Add slight variation based on actual data if available\n",
    "            if len(self.daily_data) > 30:\n",
    "                # Look at new vs existing customer ratio trends\n",
    "                avg_new_pct = self.daily_data['pct_new_customers'].mean()\n",
    "                if avg_new_pct > 0.5:  # High churn environment\n",
    "                    retention *= 0.9  # Slightly lower retention\n",
    "                elif avg_new_pct < 0.3:  # Low churn environment\n",
    "                    retention *= 1.1  # Slightly higher retention\n",
    "            \n",
    "            # Ensure reasonable bounds\n",
    "            retention = max(0.05, min(0.95, retention))\n",
    "            retention_lookup[age_months] = retention\n",
    "        \n",
    "        print(f\"  ✅ Retention patterns calculated\")\n",
    "        for age in [1, 3, 6, 12]:\n",
    "            if age in retention_lookup:\n",
    "                print(f\"     {age:2d} months: {retention_lookup[age]:.1%} retention\")\n",
    "        \n",
    "        return retention_lookup\n",
    "    \n",
    "    def get_safe_aov(self, df, aov_lookups):\n",
    "        \"\"\"\n",
    "        Get AOV safely with multiple fallbacks\n",
    "        \"\"\"\n",
    "        # Start with default\n",
    "        aov_series = pd.Series(self.aov_avg, index=df.index)\n",
    "        \n",
    "        # Try SKU AOV\n",
    "        if aov_lookups['sku_aov_lookup']:\n",
    "            sku_aov = df['product_name'].map(aov_lookups['sku_aov_lookup'])\n",
    "            # Only use if reasonable\n",
    "            sku_aov_clean = sku_aov[(sku_aov >= 1) & (sku_aov <= 1000)]\n",
    "            aov_series.update(sku_aov_clean)\n",
    "        \n",
    "        # Try channel AOV\n",
    "        if aov_lookups['channel_aov_lookup']:\n",
    "            channel_aov = df['channel'].map(aov_lookups['channel_aov_lookup'])\n",
    "            channel_aov_clean = channel_aov[(channel_aov >= 1) & (channel_aov <= 1000)]\n",
    "            aov_series = aov_series.fillna(channel_aov_clean)\n",
    "        \n",
    "        # Try monthly AOV\n",
    "        if aov_lookups['monthly_aov_lookup']:\n",
    "            df['year_month'] = df['order_date'].dt.to_period('M')\n",
    "            monthly_aov = df['year_month'].map(aov_lookups['monthly_aov_lookup'])\n",
    "            monthly_aov_clean = monthly_aov[(monthly_aov >= 1) & (monthly_aov <= 1000)]\n",
    "            aov_series = aov_series.fillna(monthly_aov_clean)\n",
    "        \n",
    "        # Final safety check\n",
    "        aov_series = aov_series.fillna(self.aov_avg)\n",
    "        aov_series = np.clip(aov_series, 1, 1000)  # Reasonable AOV bounds\n",
    "        \n",
    "        return aov_series\n",
    "    \n",
    "    def calculate_fixed_benchmark(self):\n",
    "        \"\"\"\n",
    "        Calculate benchmark with all fixes applied\n",
    "        \"\"\"\n",
    "        print(\"\\n🔧 Calculating FIXED benchmark...\")\n",
    "        \n",
    "        # Get all lookup tables\n",
    "        aov_lookups = self.calculate_safe_aov_lookup()\n",
    "        monthly_factors, dow_factors = self.calculate_seasonal_patterns()\n",
    "        retention_lookup = self.calculate_retention_patterns()\n",
    "        \n",
    "        # Start with daily data\n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Basic customer calculations\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        benchmark_df['total_customers'] = benchmark_df['num_customers']\n",
    "        \n",
    "        # Get SAFE AOV\n",
    "        print(\"  💰 Applying safe AOV...\")\n",
    "        benchmark_df['aov_used'] = self.get_safe_aov(benchmark_df, aov_lookups)\n",
    "        \n",
    "        # Seasonal adjustments\n",
    "        print(\"  🗓️ Applying seasonal adjustments...\")\n",
    "        benchmark_df['monthly_factor'] = benchmark_df['month'].map(monthly_factors)\n",
    "        benchmark_df['dow_factor'] = benchmark_df['dayofweek'].map(dow_factors)\n",
    "        benchmark_df['seasonal_adjustment'] = (benchmark_df['monthly_factor'] * 0.7 + \n",
    "                                             benchmark_df['dow_factor'] * 0.3)\n",
    "        \n",
    "        # Calculate demand\n",
    "        print(\"  🎯 Calculating demand...\")\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['base_existing_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['adjusted_existing_demand'] = (benchmark_df['base_existing_demand'] * \n",
    "                                                   benchmark_df['seasonal_adjustment'])\n",
    "        benchmark_df['bm_demand'] = (benchmark_df['new_customer_demand'] + \n",
    "                                   benchmark_df['adjusted_existing_demand'])\n",
    "        \n",
    "        # Calculate actual demand more carefully\n",
    "        print(\"  📊 Calculating actual demand...\")\n",
    "        \n",
    "        # Try to get actual revenue, but be more careful about aggregation\n",
    "        try:\n",
    "            actual_revenue = self.order_items.groupby(['product_name', 'order_date']).agg({\n",
    "                'sku_gross_sales': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                            left_on=['product_name', 'order_date'],\n",
    "                                            right_on=['product_name', 'order_date'], \n",
    "                                            how='left')\n",
    "            \n",
    "            # Clean actual demand\n",
    "            benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(0)\n",
    "            \n",
    "            # If actual demand seems too low, use customer-based estimate\n",
    "            customer_based_estimate = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "            \n",
    "            # Use the minimum of the two (more conservative)\n",
    "            benchmark_df['actual_demand'] = np.where(\n",
    "                benchmark_df['actual_demand'] > 0,\n",
    "                benchmark_df['actual_demand'],\n",
    "                customer_based_estimate\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ Revenue calculation failed: {e}, using customer-based estimate\")\n",
    "            benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # Calculate errors\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add retention rates\n",
    "        print(\"  📉 Adding retention rates...\")\n",
    "        for age_months in range(1, 13):\n",
    "            base_retention = retention_lookup[age_months]\n",
    "            seasonal_retention = base_retention * benchmark_df['seasonal_adjustment']\n",
    "            seasonal_retention = np.clip(seasonal_retention, 0.05, 0.95)\n",
    "            \n",
    "            benchmark_df[f'returning_rate_{age_months}m'] = seasonal_retention\n",
    "            benchmark_df[f'falloff_rate_{age_months}m'] = 1 - seasonal_retention\n",
    "        \n",
    "        # Clean up columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'total_customers',\n",
    "            'aov_used', 'new_customer_demand', 'adjusted_existing_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + [f'returning_rate_{i}m' for i in range(1, 13)] + [f'falloff_rate_{i}m' for i in range(1, 13)]\n",
    "        \n",
    "        benchmark_df = benchmark_df[final_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku'}, inplace=True)\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_fixed_benchmark(self):\n",
    "        \"\"\"\n",
    "        Run the fixed benchmark model\n",
    "        \"\"\"\n",
    "        print(\"🚀 RUNNING FIXED BALANCED BENCHMARK\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        benchmark_df = self.calculate_fixed_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n📊 FIXED BENCHMARK RESULTS:\")\n",
    "        print(f\"  ⚡ Processed {len(benchmark_df):,} records in {duration:.1f} seconds\")\n",
    "        print(f\"  🛍️ SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  📅 Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        print(f\"  💰 Avg AOV: ${benchmark_df['aov_used'].mean():.2f} (vs ${self.aov_avg} default)\")\n",
    "        print(f\"  📊 AOV range: ${benchmark_df['aov_used'].min():.2f} - ${benchmark_df['aov_used'].max():.2f}\")\n",
    "        print(f\"  💰 Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  💰 Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  📈 Avg Error: ${benchmark_df['error_metric'].mean():.2f}\")\n",
    "        print(f\"  📊 MAPE: {benchmark_df['error_percentage'].abs().mean():.1f}%\")\n",
    "        \n",
    "        # Sanity checks\n",
    "        reasonable_aov = (benchmark_df['aov_used'] >= 1) & (benchmark_df['aov_used'] <= 1000)\n",
    "        print(f\"  ✅ Reasonable AOV %: {reasonable_aov.mean():.1%}\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_fixed_benchmark_model(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                            shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                            tiktok__order_items, shopify__order_items, aov_avg=43):\n",
    "    \"\"\"\n",
    "    Fixed benchmark model with proper AOV calculation\n",
    "    \"\"\"\n",
    "    print(\"🔧⚡ FIXED BALANCED BENCHMARK MODEL\")\n",
    "    print(\"🛠️  Fixes:\")\n",
    "    print(\"   ✅ Proper AOV calculation (median-based)\")\n",
    "    print(\"   ✅ Outlier removal\")\n",
    "    print(\"   ✅ Multiple AOV fallbacks\")\n",
    "    print(\"   ✅ Reasonable bounds checking\")\n",
    "    print(\"   ✅ Safe error handling\")\n",
    "    \n",
    "    benchmark = FixedBalancedBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    benchmark_df = benchmark.run_fixed_benchmark()\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "# USAGE:\n",
    "fixed_model, fixed_benchmark_df = run_fixed_benchmark_model(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items, aov_avg=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df55a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_amazon_cohorts = pd.read_parquet(f\"{BASE_PATH}final_amazon__cohorts_monthly_monthly.parquet\")\n",
    "final_shopify_cohorts = pd.read_parquet(f\"{BASE_PATH}final_shopify__cohorts_monthly_monthly.parquet\")\n",
    "final_tiktok_cohorts = pd.read_parquet(f\"{BASE_PATH}final_tiktok_shop__cohorts_monthly_monthly.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f315b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛡️ ENHANCED ROBUST BENCHMARK MODEL\n",
      "🔧 Features:\n",
      "   ✅ Safe data handling\n",
      "   ✅ Simple operations (no complex groupby)\n",
      "   ✅ Multiple fallbacks\n",
      "   ✅ Channel-specific AOV\n",
      "   ✅ Seasonal adjustments\n",
      "   ✅ REAL cohort-based retention rates\n",
      "   ✅ Your exact formulas: BM_Demand = new_customers * AOV + existing_customers * AOV\n",
      "   ✅ Your exact error: Actual_Demand - BM_Demand\n",
      "🛡️ ENHANCED ROBUST Benchmark Model\n",
      "🚀 Designed to avoid pandas groupby errors + Real cohort data\n",
      "   📊 Amazon cohorts: 768 records\n",
      "   📊 TikTok cohorts: 249 records\n",
      "   📊 Shopify cohorts: 3,635 records\n",
      "📊 Combining data...\n",
      "   TikTok: 6,836 records\n",
      "   Amazon: 24,690 records\n",
      "   Shopify: 70,585 records\n",
      "📅 Cleaning dates...\n",
      "   Daily data date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "   Order data date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "✅ Data loaded: 102,111 daily records, 12,002,808 order items\n",
      "🚀 RUNNING ENHANCED ROBUST BENCHMARK\n",
      "==================================================\n",
      "\n",
      "🎯 Calculating enhanced robust benchmark...\n",
      "\n",
      "💰 Calculating simple AOV by channel...\n",
      "   Calculating tiktok AOV...\n",
      "     Tiktok AOV: $30.00 (from 182,645 clean orders)\n",
      "   Calculating amazon AOV...\n",
      "     No clean amazon orders, using default $43\n",
      "   Calculating shopify AOV...\n",
      "     Shopify AOV: $19.95 (from 6,325,992 clean orders)\n",
      "\n",
      "🗓️ Calculating simple seasonal factors...\n",
      "   Monthly variation: 25.5%\n",
      "   Weekly variation: 36.1%\n",
      "\n",
      "📊 Calculating returning rates from real cohort data...\n",
      "\n",
      "🔍 Processing amazon cohorts...\n",
      "    1 months: 9.2% returning rate from 39 cohorts\n",
      "    2 months: 67.2% returning rate from 32 cohorts\n",
      "    3 months: 72.2% returning rate from 30 cohorts\n",
      "    4 months: 78.4% returning rate from 30 cohorts\n",
      "    5 months: 77.2% returning rate from 29 cohorts\n",
      "    6 months: 85.4% returning rate from 30 cohorts\n",
      "    7 months: 79.5% returning rate from 29 cohorts\n",
      "    8 months: 85.0% returning rate from 29 cohorts\n",
      "    9 months: 84.6% returning rate from 29 cohorts\n",
      "   10 months: 80.7% returning rate from 28 cohorts\n",
      "   11 months: 85.8% returning rate from 27 cohorts\n",
      "   12 months: 81.3% returning rate from 26 cohorts\n",
      "\n",
      "🔍 Processing tiktok cohorts...\n",
      "    1 months: 6.4% returning rate from 23 cohorts\n",
      "    2 months: 55.6% returning rate from 21 cohorts\n",
      "    3 months: 64.6% returning rate from 20 cohorts\n",
      "    4 months: 67.4% returning rate from 18 cohorts\n",
      "    5 months: 72.7% returning rate from 17 cohorts\n",
      "    6 months: 77.5% returning rate from 17 cohorts\n",
      "    7 months: 76.9% returning rate from 16 cohorts\n",
      "    8 months: 63.3% returning rate from 16 cohorts\n",
      "    9 months: 72.9% returning rate from 13 cohorts\n",
      "   10 months: 76.9% returning rate from 12 cohorts\n",
      "   11 months: 67.6% returning rate from 11 cohorts\n",
      "   12 months: 76.5% returning rate from 12 cohorts\n",
      "\n",
      "🔍 Processing shopify cohorts...\n",
      "    1 months: 35.5% returning rate from 56 cohorts\n",
      "    2 months: 48.8% returning rate from 50 cohorts\n",
      "    3 months: 63.9% returning rate from 46 cohorts\n",
      "    4 months: 72.4% returning rate from 45 cohorts\n",
      "    5 months: 66.2% returning rate from 45 cohorts\n",
      "    6 months: 76.0% returning rate from 41 cohorts\n",
      "    7 months: 69.0% returning rate from 41 cohorts\n",
      "    8 months: 82.1% returning rate from 36 cohorts\n",
      "    9 months: 85.3% returning rate from 37 cohorts\n",
      "   10 months: 83.1% returning rate from 38 cohorts\n",
      "   11 months: 87.8% returning rate from 36 cohorts\n",
      "   12 months: 85.2% returning rate from 36 cohorts\n",
      "\n",
      "📊 Blended returning rates from cohort data:\n",
      "    1 months: 17.1%\n",
      "    3 months: 66.9%\n",
      "    6 months: 79.6%\n",
      "   12 months: 81.0%\n",
      "   Applying channel-specific AOV...\n",
      "   Applying seasonal adjustments...\n",
      "   Calculating benchmark demand using your formula...\n",
      "   Calculating actual demand...\n",
      "   Adding real cohort-based retention rates...\n",
      "\n",
      "📊 ENHANCED BENCHMARK RESULTS:\n",
      "  ⚡ Processed 102,111 records in 26.2 seconds\n",
      "  🛍️ SKUs: 2332\n",
      "  📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "\n",
      "📺 PERFORMANCE BY CHANNEL:\n",
      "  Tiktok  : 6,836 records, AOV $30.00, MAPE 74.6%\n",
      "  Amazon  : 24,690 records, AOV $43.00, MAPE 50.3%\n",
      "  Shopify : 70,585 records, AOV $19.95, MAPE 49.9%\n",
      "\n",
      "🎯 OVERALL PERFORMANCE:\n",
      "  📊 Total MAPE: 51.6%\n",
      "  💰 Overall Avg AOV: $26.20\n",
      "  📈 Avg BM Demand: $2505.30\n",
      "  📈 Avg Actual Demand: $2572.11\n",
      "  ✅ Using REAL cohort data for retention rates!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedRobustBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, \n",
    "                 final_amazon_cohorts=None, final_tiktok_cohorts=None, final_shopify_cohorts=None,\n",
    "                 aov_avg=43):\n",
    "        \"\"\"\n",
    "        Enhanced robust benchmark that integrates real cohort data for better retention rates\n",
    "        \"\"\"\n",
    "        print(\"🛡️ ENHANCED ROBUST Benchmark Model\")\n",
    "        print(\"🚀 Designed to avoid pandas groupby errors + Real cohort data\")\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Store cohort data\n",
    "        self.cohort_data = {}\n",
    "        if final_amazon_cohorts is not None:\n",
    "            self.cohort_data['amazon'] = final_amazon_cohorts.copy()\n",
    "            print(f\"   📊 Amazon cohorts: {len(final_amazon_cohorts):,} records\")\n",
    "        if final_tiktok_cohorts is not None:\n",
    "            self.cohort_data['tiktok'] = final_tiktok_cohorts.copy()\n",
    "            print(f\"   📊 TikTok cohorts: {len(final_tiktok_cohorts):,} records\")\n",
    "        if final_shopify_cohorts is not None:\n",
    "            self.cohort_data['shopify'] = final_shopify_cohorts.copy()\n",
    "            print(f\"   📊 Shopify cohorts: {len(final_shopify_cohorts):,} records\")\n",
    "        \n",
    "        # Combine data safely (same as original)\n",
    "        print(\"📊 Combining data...\")\n",
    "        daily_datasets = []\n",
    "        \n",
    "        if not tiktok_daily_sku_metrics.empty:\n",
    "            tiktok_data = tiktok_daily_sku_metrics.copy()\n",
    "            tiktok_data['channel'] = 'tiktok'\n",
    "            daily_datasets.append(tiktok_data)\n",
    "            print(f\"   TikTok: {len(tiktok_data):,} records\")\n",
    "        \n",
    "        if not amazon_daily_sku_metrics.empty:\n",
    "            amazon_data = amazon_daily_sku_metrics.copy()\n",
    "            amazon_data['channel'] = 'amazon'\n",
    "            daily_datasets.append(amazon_data)\n",
    "            print(f\"   Amazon: {len(amazon_data):,} records\")\n",
    "        \n",
    "        if not shopify_daily_sku_metrics.empty:\n",
    "            shopify_data = shopify_daily_sku_metrics.copy()\n",
    "            shopify_data['channel'] = 'shopify'\n",
    "            daily_datasets.append(shopify_data)\n",
    "            print(f\"   Shopify: {len(shopify_data):,} records\")\n",
    "        \n",
    "        if daily_datasets:\n",
    "            self.daily_data = pd.concat(daily_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"❌ No daily data available!\")\n",
    "            self.daily_data = pd.DataFrame()\n",
    "            return\n",
    "        \n",
    "        # Combine order data safely (same as original)\n",
    "        order_datasets = []\n",
    "        \n",
    "        if not tiktok__order_items.empty:\n",
    "            tiktok_orders = tiktok__order_items.copy()\n",
    "            tiktok_orders['channel'] = 'tiktok'\n",
    "            order_datasets.append(tiktok_orders)\n",
    "        \n",
    "        if not amazon_order_item_metrics.empty:\n",
    "            amazon_orders = amazon_order_item_metrics.copy()\n",
    "            amazon_orders['channel'] = 'amazon'\n",
    "            order_datasets.append(amazon_orders)\n",
    "        \n",
    "        if not shopify__order_items.empty:\n",
    "            shopify_orders = shopify__order_items.copy()\n",
    "            shopify_orders['channel'] = 'shopify'\n",
    "            order_datasets.append(shopify_orders)\n",
    "        \n",
    "        if order_datasets:\n",
    "            self.order_items = pd.concat(order_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"⚠️ No order data available - will use customer-based estimates\")\n",
    "            self.order_items = pd.DataFrame()\n",
    "        \n",
    "        # Clean dates safely (same as original)\n",
    "        print(\"📅 Cleaning dates...\")\n",
    "        try:\n",
    "            self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "            print(f\"   Daily data date range: {self.daily_data['order_date'].min()} to {self.daily_data['order_date'].max()}\")\n",
    "        except:\n",
    "            print(\"❌ Error cleaning daily data dates\")\n",
    "            return\n",
    "        \n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "                self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "                self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "                print(f\"   Order data date range: {self.order_items['order_date'].min()} to {self.order_items['order_date'].max()}\")\n",
    "            except:\n",
    "                print(\"⚠️ Error cleaning order dates - will use fallback AOV\")\n",
    "                self.order_items = pd.DataFrame()\n",
    "        \n",
    "        print(f\"✅ Data loaded: {len(self.daily_data):,} daily records, {len(self.order_items):,} order items\")\n",
    "    \n",
    "    def calculate_cohort_returning_rates(self):\n",
    "        \"\"\"\n",
    "        Calculate returning rates from real cohort data using customer counts\n",
    "        \n",
    "        Returning (t+1) rate = customers at t+1 / customers at t\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 Calculating returning rates from real cohort data...\")\n",
    "        \n",
    "        if not self.cohort_data:\n",
    "            print(\"   No cohort data available, using default rates\")\n",
    "            return self.calculate_fallback_retention_rates()\n",
    "        \n",
    "        all_channel_rates = {}\n",
    "        \n",
    "        for channel, cohort_df in self.cohort_data.items():\n",
    "            print(f\"\\n🔍 Processing {channel} cohorts...\")\n",
    "            \n",
    "            # Prepare cohort data\n",
    "            df = cohort_df.copy()\n",
    "            df['customer_cohort'] = pd.to_datetime(df['customer_cohort'])\n",
    "            df['order_month'] = pd.to_datetime(df['order_month'])\n",
    "            \n",
    "            # Calculate customer age in months\n",
    "            df['customer_age_months'] = (\n",
    "                (df['order_month'].dt.year - df['customer_cohort'].dt.year) * 12 +\n",
    "                (df['order_month'].dt.month - df['customer_cohort'].dt.month)\n",
    "            )\n",
    "            \n",
    "            # Filter to reasonable ages (0-12 months)\n",
    "            df = df[(df['customer_age_months'] >= 0) & (df['customer_age_months'] <= 12)]\n",
    "            \n",
    "            channel_rates = {}\n",
    "            \n",
    "            # Calculate returning rate for each age (1-12 months)\n",
    "            for age_months in range(1, 13):\n",
    "                \n",
    "                returning_rates_for_age = []\n",
    "                \n",
    "                # For each cohort, calculate returning rate from (age-1) to age\n",
    "                for cohort_date in df['customer_cohort'].unique():\n",
    "                    cohort_data = df[df['customer_cohort'] == cohort_date]\n",
    "                    \n",
    "                    # Get customers at age (age_months - 1)\n",
    "                    customers_at_prev = cohort_data[cohort_data['customer_age_months'] == (age_months - 1)]\n",
    "                    if len(customers_at_prev) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Use the best available customer count column\n",
    "                    if 'unique_customer_ct' in customers_at_prev.columns:\n",
    "                        count_prev = customers_at_prev['unique_customer_ct'].sum()\n",
    "                    elif 'cohort_size' in customers_at_prev.columns:\n",
    "                        count_prev = customers_at_prev['cohort_size'].sum()\n",
    "                    else:\n",
    "                        # Fallback: estimate from orders\n",
    "                        count_prev = customers_at_prev['total_order_ct'].sum() if 'total_order_ct' in customers_at_prev.columns else 0\n",
    "                    \n",
    "                    if count_prev <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get customers at age (age_months)\n",
    "                    customers_at_curr = cohort_data[cohort_data['customer_age_months'] == age_months]\n",
    "                    if len(customers_at_curr) == 0:\n",
    "                        count_curr = 0\n",
    "                    else:\n",
    "                        if 'unique_customer_ct' in customers_at_curr.columns:\n",
    "                            count_curr = customers_at_curr['unique_customer_ct'].sum()\n",
    "                        elif 'cohort_size' in customers_at_curr.columns:\n",
    "                            count_curr = customers_at_curr['cohort_size'].sum()\n",
    "                        else:\n",
    "                            count_curr = customers_at_curr['total_order_ct'].sum() if 'total_order_ct' in customers_at_curr.columns else 0\n",
    "                    \n",
    "                    # Calculate returning rate: customers at t+1 / customers at t\n",
    "                    returning_rate = count_curr / count_prev\n",
    "                    returning_rate = min(returning_rate, 1.0)  # Cap at 100%\n",
    "                    \n",
    "                    returning_rates_for_age.append(returning_rate)\n",
    "                \n",
    "                # Average returning rate across all cohorts for this age\n",
    "                if returning_rates_for_age:\n",
    "                    avg_returning_rate = np.mean(returning_rates_for_age)\n",
    "                    channel_rates[age_months] = avg_returning_rate\n",
    "                    print(f\"   {age_months:2d} months: {avg_returning_rate:.1%} returning rate from {len(returning_rates_for_age)} cohorts\")\n",
    "                else:\n",
    "                    # Fallback if no data\n",
    "                    fallback_rate = 0.75 ** age_months\n",
    "                    channel_rates[age_months] = fallback_rate\n",
    "                    print(f\"   {age_months:2d} months: {fallback_rate:.1%} returning rate (fallback)\")\n",
    "            \n",
    "            all_channel_rates[channel] = channel_rates\n",
    "        \n",
    "        # Calculate blended returning rates across channels\n",
    "        blended_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            rates_for_age = []\n",
    "            for channel_rates in all_channel_rates.values():\n",
    "                if age_months in channel_rates:\n",
    "                    rates_for_age.append(channel_rates[age_months])\n",
    "            \n",
    "            if rates_for_age:\n",
    "                blended_rates[age_months] = np.mean(rates_for_age)\n",
    "            else:\n",
    "                # Fallback\n",
    "                blended_rates[age_months] = 0.75 ** age_months\n",
    "        \n",
    "        print(f\"\\n📊 Blended returning rates from cohort data:\")\n",
    "        for age in [1, 3, 6, 12]:\n",
    "            if age in blended_rates:\n",
    "                print(f\"   {age:2d} months: {blended_rates[age]:.1%}\")\n",
    "        \n",
    "        return blended_rates\n",
    "    \n",
    "    def calculate_fallback_retention_rates(self):\n",
    "        \"\"\"\n",
    "        Fallback retention calculation (same as original) if no cohort data\n",
    "        \"\"\"\n",
    "        print(\"\\n👥 Calculating fallback retention rates...\")\n",
    "        \n",
    "        # Check if we have customer lifecycle data\n",
    "        has_lifecycle_data = ('pct_new_customers' in self.daily_data.columns and \n",
    "                             not self.daily_data['pct_new_customers'].isna().all())\n",
    "        \n",
    "        if has_lifecycle_data:\n",
    "            avg_new_pct = self.daily_data['pct_new_customers'].mean()\n",
    "            \n",
    "            if avg_new_pct > 0.6:  # High churn environment\n",
    "                base_retention = 0.70\n",
    "                print(\"   High acquisition environment detected - using 70% base retention\")\n",
    "            elif avg_new_pct < 0.3:  # Low churn environment\n",
    "                base_retention = 0.80\n",
    "                print(\"   High retention environment detected - using 80% base retention\")\n",
    "            else:\n",
    "                base_retention = 0.75\n",
    "                print(\"   Balanced environment detected - using 75% base retention\")\n",
    "        else:\n",
    "            base_retention = 0.75\n",
    "            print(\"   Using standard 75% base retention\")\n",
    "        \n",
    "        # Calculate retention for each age month\n",
    "        retention_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            retention = base_retention ** age_months\n",
    "            retention = max(0.05, min(0.95, retention))\n",
    "            retention_rates[age_months] = retention\n",
    "        \n",
    "        print(f\"   1-month retention: {retention_rates[1]:.1%}\")\n",
    "        print(f\"   6-month retention: {retention_rates[6]:.1%}\")\n",
    "        print(f\"   12-month retention: {retention_rates[12]:.1%}\")\n",
    "        \n",
    "        return retention_rates\n",
    "    \n",
    "    # Keep all the other methods from the original SimpleRobustBenchmark\n",
    "    def calculate_simple_aov_by_channel(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\n💰 Calculating simple AOV by channel...\")\n",
    "        \n",
    "        channel_aov = {}\n",
    "        \n",
    "        if self.order_items.empty:\n",
    "            print(\"   Using default AOV for all channels\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['sku_gross_sales', 'quantity', 'channel']\n",
    "        missing_cols = [col for col in required_cols if col not in self.order_items.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"   Missing columns: {missing_cols}, using default AOV\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        for channel in ['tiktok', 'amazon', 'shopify']:\n",
    "            print(f\"   Calculating {channel} AOV...\")\n",
    "            \n",
    "            # Filter channel data\n",
    "            channel_orders = self.order_items[self.order_items['channel'] == channel].copy()\n",
    "            \n",
    "            if len(channel_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Clean data simply\n",
    "            clean_orders = channel_orders[\n",
    "                (channel_orders['quantity'] > 0) & \n",
    "                (channel_orders['sku_gross_sales'] > 0) &\n",
    "                (channel_orders['sku_gross_sales'] < 10000) &  # Remove extreme outliers\n",
    "                (channel_orders['quantity'] < 1000)  # Remove bulk orders\n",
    "            ].copy()\n",
    "            \n",
    "            if len(clean_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No clean {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate unit prices\n",
    "            clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "            \n",
    "            # Remove unit price outliers\n",
    "            unit_prices = clean_orders['unit_price']\n",
    "            q1 = unit_prices.quantile(0.25)\n",
    "            q3 = unit_prices.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(1, q1 - 1.5 * iqr)\n",
    "            upper_bound = min(1000, q3 + 1.5 * iqr)\n",
    "            \n",
    "            final_prices = unit_prices[(unit_prices >= lower_bound) & (unit_prices <= upper_bound)]\n",
    "            \n",
    "            if len(final_prices) > 0:\n",
    "                channel_aov[channel] = final_prices.median()\n",
    "                print(f\"     {channel.capitalize()} AOV: ${channel_aov[channel]:.2f} (from {len(final_prices):,} clean orders)\")\n",
    "            else:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No valid {channel} prices, using default ${self.aov_avg}\")\n",
    "        \n",
    "        return channel_aov\n",
    "    \n",
    "    def calculate_simple_seasonal_factors(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\n🗓️ Calculating simple seasonal factors...\")\n",
    "        \n",
    "        # Add temporal columns safely\n",
    "        try:\n",
    "            self.daily_data['month'] = self.daily_data['order_date'].dt.month\n",
    "            self.daily_data['dayofweek'] = self.daily_data['order_date'].dt.dayofweek\n",
    "        except:\n",
    "            print(\"   Error adding temporal columns, using default factors\")\n",
    "            return {\n",
    "                'monthly_factors': {i: 1.0 for i in range(1, 13)},\n",
    "                'dow_factors': {i: 1.0 for i in range(7)}\n",
    "            }\n",
    "        \n",
    "        # Calculate monthly factors simply\n",
    "        monthly_factors = {}\n",
    "        overall_avg = self.daily_data['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            for month in range(1, 13):\n",
    "                month_data = self.daily_data[self.daily_data['month'] == month]\n",
    "                if len(month_data) > 0:\n",
    "                    month_avg = month_data['num_customers'].mean()\n",
    "                    factor = month_avg / overall_avg\n",
    "                    monthly_factors[month] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    monthly_factors[month] = 1.0\n",
    "        else:\n",
    "            monthly_factors = {i: 1.0 for i in range(1, 13)}\n",
    "        \n",
    "        # Calculate day-of-week factors simply\n",
    "        dow_factors = {}\n",
    "        if overall_avg > 0:\n",
    "            for dow in range(7):\n",
    "                dow_data = self.daily_data[self.daily_data['dayofweek'] == dow]\n",
    "                if len(dow_data) > 0:\n",
    "                    dow_avg = dow_data['num_customers'].mean()\n",
    "                    factor = dow_avg / overall_avg\n",
    "                    dow_factors[dow] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    dow_factors[dow] = 1.0\n",
    "        else:\n",
    "            dow_factors = {i: 1.0 for i in range(7)}\n",
    "        \n",
    "        print(f\"   Monthly variation: {max(monthly_factors.values()) - min(monthly_factors.values()):.1%}\")\n",
    "        print(f\"   Weekly variation: {max(dow_factors.values()) - min(dow_factors.values()):.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'monthly_factors': monthly_factors,\n",
    "            'dow_factors': dow_factors\n",
    "        }\n",
    "    \n",
    "    def calculate_robust_benchmark(self):\n",
    "        \"\"\"\n",
    "        Enhanced benchmark calculation using real cohort data for retention rates\n",
    "        \"\"\"\n",
    "        print(\"\\n🎯 Calculating enhanced robust benchmark...\")\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"❌ No daily data available!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get patterns (using cohort data for retention rates!)\n",
    "        channel_aov = self.calculate_simple_aov_by_channel()\n",
    "        seasonal_patterns = self.calculate_simple_seasonal_factors()\n",
    "        retention_rates = self.calculate_cohort_returning_rates()  # ✅ Use real cohort data!\n",
    "        \n",
    "        # Start with daily data\n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Basic customer calculations\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        benchmark_df['total_customers'] = benchmark_df['num_customers']\n",
    "        \n",
    "        # Apply AOV by channel\n",
    "        print(\"   Applying channel-specific AOV...\")\n",
    "        benchmark_df['aov_used'] = benchmark_df['channel'].map(channel_aov).fillna(self.aov_avg)\n",
    "        \n",
    "        # Apply seasonal adjustments\n",
    "        print(\"   Applying seasonal adjustments...\")\n",
    "        benchmark_df['monthly_factor'] = benchmark_df['month'].map(seasonal_patterns['monthly_factors']).fillna(1.0)\n",
    "        benchmark_df['dow_factor'] = benchmark_df['dayofweek'].map(seasonal_patterns['dow_factors']).fillna(1.0)\n",
    "        benchmark_df['seasonal_adjustment'] = (benchmark_df['monthly_factor'] * 0.7 + \n",
    "                                             benchmark_df['dow_factor'] * 0.3)\n",
    "        \n",
    "        # Calculate demand using YOUR EXACT FORMULA\n",
    "        print(\"   Calculating benchmark demand using your formula...\")\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['existing_customer_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # ✅ YOUR EXACT FORMULA: BM_Demand = new_customers * AOV + existing_customers * AOV\n",
    "        benchmark_df['bm_demand'] = (benchmark_df['new_customer_demand'] + \n",
    "                                   benchmark_df['existing_customer_demand'])\n",
    "        \n",
    "        # Calculate actual demand\n",
    "        print(\"   Calculating actual demand...\")\n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                # Try to get actual revenue\n",
    "                actual_revenue = self.order_items.groupby(['product_name', 'order_date', 'channel']).agg({\n",
    "                    'sku_gross_sales': 'sum'\n",
    "                }).reset_index()\n",
    "                \n",
    "                benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                                left_on=['product_name', 'order_date', 'channel'],\n",
    "                                                right_on=['product_name', 'order_date', 'channel'], \n",
    "                                                how='left')\n",
    "                \n",
    "                benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(\n",
    "                    benchmark_df['total_customers'] * benchmark_df['aov_used'])\n",
    "            except:\n",
    "                print(\"     Revenue calculation failed, using customer-based estimate\")\n",
    "                benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        else:\n",
    "            benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # ✅ YOUR EXACT ERROR FORMULA: Actual_Demand - BM_Demand\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add real cohort-based retention rates\n",
    "        print(\"   Adding real cohort-based retention rates...\")\n",
    "        for age_months in range(1, 13):\n",
    "            real_retention = retention_rates[age_months]\n",
    "            # Apply seasonal adjustment\n",
    "            seasonal_retention = real_retention * benchmark_df['seasonal_adjustment']\n",
    "            seasonal_retention = np.clip(seasonal_retention, 0.05, 0.95)\n",
    "            \n",
    "            benchmark_df[f'returning_rate_{age_months}m'] = seasonal_retention\n",
    "            benchmark_df[f'falloff_rate_{age_months}m'] = 1 - seasonal_retention\n",
    "        \n",
    "        # Select final columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'total_customers',\n",
    "            'aov_used', 'new_customer_demand', 'existing_customer_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + [f'returning_rate_{i}m' for i in range(1, 13)] + [f'falloff_rate_{i}m' for i in range(1, 13)]\n",
    "        \n",
    "        # Only keep columns that exist\n",
    "        existing_columns = [col for col in final_columns if col in benchmark_df.columns]\n",
    "        benchmark_df = benchmark_df[existing_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku'}, inplace=True)\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_enhanced_benchmark(self):\n",
    "        \"\"\"\n",
    "        Run the enhanced benchmark with real cohort data\n",
    "        \"\"\"\n",
    "        print(\"🚀 RUNNING ENHANCED ROBUST BENCHMARK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"❌ No data to process!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        benchmark_df = self.calculate_robust_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if benchmark_df.empty:\n",
    "            print(\"❌ No benchmark results generated!\")\n",
    "            return benchmark_df\n",
    "        \n",
    "        print(f\"\\n📊 ENHANCED BENCHMARK RESULTS:\")\n",
    "        print(f\"  ⚡ Processed {len(benchmark_df):,} records in {duration:.1f} seconds\")\n",
    "        print(f\"  🛍️ SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  📅 Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        \n",
    "        # Performance by channel\n",
    "        if 'channel' in benchmark_df.columns:\n",
    "            print(f\"\\n📺 PERFORMANCE BY CHANNEL:\")\n",
    "            for channel in benchmark_df['channel'].unique():\n",
    "                channel_data = benchmark_df[benchmark_df['channel'] == channel]\n",
    "                avg_aov = channel_data['aov_used'].mean()\n",
    "                mape = channel_data['error_percentage'].abs().mean()\n",
    "                records = len(channel_data)\n",
    "                \n",
    "                print(f\"  {channel.capitalize():<8}: {records:,} records, AOV ${avg_aov:.2f}, MAPE {mape:.1f}%\")\n",
    "        \n",
    "        # Overall performance\n",
    "        overall_mape = benchmark_df['error_percentage'].abs().mean()\n",
    "        overall_aov = benchmark_df['aov_used'].mean()\n",
    "        \n",
    "        print(f\"\\n🎯 OVERALL PERFORMANCE:\")\n",
    "        print(f\"  📊 Total MAPE: {overall_mape:.1f}%\")\n",
    "        print(f\"  💰 Overall Avg AOV: ${overall_aov:.2f}\")\n",
    "        print(f\"  📈 Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  📈 Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  ✅ Using REAL cohort data for retention rates!\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_enhanced_robust_benchmark(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                                shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                                tiktok__order_items, shopify__order_items,\n",
    "                                final_amazon_cohorts=None, final_tiktok_cohorts=None, \n",
    "                                final_shopify_cohorts=None, aov_avg=43):\n",
    "    \"\"\"\n",
    "    Enhanced robust benchmark model with real cohort data integration\n",
    "    \"\"\"\n",
    "    print(\"🛡️ ENHANCED ROBUST BENCHMARK MODEL\")\n",
    "    print(\"🔧 Features:\")\n",
    "    print(\"   ✅ Safe data handling\")\n",
    "    print(\"   ✅ Simple operations (no complex groupby)\")\n",
    "    print(\"   ✅ Multiple fallbacks\")\n",
    "    print(\"   ✅ Channel-specific AOV\")\n",
    "    print(\"   ✅ Seasonal adjustments\")\n",
    "    print(\"   ✅ REAL cohort-based retention rates\")\n",
    "    print(\"   ✅ Your exact formulas: BM_Demand = new_customers * AOV + existing_customers * AOV\")\n",
    "    print(\"   ✅ Your exact error: Actual_Demand - BM_Demand\")\n",
    "    \n",
    "    benchmark = EnhancedRobustBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        final_amazon_cohorts=final_amazon_cohorts,\n",
    "        final_tiktok_cohorts=final_tiktok_cohorts,\n",
    "        final_shopify_cohorts=final_shopify_cohorts,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    benchmark_df = benchmark.run_enhanced_benchmark()\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "# USAGE:\n",
    "enhanced_model, enhanced_benchmark_df = run_enhanced_robust_benchmark(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items,\n",
    "    final_amazon_cohorts, final_tiktok_cohorts, final_shopify_cohorts, aov_avg=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛡️ ENHANCED ROBUST BENCHMARK MODEL\n",
      "🔧 Features:\n",
      "   ✅ Safe data handling\n",
      "   ✅ Simple operations (no complex groupby)\n",
      "   ✅ Multiple fallbacks\n",
      "   ✅ Channel-specific AOV\n",
      "   ✅ Seasonal adjustments\n",
      "   ✅ REAL cohort-based retention rates\n",
      "   ✅ Your exact formulas: BM_Demand = new_customers * AOV + existing_customers * AOV\n",
      "   ✅ Your exact error: Actual_Demand - BM_Demand\n",
      "🛡️ ENHANCED ROBUST Benchmark Model\n",
      "🚀 Designed to avoid pandas groupby errors + Real cohort data\n",
      "   📊 Amazon cohorts: 768 records\n",
      "   📊 TikTok cohorts: 249 records\n",
      "   📊 Shopify cohorts: 3,635 records\n",
      "📊 Combining data...\n",
      "   TikTok: 6,836 records\n",
      "   Amazon: 24,690 records\n",
      "   Shopify: 70,585 records\n",
      "📅 Cleaning dates...\n",
      "   Daily data date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "   Order data date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "✅ Data loaded: 102,111 daily records, 12,002,808 order items\n",
      "🚀 RUNNING ENHANCED ROBUST BENCHMARK\n",
      "==================================================\n",
      "\n",
      "🎯 Calculating enhanced robust benchmark...\n",
      "\n",
      "💰 Calculating simple AOV by channel...\n",
      "   Calculating tiktok AOV...\n",
      "     Tiktok AOV: $30.00 (from 182,645 clean orders)\n",
      "   Calculating amazon AOV...\n",
      "     No clean amazon orders, using default $43\n",
      "   Calculating shopify AOV...\n",
      "     Shopify AOV: $19.95 (from 6,325,992 clean orders)\n",
      "\n",
      "🗓️ Calculating simple seasonal factors...\n",
      "   Monthly variation: 25.5%\n",
      "   Weekly variation: 36.1%\n",
      "\n",
      "📊 Calculating returning rates based on sales fall-off...\n",
      "\n",
      "🔍 Processing amazon cohorts...\n",
      "   📊 Sample cohort sales progression:\n",
      "     Age  0: $  100.95\n",
      "     Debug 2022-04 age 1: $101 → $0 = 0.0%\n",
      "     Debug 2022-05 age 1: $1534 → $0 = 0.0%\n",
      "     Debug 2022-06 age 1: $2347 → $0 = 0.0%\n",
      "    1 months: 11.7% sales retention from 39 cohorts\n",
      "     Debug 2022-04 age 2: $101 → $0 = 0.0%\n",
      "     Debug 2022-05 age 2: $1534 → $0 = 0.0%\n",
      "     Debug 2022-06 age 2: $2347 → $0 = 0.0%\n",
      "    2 months: 8.8% sales retention from 39 cohorts\n",
      "     Debug 2022-04 age 3: $101 → $0 = 0.0%\n",
      "     Debug 2022-05 age 3: $1534 → $0 = 0.0%\n",
      "     Debug 2022-06 age 3: $2347 → $0 = 0.0%\n",
      "    3 months: 6.9% sales retention from 39 cohorts\n",
      "    4 months: 5.8% sales retention from 39 cohorts\n",
      "    5 months: 4.7% sales retention from 39 cohorts\n",
      "    6 months: 4.7% sales retention from 39 cohorts\n",
      "    7 months: 3.9% sales retention from 39 cohorts\n",
      "    8 months: 3.9% sales retention from 39 cohorts\n",
      "    9 months: 3.4% sales retention from 39 cohorts\n",
      "   10 months: 2.8% sales retention from 39 cohorts\n",
      "   11 months: 2.6% sales retention from 39 cohorts\n",
      "   12 months: 2.4% sales retention from 39 cohorts\n",
      "\n",
      "🔍 Processing tiktok cohorts...\n",
      "   📊 Sample cohort sales progression:\n",
      "     Age  0: $  603.85\n",
      "     Age  2: $   74.85\n",
      "     Age  7: $   24.95\n",
      "     Age 11: $  139.78\n",
      "     Age 12: $   29.99\n",
      "     Debug 2023-08 age 1: $604 → $0 = 0.0%\n",
      "     Debug 2023-09 age 1: $4049 → $150 = 3.7%\n",
      "     Debug 2023-11 age 1: $13261 → $698 = 5.3%\n",
      "    1 months: 10.6% sales retention from 23 cohorts\n",
      "     Debug 2023-08 age 2: $604 → $75 = 12.4%\n",
      "     Debug 2023-09 age 2: $4049 → $50 = 1.2%\n",
      "     Debug 2023-11 age 2: $13261 → $264 = 2.0%\n",
      "    2 months: 7.3% sales retention from 23 cohorts\n",
      "     Debug 2023-08 age 3: $604 → $0 = 0.0%\n",
      "     Debug 2023-09 age 3: $4049 → $0 = 0.0%\n",
      "     Debug 2023-11 age 3: $13261 → $507 = 3.8%\n",
      "    3 months: 5.4% sales retention from 23 cohorts\n",
      "    4 months: 3.8% sales retention from 23 cohorts\n",
      "    5 months: 3.3% sales retention from 23 cohorts\n",
      "    6 months: 3.1% sales retention from 23 cohorts\n",
      "    7 months: 2.9% sales retention from 23 cohorts\n",
      "    8 months: 2.0% sales retention from 23 cohorts\n",
      "    9 months: 1.6% sales retention from 23 cohorts\n",
      "   10 months: 1.5% sales retention from 23 cohorts\n",
      "   11 months: 2.1% sales retention from 23 cohorts\n",
      "   12 months: 1.3% sales retention from 23 cohorts\n",
      "\n",
      "🔍 Processing shopify cohorts...\n",
      "   📊 Sample cohort sales progression:\n",
      "     Age  0: $  155.75\n",
      "     Age  6: $  119.75\n",
      "     Age  9: $  145.64\n",
      "     Debug 2020-11 age 1: $156 → $0 = 0.0%\n",
      "     Debug 2020-12 age 1: $602 → $0 = 0.0%\n",
      "     Debug 2021-01 age 1: $958 → $0 = 0.0%\n",
      "    1 months: 25.7% sales retention from 56 cohorts\n",
      "     Debug 2020-11 age 2: $156 → $0 = 0.0%\n",
      "     Debug 2020-12 age 2: $602 → $0 = 0.0%\n",
      "     Debug 2021-01 age 2: $958 → $0 = 0.0%\n",
      "    2 months: 11.2% sales retention from 56 cohorts\n",
      "     Debug 2020-11 age 3: $156 → $0 = 0.0%\n",
      "     Debug 2020-12 age 3: $602 → $0 = 0.0%\n",
      "     Debug 2021-01 age 3: $958 → $0 = 0.0%\n",
      "    3 months: 7.7% sales retention from 56 cohorts\n",
      "    4 months: 6.0% sales retention from 56 cohorts\n",
      "    5 months: 4.7% sales retention from 56 cohorts\n",
      "    6 months: 5.6% sales retention from 56 cohorts\n",
      "    7 months: 3.9% sales retention from 56 cohorts\n",
      "    8 months: 3.8% sales retention from 56 cohorts\n",
      "    9 months: 5.1% sales retention from 56 cohorts\n",
      "   10 months: 3.2% sales retention from 56 cohorts\n",
      "   11 months: 3.5% sales retention from 56 cohorts\n",
      "   12 months: 3.2% sales retention from 56 cohorts\n",
      "\n",
      "📊 Blended sales retention rates from cohort data:\n",
      "    1 months: 16.0% (vs initial spend)\n",
      "    3 months: 6.6% (vs initial spend)\n",
      "    6 months: 4.5% (vs initial spend)\n",
      "   12 months: 2.3% (vs initial spend)\n",
      "\n",
      "🔍 Sanity check - sales retention should generally decline:\n",
      "   ✅ Sales retention pattern looks realistic (generally declining)\n",
      "   Applying channel-specific AOV...\n",
      "   Applying seasonal adjustments...\n",
      "   Calculating benchmark demand using your formula...\n",
      "   Calculating actual demand...\n",
      "   Adding real cohort-based retention rates...\n",
      "\n",
      "📊 ENHANCED BENCHMARK RESULTS:\n",
      "  ⚡ Processed 102,111 records in 33.2 seconds\n",
      "  🛍️ SKUs: 2332\n",
      "  📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "\n",
      "📺 PERFORMANCE BY CHANNEL:\n",
      "  Tiktok  : 6,836 records, AOV $30.00, MAPE 74.6%\n",
      "  Amazon  : 24,690 records, AOV $43.00, MAPE 50.3%\n",
      "  Shopify : 70,585 records, AOV $19.95, MAPE 49.9%\n",
      "\n",
      "🎯 OVERALL PERFORMANCE:\n",
      "  📊 Total MAPE: 51.6%\n",
      "  💰 Overall Avg AOV: $26.20\n",
      "  📈 Avg BM Demand: $2505.30\n",
      "  📈 Avg Actual Demand: $2572.11\n",
      "  ✅ Using REAL cohort data for retention rates!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedRobustBenchmark:\n",
    "    def __init__(self, tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "                 amazon_order_item_metrics, tiktok__order_items, shopify__order_items, \n",
    "                 final_amazon_cohorts=None, final_tiktok_cohorts=None, final_shopify_cohorts=None,\n",
    "                 aov_avg=43):\n",
    "        \"\"\"\n",
    "        Enhanced robust benchmark that integrates real cohort data for better retention rates\n",
    "        \"\"\"\n",
    "        print(\"🛡️ ENHANCED ROBUST Benchmark Model\")\n",
    "        print(\"🚀 Designed to avoid pandas groupby errors + Real cohort data\")\n",
    "        \n",
    "        self.aov_avg = aov_avg\n",
    "        \n",
    "        # Store cohort data\n",
    "        self.cohort_data = {}\n",
    "        if final_amazon_cohorts is not None:\n",
    "            self.cohort_data['amazon'] = final_amazon_cohorts.copy()\n",
    "            print(f\"   📊 Amazon cohorts: {len(final_amazon_cohorts):,} records\")\n",
    "        if final_tiktok_cohorts is not None:\n",
    "            self.cohort_data['tiktok'] = final_tiktok_cohorts.copy()\n",
    "            print(f\"   📊 TikTok cohorts: {len(final_tiktok_cohorts):,} records\")\n",
    "        if final_shopify_cohorts is not None:\n",
    "            self.cohort_data['shopify'] = final_shopify_cohorts.copy()\n",
    "            print(f\"   📊 Shopify cohorts: {len(final_shopify_cohorts):,} records\")\n",
    "        \n",
    "        # Combine data safely (same as original)\n",
    "        print(\"📊 Combining data...\")\n",
    "        daily_datasets = []\n",
    "        \n",
    "        if not tiktok_daily_sku_metrics.empty:\n",
    "            tiktok_data = tiktok_daily_sku_metrics.copy()\n",
    "            tiktok_data['channel'] = 'tiktok'\n",
    "            daily_datasets.append(tiktok_data)\n",
    "            print(f\"   TikTok: {len(tiktok_data):,} records\")\n",
    "        \n",
    "        if not amazon_daily_sku_metrics.empty:\n",
    "            amazon_data = amazon_daily_sku_metrics.copy()\n",
    "            amazon_data['channel'] = 'amazon'\n",
    "            daily_datasets.append(amazon_data)\n",
    "            print(f\"   Amazon: {len(amazon_data):,} records\")\n",
    "        \n",
    "        if not shopify_daily_sku_metrics.empty:\n",
    "            shopify_data = shopify_daily_sku_metrics.copy()\n",
    "            shopify_data['channel'] = 'shopify'\n",
    "            daily_datasets.append(shopify_data)\n",
    "            print(f\"   Shopify: {len(shopify_data):,} records\")\n",
    "        \n",
    "        if daily_datasets:\n",
    "            self.daily_data = pd.concat(daily_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"❌ No daily data available!\")\n",
    "            self.daily_data = pd.DataFrame()\n",
    "            return\n",
    "        \n",
    "        # Combine order data safely (same as original)\n",
    "        order_datasets = []\n",
    "        \n",
    "        if not tiktok__order_items.empty:\n",
    "            tiktok_orders = tiktok__order_items.copy()\n",
    "            tiktok_orders['channel'] = 'tiktok'\n",
    "            order_datasets.append(tiktok_orders)\n",
    "        \n",
    "        if not amazon_order_item_metrics.empty:\n",
    "            amazon_orders = amazon_order_item_metrics.copy()\n",
    "            amazon_orders['channel'] = 'amazon'\n",
    "            order_datasets.append(amazon_orders)\n",
    "        \n",
    "        if not shopify__order_items.empty:\n",
    "            shopify_orders = shopify__order_items.copy()\n",
    "            shopify_orders['channel'] = 'shopify'\n",
    "            order_datasets.append(shopify_orders)\n",
    "        \n",
    "        if order_datasets:\n",
    "            self.order_items = pd.concat(order_datasets, ignore_index=True)\n",
    "        else:\n",
    "            print(\"⚠️ No order data available - will use customer-based estimates\")\n",
    "            self.order_items = pd.DataFrame()\n",
    "        \n",
    "        # Clean dates safely (same as original)\n",
    "        print(\"📅 Cleaning dates...\")\n",
    "        try:\n",
    "            self.daily_data['order_date'] = pd.to_datetime(self.daily_data['order_date'])\n",
    "            print(f\"   Daily data date range: {self.daily_data['order_date'].min()} to {self.daily_data['order_date'].max()}\")\n",
    "        except:\n",
    "            print(\"❌ Error cleaning daily data dates\")\n",
    "            return\n",
    "        \n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                self.order_items['local_order_ts'] = pd.to_datetime(self.order_items['local_order_ts'])\n",
    "                self.order_items['order_date'] = self.order_items['local_order_ts'].dt.date\n",
    "                self.order_items['order_date'] = pd.to_datetime(self.order_items['order_date'])\n",
    "                print(f\"   Order data date range: {self.order_items['order_date'].min()} to {self.order_items['order_date'].max()}\")\n",
    "            except:\n",
    "                print(\"⚠️ Error cleaning order dates - will use fallback AOV\")\n",
    "                self.order_items = pd.DataFrame()\n",
    "        \n",
    "        print(f\"✅ Data loaded: {len(self.daily_data):,} daily records, {len(self.order_items):,} order items\")\n",
    "    \n",
    "    def calculate_cohort_returning_rates(self):\n",
    "       \n",
    "        print(\"\\n📊 Calculating returning rates based on sales fall-off...\")\n",
    "        \n",
    "        if not self.cohort_data:\n",
    "            print(\"   No cohort data available, using default rates\")\n",
    "            return self.calculate_fallback_retention_rates()\n",
    "        \n",
    "        all_channel_rates = {}\n",
    "        \n",
    "        for channel, cohort_df in self.cohort_data.items():\n",
    "            print(f\"\\n🔍 Processing {channel} cohorts...\")\n",
    "            \n",
    "            # Prepare cohort data\n",
    "            df = cohort_df.copy()\n",
    "            df['customer_cohort'] = pd.to_datetime(df['customer_cohort'])\n",
    "            df['order_month'] = pd.to_datetime(df['order_month'])\n",
    "            \n",
    "            # Calculate customer age in months\n",
    "            df['customer_age_months'] = (\n",
    "                (df['order_month'].dt.year - df['customer_cohort'].dt.year) * 12 +\n",
    "                (df['order_month'].dt.month - df['customer_cohort'].dt.month)\n",
    "            )\n",
    "            \n",
    "            # Filter to reasonable ages (0-12 months)\n",
    "            df = df[(df['customer_age_months'] >= 0) & (df['customer_age_months'] <= 12)]\n",
    "            \n",
    "            # DEBUGGING: Show sample data to understand the pattern\n",
    "            print(f\"   📊 Sample cohort sales progression:\")\n",
    "            sample_cohort = df['customer_cohort'].unique()[0]\n",
    "            sample_data = df[df['customer_cohort'] == sample_cohort].sort_values('customer_age_months')\n",
    "            \n",
    "            for _, row in sample_data.head(8).iterrows():\n",
    "                sales_amount = row['total_gross_sales']\n",
    "                print(f\"     Age {row['customer_age_months']:2d}: ${sales_amount:8.2f}\")\n",
    "            \n",
    "            channel_rates = {}\n",
    "            \n",
    "            # Calculate sales retention rate for each age (1-12 months)\n",
    "            for age_months in range(1, 13):\n",
    "                \n",
    "                retention_rates_for_age = []\n",
    "                \n",
    "                # For each cohort, calculate sales retention from initial spend\n",
    "                for cohort_date in df['customer_cohort'].unique():\n",
    "                    cohort_data = df[df['customer_cohort'] == cohort_date]\n",
    "                    \n",
    "                    # Find initial spend (where customer_cohort = order_month, i.e., age 0)\n",
    "                    initial_data = cohort_data[cohort_data['customer_age_months'] == 0]\n",
    "                    if len(initial_data) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    initial_sales = initial_data['total_gross_sales'].sum()\n",
    "                    if initial_sales <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Find current spend at this age\n",
    "                    current_data = cohort_data[cohort_data['customer_age_months'] == age_months]\n",
    "                    if len(current_data) == 0:\n",
    "                        current_sales = 0\n",
    "                    else:\n",
    "                        current_sales = current_data['total_gross_sales'].sum()\n",
    "                    \n",
    "                    # Calculate sales retention: current_sales / initial_sales\n",
    "                    sales_retention = current_sales / initial_sales\n",
    "                    sales_retention = min(sales_retention, 1.0)  # Cap at 100%\n",
    "                    \n",
    "                    # DEBUG: Show calculation for first few cohorts and ages\n",
    "                    if age_months <= 3 and len(retention_rates_for_age) < 3:\n",
    "                        print(f\"     Debug {cohort_date.strftime('%Y-%m')} age {age_months}: ${initial_sales:.0f} → ${current_sales:.0f} = {sales_retention:.1%}\")\n",
    "                    \n",
    "                    retention_rates_for_age.append(sales_retention)\n",
    "                \n",
    "                # Average sales retention rate across all cohorts for this age\n",
    "                if retention_rates_for_age:\n",
    "                    avg_retention_rate = np.mean(retention_rates_for_age)\n",
    "                    channel_rates[age_months] = avg_retention_rate\n",
    "                    print(f\"   {age_months:2d} months: {avg_retention_rate:.1%} sales retention from {len(retention_rates_for_age)} cohorts\")\n",
    "                else:\n",
    "                    # Fallback if no data - ensure it declines\n",
    "                    fallback_rate = max(0.05, 0.75 ** age_months)\n",
    "                    channel_rates[age_months] = fallback_rate\n",
    "                    print(f\"   {age_months:2d} months: {fallback_rate:.1%} sales retention (fallback - no data)\")\n",
    "            \n",
    "            all_channel_rates[channel] = channel_rates\n",
    "        \n",
    "        # Calculate blended retention rates across channels\n",
    "        blended_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            rates_for_age = []\n",
    "            for channel_rates in all_channel_rates.values():\n",
    "                if age_months in channel_rates:\n",
    "                    rates_for_age.append(channel_rates[age_months])\n",
    "            \n",
    "            if rates_for_age:\n",
    "                blended_rates[age_months] = np.mean(rates_for_age)\n",
    "            else:\n",
    "                # Fallback that ensures declining pattern\n",
    "                blended_rates[age_months] = max(0.05, 0.75 ** age_months)\n",
    "        \n",
    "        print(f\"\\n📊 Blended sales retention rates from cohort data:\")\n",
    "        for age in [1, 3, 6, 12]:\n",
    "            if age in blended_rates:\n",
    "                print(f\"   {age:2d} months: {blended_rates[age]:.1%} (vs initial spend)\")\n",
    "        \n",
    "        # SANITY CHECK: Ensure rates generally decline\n",
    "        print(f\"\\n🔍 Sanity check - sales retention should generally decline:\")\n",
    "        declining_pattern = True\n",
    "        for age in range(1, 12):\n",
    "            curr_rate = blended_rates.get(age, 0)\n",
    "            next_rate = blended_rates.get(age + 1, 0)\n",
    "            if next_rate > curr_rate * 1.3:  # Allow some variation but flag big increases\n",
    "                print(f\"   ⚠️ WARNING: Month {age} to {age+1} shows unexpected increase: {curr_rate:.1%} → {next_rate:.1%}\")\n",
    "                declining_pattern = False\n",
    "        \n",
    "        if declining_pattern:\n",
    "            print(f\"   ✅ Sales retention pattern looks realistic (generally declining)\")\n",
    "        \n",
    "        return blended_rates\n",
    "    \n",
    "    def calculate_fallback_retention_rates(self):\n",
    "        \"\"\"\n",
    "        Fallback retention calculation (same as original) if no cohort data\n",
    "        \"\"\"\n",
    "        print(\"\\n👥 Calculating fallback retention rates...\")\n",
    "        \n",
    "        # Check if we have customer lifecycle data\n",
    "        has_lifecycle_data = ('pct_new_customers' in self.daily_data.columns and \n",
    "                             not self.daily_data['pct_new_customers'].isna().all())\n",
    "        \n",
    "        if has_lifecycle_data:\n",
    "            avg_new_pct = self.daily_data['pct_new_customers'].mean()\n",
    "            \n",
    "            if avg_new_pct > 0.6:  # High churn environment\n",
    "                base_retention = 0.70\n",
    "                print(\"   High acquisition environment detected - using 70% base retention\")\n",
    "            elif avg_new_pct < 0.3:  # Low churn environment\n",
    "                base_retention = 0.80\n",
    "                print(\"   High retention environment detected - using 80% base retention\")\n",
    "            else:\n",
    "                base_retention = 0.75\n",
    "                print(\"   Balanced environment detected - using 75% base retention\")\n",
    "        else:\n",
    "            base_retention = 0.75\n",
    "            print(\"   Using standard 75% base retention\")\n",
    "        \n",
    "        # Calculate retention for each age month\n",
    "        retention_rates = {}\n",
    "        for age_months in range(1, 13):\n",
    "            retention = base_retention ** age_months\n",
    "            retention = max(0.05, min(0.95, retention))\n",
    "            retention_rates[age_months] = retention\n",
    "        \n",
    "        print(f\"   1-month retention: {retention_rates[1]:.1%}\")\n",
    "        print(f\"   6-month retention: {retention_rates[6]:.1%}\")\n",
    "        print(f\"   12-month retention: {retention_rates[12]:.1%}\")\n",
    "        \n",
    "        return retention_rates\n",
    "    \n",
    "    # Keep all the other methods from the original SimpleRobustBenchmark\n",
    "    def calculate_simple_aov_by_channel(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\n💰 Calculating simple AOV by channel...\")\n",
    "        \n",
    "        channel_aov = {}\n",
    "        \n",
    "        if self.order_items.empty:\n",
    "            print(\"   Using default AOV for all channels\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['sku_gross_sales', 'quantity', 'channel']\n",
    "        missing_cols = [col for col in required_cols if col not in self.order_items.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"   Missing columns: {missing_cols}, using default AOV\")\n",
    "            return {'tiktok': self.aov_avg, 'amazon': self.aov_avg, 'shopify': self.aov_avg}\n",
    "        \n",
    "        for channel in ['tiktok', 'amazon', 'shopify']:\n",
    "            print(f\"   Calculating {channel} AOV...\")\n",
    "            \n",
    "            # Filter channel data\n",
    "            channel_orders = self.order_items[self.order_items['channel'] == channel].copy()\n",
    "            \n",
    "            if len(channel_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Clean data simply\n",
    "            clean_orders = channel_orders[\n",
    "                (channel_orders['quantity'] > 0) & \n",
    "                (channel_orders['sku_gross_sales'] > 0) &\n",
    "                (channel_orders['sku_gross_sales'] < 10000) &  # Remove extreme outliers\n",
    "                (channel_orders['quantity'] < 1000)  # Remove bulk orders\n",
    "            ].copy()\n",
    "            \n",
    "            if len(clean_orders) == 0:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No clean {channel} orders, using default ${self.aov_avg}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate unit prices\n",
    "            clean_orders['unit_price'] = clean_orders['sku_gross_sales'] / clean_orders['quantity']\n",
    "            \n",
    "            # Remove unit price outliers\n",
    "            unit_prices = clean_orders['unit_price']\n",
    "            q1 = unit_prices.quantile(0.25)\n",
    "            q3 = unit_prices.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(1, q1 - 1.5 * iqr)\n",
    "            upper_bound = min(1000, q3 + 1.5 * iqr)\n",
    "            \n",
    "            final_prices = unit_prices[(unit_prices >= lower_bound) & (unit_prices <= upper_bound)]\n",
    "            \n",
    "            if len(final_prices) > 0:\n",
    "                channel_aov[channel] = final_prices.median()\n",
    "                print(f\"     {channel.capitalize()} AOV: ${channel_aov[channel]:.2f} (from {len(final_prices):,} clean orders)\")\n",
    "            else:\n",
    "                channel_aov[channel] = self.aov_avg\n",
    "                print(f\"     No valid {channel} prices, using default ${self.aov_avg}\")\n",
    "        \n",
    "        return channel_aov\n",
    "    \n",
    "    def calculate_simple_seasonal_factors(self):\n",
    "        \"\"\"Same as original\"\"\"\n",
    "        print(\"\\n🗓️ Calculating simple seasonal factors...\")\n",
    "        \n",
    "        # Add temporal columns safely\n",
    "        try:\n",
    "            self.daily_data['month'] = self.daily_data['order_date'].dt.month\n",
    "            self.daily_data['dayofweek'] = self.daily_data['order_date'].dt.dayofweek\n",
    "        except:\n",
    "            print(\"   Error adding temporal columns, using default factors\")\n",
    "            return {\n",
    "                'monthly_factors': {i: 1.0 for i in range(1, 13)},\n",
    "                'dow_factors': {i: 1.0 for i in range(7)}\n",
    "            }\n",
    "        \n",
    "        # Calculate monthly factors simply\n",
    "        monthly_factors = {}\n",
    "        overall_avg = self.daily_data['num_customers'].mean()\n",
    "        \n",
    "        if overall_avg > 0:\n",
    "            for month in range(1, 13):\n",
    "                month_data = self.daily_data[self.daily_data['month'] == month]\n",
    "                if len(month_data) > 0:\n",
    "                    month_avg = month_data['num_customers'].mean()\n",
    "                    factor = month_avg / overall_avg\n",
    "                    monthly_factors[month] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    monthly_factors[month] = 1.0\n",
    "        else:\n",
    "            monthly_factors = {i: 1.0 for i in range(1, 13)}\n",
    "        \n",
    "        # Calculate day-of-week factors simply\n",
    "        dow_factors = {}\n",
    "        if overall_avg > 0:\n",
    "            for dow in range(7):\n",
    "                dow_data = self.daily_data[self.daily_data['dayofweek'] == dow]\n",
    "                if len(dow_data) > 0:\n",
    "                    dow_avg = dow_data['num_customers'].mean()\n",
    "                    factor = dow_avg / overall_avg\n",
    "                    dow_factors[dow] = max(0.5, min(2.0, factor))  # Reasonable bounds\n",
    "                else:\n",
    "                    dow_factors[dow] = 1.0\n",
    "        else:\n",
    "            dow_factors = {i: 1.0 for i in range(7)}\n",
    "        \n",
    "        print(f\"   Monthly variation: {max(monthly_factors.values()) - min(monthly_factors.values()):.1%}\")\n",
    "        print(f\"   Weekly variation: {max(dow_factors.values()) - min(dow_factors.values()):.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'monthly_factors': monthly_factors,\n",
    "            'dow_factors': dow_factors\n",
    "        }\n",
    "    \n",
    "    def calculate_robust_benchmark(self):\n",
    "        \"\"\"\n",
    "        Enhanced benchmark calculation using real cohort data for retention rates\n",
    "        \"\"\"\n",
    "        print(\"\\n🎯 Calculating enhanced robust benchmark...\")\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"❌ No daily data available!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get patterns (using cohort data for retention rates!)\n",
    "        channel_aov = self.calculate_simple_aov_by_channel()\n",
    "        seasonal_patterns = self.calculate_simple_seasonal_factors()\n",
    "        retention_rates = self.calculate_cohort_returning_rates()  # ✅ Use real cohort data!\n",
    "        \n",
    "        # Start with daily data\n",
    "        benchmark_df = self.daily_data.copy()\n",
    "        \n",
    "        # Basic customer calculations\n",
    "        benchmark_df['new_customers'] = benchmark_df['num_new_customers'].fillna(0)\n",
    "        benchmark_df['existing_customers'] = np.maximum(0, \n",
    "            benchmark_df['num_customers'] - benchmark_df['new_customers'])\n",
    "        benchmark_df['total_customers'] = benchmark_df['num_customers']\n",
    "        \n",
    "        # Apply AOV by channel\n",
    "        print(\"   Applying channel-specific AOV...\")\n",
    "        benchmark_df['aov_used'] = benchmark_df['channel'].map(channel_aov).fillna(self.aov_avg)\n",
    "        \n",
    "        # Apply seasonal adjustments\n",
    "        print(\"   Applying seasonal adjustments...\")\n",
    "        benchmark_df['monthly_factor'] = benchmark_df['month'].map(seasonal_patterns['monthly_factors']).fillna(1.0)\n",
    "        benchmark_df['dow_factor'] = benchmark_df['dayofweek'].map(seasonal_patterns['dow_factors']).fillna(1.0)\n",
    "        benchmark_df['seasonal_adjustment'] = (benchmark_df['monthly_factor'] * 0.7 + \n",
    "                                             benchmark_df['dow_factor'] * 0.3)\n",
    "        \n",
    "        # Calculate demand using YOUR EXACT FORMULA\n",
    "        print(\"   Calculating benchmark demand using your formula...\")\n",
    "        benchmark_df['new_customer_demand'] = benchmark_df['new_customers'] * benchmark_df['aov_used']\n",
    "        benchmark_df['existing_customer_demand'] = benchmark_df['existing_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # ✅ YOUR EXACT FORMULA: BM_Demand = new_customers * AOV + existing_customers * AOV\n",
    "        benchmark_df['bm_demand'] = (benchmark_df['new_customer_demand'] + \n",
    "                                   benchmark_df['existing_customer_demand'])\n",
    "        \n",
    "        # Calculate actual demand\n",
    "        print(\"   Calculating actual demand...\")\n",
    "        if not self.order_items.empty:\n",
    "            try:\n",
    "                # Try to get actual revenue\n",
    "                actual_revenue = self.order_items.groupby(['product_name', 'order_date', 'channel']).agg({\n",
    "                    'sku_gross_sales': 'sum'\n",
    "                }).reset_index()\n",
    "                \n",
    "                benchmark_df = benchmark_df.merge(actual_revenue, \n",
    "                                                left_on=['product_name', 'order_date', 'channel'],\n",
    "                                                right_on=['product_name', 'order_date', 'channel'], \n",
    "                                                how='left')\n",
    "                \n",
    "                benchmark_df['actual_demand'] = benchmark_df['sku_gross_sales'].fillna(\n",
    "                    benchmark_df['total_customers'] * benchmark_df['aov_used'])\n",
    "            except:\n",
    "                print(\"     Revenue calculation failed, using customer-based estimate\")\n",
    "                benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        else:\n",
    "            benchmark_df['actual_demand'] = benchmark_df['total_customers'] * benchmark_df['aov_used']\n",
    "        \n",
    "        # ✅ YOUR EXACT ERROR FORMULA: Actual_Demand - BM_Demand\n",
    "        benchmark_df['error_metric'] = benchmark_df['actual_demand'] - benchmark_df['bm_demand']\n",
    "        benchmark_df['error_percentage'] = np.where(\n",
    "            benchmark_df['actual_demand'] > 0,\n",
    "            (benchmark_df['error_metric'] / benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Add real cohort-based retention rates\n",
    "        print(\"   Adding real cohort-based retention rates...\")\n",
    "        for age_months in range(1, 13):\n",
    "            real_retention = retention_rates[age_months]\n",
    "            # Apply seasonal adjustment\n",
    "            seasonal_retention = real_retention * benchmark_df['seasonal_adjustment']\n",
    "            seasonal_retention = np.clip(seasonal_retention, 0.05, 0.95)\n",
    "            \n",
    "            benchmark_df[f'returning_rate_{age_months}m'] = seasonal_retention\n",
    "            benchmark_df[f'falloff_rate_{age_months}m'] = 1 - seasonal_retention\n",
    "        \n",
    "        # Select final columns\n",
    "        final_columns = [\n",
    "            'product_name', 'order_date', 'channel',\n",
    "            'new_customers', 'existing_customers', 'total_customers',\n",
    "            'aov_used', 'new_customer_demand', 'existing_customer_demand',\n",
    "            'bm_demand', 'actual_demand', 'error_metric', 'error_percentage'\n",
    "        ] + [f'returning_rate_{i}m' for i in range(1, 13)] + [f'falloff_rate_{i}m' for i in range(1, 13)]\n",
    "        \n",
    "        # Only keep columns that exist\n",
    "        existing_columns = [col for col in final_columns if col in benchmark_df.columns]\n",
    "        benchmark_df = benchmark_df[existing_columns].copy()\n",
    "        benchmark_df.rename(columns={'product_name': 'sku'}, inplace=True)\n",
    "        \n",
    "        return benchmark_df\n",
    "    \n",
    "    def run_enhanced_benchmark(self):\n",
    "        \"\"\"\n",
    "        Run the enhanced benchmark with real cohort data\n",
    "        \"\"\"\n",
    "        print(\"🚀 RUNNING ENHANCED ROBUST BENCHMARK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.daily_data.empty:\n",
    "            print(\"❌ No data to process!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        benchmark_df = self.calculate_robust_benchmark()\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if benchmark_df.empty:\n",
    "            print(\"❌ No benchmark results generated!\")\n",
    "            return benchmark_df\n",
    "        \n",
    "        print(f\"\\n📊 ENHANCED BENCHMARK RESULTS:\")\n",
    "        print(f\"  ⚡ Processed {len(benchmark_df):,} records in {duration:.1f} seconds\")\n",
    "        print(f\"  🛍️ SKUs: {benchmark_df['sku'].nunique()}\")\n",
    "        print(f\"  📅 Date range: {benchmark_df['order_date'].min()} to {benchmark_df['order_date'].max()}\")\n",
    "        \n",
    "        # Performance by channel\n",
    "        if 'channel' in benchmark_df.columns:\n",
    "            print(f\"\\n📺 PERFORMANCE BY CHANNEL:\")\n",
    "            for channel in benchmark_df['channel'].unique():\n",
    "                channel_data = benchmark_df[benchmark_df['channel'] == channel]\n",
    "                avg_aov = channel_data['aov_used'].mean()\n",
    "                mape = channel_data['error_percentage'].abs().mean()\n",
    "                records = len(channel_data)\n",
    "                \n",
    "                print(f\"  {channel.capitalize():<8}: {records:,} records, AOV ${avg_aov:.2f}, MAPE {mape:.1f}%\")\n",
    "        \n",
    "        # Overall performance\n",
    "        overall_mape = benchmark_df['error_percentage'].abs().mean()\n",
    "        overall_aov = benchmark_df['aov_used'].mean()\n",
    "        \n",
    "        print(f\"\\n🎯 OVERALL PERFORMANCE:\")\n",
    "        print(f\"  📊 Total MAPE: {overall_mape:.1f}%\")\n",
    "        print(f\"  💰 Overall Avg AOV: ${overall_aov:.2f}\")\n",
    "        print(f\"  📈 Avg BM Demand: ${benchmark_df['bm_demand'].mean():.2f}\")\n",
    "        print(f\"  📈 Avg Actual Demand: ${benchmark_df['actual_demand'].mean():.2f}\")\n",
    "        print(f\"  ✅ Using REAL cohort data for retention rates!\")\n",
    "        \n",
    "        return benchmark_df\n",
    "\n",
    "def run_enhanced_robust_benchmark(tiktok_daily_sku_metrics, amazon_daily_sku_metrics, \n",
    "                                shopify_daily_sku_metrics, amazon_order_item_metrics, \n",
    "                                tiktok__order_items, shopify__order_items,\n",
    "                                final_amazon_cohorts=None, final_tiktok_cohorts=None, \n",
    "                                final_shopify_cohorts=None, aov_avg=43):\n",
    "    \"\"\"\n",
    "    Enhanced robust benchmark model with real cohort data integration\n",
    "    \"\"\"\n",
    "    print(\"🛡️ ENHANCED ROBUST BENCHMARK MODEL\")\n",
    "    print(\"🔧 Features:\")\n",
    "    print(\"   ✅ Safe data handling\")\n",
    "    print(\"   ✅ Simple operations (no complex groupby)\")\n",
    "    print(\"   ✅ Multiple fallbacks\")\n",
    "    print(\"   ✅ Channel-specific AOV\")\n",
    "    print(\"   ✅ Seasonal adjustments\")\n",
    "    print(\"   ✅ REAL cohort-based retention rates\")\n",
    "    print(\"   ✅ Your exact formulas: BM_Demand = new_customers * AOV + existing_customers * AOV\")\n",
    "    print(\"   ✅ Your exact error: Actual_Demand - BM_Demand\")\n",
    "    \n",
    "    benchmark = EnhancedRobustBenchmark(\n",
    "        tiktok_daily_sku_metrics=tiktok_daily_sku_metrics,\n",
    "        amazon_daily_sku_metrics=amazon_daily_sku_metrics,\n",
    "        shopify_daily_sku_metrics=shopify_daily_sku_metrics,\n",
    "        amazon_order_item_metrics=amazon_order_item_metrics,\n",
    "        tiktok__order_items=tiktok__order_items,\n",
    "        shopify__order_items=shopify__order_items,\n",
    "        final_amazon_cohorts=final_amazon_cohorts,\n",
    "        final_tiktok_cohorts=final_tiktok_cohorts,\n",
    "        final_shopify_cohorts=final_shopify_cohorts,\n",
    "        aov_avg=aov_avg\n",
    "    )\n",
    "    \n",
    "    benchmark_df = benchmark.run_enhanced_benchmark()\n",
    "    \n",
    "    return benchmark, benchmark_df\n",
    "\n",
    "# USAGE:\n",
    "enhanced_model, enhanced_benchmark_df = run_enhanced_robust_benchmark(\n",
    "    tiktok_daily_sku_metrics, amazon_daily_sku_metrics, shopify_daily_sku_metrics,\n",
    "    amazon_order_item_metrics, tiktok__order_items, shopify__order_items,\n",
    "    final_amazon_cohorts, final_tiktok_cohorts, final_shopify_cohorts, aov_avg=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4132c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 COMPREHENSIVE MODEL EVALUATION FRAMEWORK\n",
      "============================================================\n",
      "\n",
      "🔧 Preparing evaluation data...\n",
      "   ✅ Clean records: 97,190\n",
      "   📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "   🛍️ Unique SKUs: 2262\n",
      "   📺 Channels: ['tiktok' 'amazon' 'shopify']\n",
      "✅ Loaded benchmark data: 97,190 records\n",
      "ℹ️ No baseline model provided - will use naive benchmarks\n",
      "🚀 RUNNING COMPREHENSIVE MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "📈 OVERALL ACCURACY METRICS\n",
      "----------------------------------------\n",
      "📊 ACCURACY SUMMARY:\n",
      "   Mean Absolute Error (MAE):     $967.73\n",
      "   Mean Absolute % Error (MAPE):  54.3%\n",
      "   Root Mean Square Error (RMSE): $3,310.72\n",
      "\n",
      "📈 BIAS ANALYSIS:\n",
      "   Overall Bias:                  -8.1% (Under-prediction)\n",
      "   Total Actual Demand:           $262,641,025.18\n",
      "   Total Predicted Demand:        $241,250,048.15\n",
      "\n",
      "🎯 PREDICTION ACCURACY:\n",
      "   Within 10%:  18.9% of predictions\n",
      "   Within 20%:  31.7% of predictions\n",
      "   Within 50%:  65.0% of predictions\n",
      "\n",
      "📊 PREDICTION DIRECTION:\n",
      "   Over-predictions:  33,332 (34.3%)\n",
      "   Under-predictions: 61,153 (62.9%)\n",
      "   Perfect predictions: 2,705 (2.8%)\n",
      "\n",
      "💰 WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\n",
      "--------------------------------------------------\n",
      "💎 WEIGHTED PERFORMANCE:\n",
      "   Revenue-Weighted MAPE:    43.9%\n",
      "   Revenue-Weighted MAE:     $3,952.97\n",
      "\n",
      "🏆 TOP 10 PRODUCTS (by revenue):\n",
      "   MAPE:                     28.1%\n",
      "   Revenue Share:            48.4%\n",
      "\n",
      "📉 BOTTOM 50% PRODUCTS:\n",
      "   MAPE:                     162.0%\n",
      "   Revenue Share:            0.1%\n",
      "\n",
      "📊 PERFORMANCE SPREAD:\n",
      "   Top vs Bottom Difference: -133.9% MAPE\n",
      "\n",
      "📺 CHANNEL PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "📊 CHANNEL RANKINGS (by MAPE):\n",
      "   Amazon    : 51.1% MAPE, +41.1% bias, 9.7% revenue share\n",
      "   Shopify   : 53.3% MAPE, -13.1% bias, 87.1% revenue share\n",
      "   Tiktok    : 75.2% MAPE, -24.1% bias, 3.2% revenue share\n",
      "\n",
      "🏆 CHANNEL INSIGHTS:\n",
      "   Best Performing:  Amazon (51.1% MAPE)\n",
      "   Worst Performing: Tiktok (75.2% MAPE)\n",
      "   Performance Gap:  24.2% MAPE difference\n",
      "\n",
      "🛍️ SKU PERFORMANCE ANALYSIS\n",
      "-----------------------------------\n",
      "🎯 TOP 10 MOST ACCURATE SKUs (among significant SKUs):\n",
      "   Coffee Concentrate -...  : 0.0% MAPE, $998 revenue\n",
      "   Concentrates - Live ...  : 0.0% MAPE, $600 revenue\n",
      "   Javy Concentrate Bun...  : 0.0% MAPE, $420 revenue\n",
      "   Limited Deal -> Javy...  : 0.0% MAPE, $978 revenue\n",
      "   Limited Edition Fall...  : 0.0% MAPE, $978 revenue\n",
      "   Limited Sale - Javy ...  : 0.0% MAPE, $379 revenue\n",
      "   Limited Sale - Javy ...  : 0.0% MAPE, $2,055 revenue\n",
      "   Limited Edition Tumb...  : 0.4% MAPE, $7,405 revenue\n",
      "   Coffee Concentrate -...  : 0.4% MAPE, $1,476 revenue\n",
      "   Fanny Pack               : 0.6% MAPE, $33,352 revenue\n",
      "\n",
      "❌ BOTTOM 10 LEAST ACCURATE SKUs (among significant SKUs):\n",
      "   Free Milk Frother        : 3184.4% MAPE, $28,309 revenue\n",
      "   Tip                      : 2523.8% MAPE, $455 revenue\n",
      "   Free Mystery Gift        : 1880.3% MAPE, $25,625 revenue\n",
      "   Shipping Insurance -...  : 565.2% MAPE, $1,396 revenue\n",
      "   Javy Coffee - Variet...  : 391.8% MAPE, $29,034 revenue\n",
      "   Caramel Coffee Syrup     : 278.9% MAPE, $108,506 revenue\n",
      "   Javy Premium Metal S...  : 251.0% MAPE, $3,104 revenue\n",
      "   Magic Measuring Spoo...  : 234.4% MAPE, $4,759,509 revenue\n",
      "   Insured Shipping         : 233.1% MAPE, $4,103,357 revenue\n",
      "   Javy Travel Coffee M...  : 221.5% MAPE, $936 revenue\n",
      "\n",
      "📊 SKU PERFORMANCE SUMMARY:\n",
      "   Average MAPE (all SKUs):        147.4%\n",
      "   Average MAPE (significant SKUs): 61.4%\n",
      "   SKUs analyzed:                  2,262\n",
      "   Significant SKUs:               1,131\n",
      "\n",
      "📅 TEMPORAL PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "📈 TEMPORAL TRENDS:\n",
      "   MAPE Trend Correlation:  -0.182 (Improving over time)\n",
      "   MAPE Standard Deviation: 96.0%\n",
      "   MAPE Consistency (CV):   1.39 (Variable)\n",
      "\n",
      "🏆 BEST MONTH:  2020-11 (22.3% MAPE)\n",
      "❌ WORST MONTH: 2021-11 (723.6% MAPE)\n",
      "\n",
      "📊 PERFORMANCE RANGE: 701.4% MAPE spread\n",
      "\n",
      "🔄 CROSS-VALIDATION ANALYSIS\n",
      "-----------------------------------\n",
      "🎯 CROSS-VALIDATION RESULTS:\n",
      "   Months Tested:           53\n",
      "   Win Rate vs Baseline:    100.0% (53/53 months)\n",
      "   Average Improvement:     +908.3% MAPE vs naive baseline\n",
      "\n",
      "📊 MONTH-BY-MONTH PERFORMANCE:\n",
      "   2024-07: 42.9% vs 1678.0% baseline = +1635.1% ✅ WIN\n",
      "   2024-08: 44.8% vs 1826.9% baseline = +1782.2% ✅ WIN\n",
      "   2024-09: 49.6% vs 1956.9% baseline = +1907.3% ✅ WIN\n",
      "   2024-10: 119.9% vs 1992.7% baseline = +1872.8% ✅ WIN\n",
      "   2024-11: 52.1% vs 1739.4% baseline = +1687.4% ✅ WIN\n",
      "   2024-12: 47.0% vs 2046.6% baseline = +1999.6% ✅ WIN\n",
      "   2025-01: 49.8% vs 2003.2% baseline = +1953.4% ✅ WIN\n",
      "   2025-02: 48.5% vs 2141.4% baseline = +2092.9% ✅ WIN\n",
      "   2025-03: 93.3% vs 2213.7% baseline = +2120.4% ✅ WIN\n",
      "   2025-04: 66.4% vs 2545.6% baseline = +2479.3% ✅ WIN\n",
      "   2025-05: 45.2% vs 2504.2% baseline = +2458.9% ✅ WIN\n",
      "   2025-06: 44.6% vs 2563.8% baseline = +2519.2% ✅ WIN\n",
      "\n",
      "📈 CUMULATIVE ERROR ANALYSIS\n",
      "-----------------------------------\n",
      "💰 CUMULATIVE IMPACT ANALYSIS:\n",
      "\n",
      "📊 CURRENT CUMULATIVE PERFORMANCE:\n",
      "   Total Actual Demand:     $262,641,025\n",
      "   Total Predicted Demand:  $241,250,048\n",
      "   Cumulative Error:        $21,390,977\n",
      "   Cumulative Error %:      +8.1%\n",
      "\n",
      "🔮 ANNUAL PROJECTION (if following model):\n",
      "   Projected Annual Demand: $56,280,220\n",
      "   Projected Annual Error:  $9,687,281\n",
      "   Projected Error Rate:    +17.2%\n",
      "\n",
      "⚠️ RISK ASSESSMENT:\n",
      "   🔴 HIGH RISK - 17.2% projected annual error\n",
      "\n",
      "📅 MONTHLY ERROR PROGRESSION:\n",
      "   2025-01: +6.2% cumulative error\n",
      "   2025-02: +6.6% cumulative error\n",
      "   2025-03: +6.9% cumulative error\n",
      "   2025-04: +7.4% cumulative error\n",
      "   2025-05: +7.8% cumulative error\n",
      "   2025-06: +8.1% cumulative error\n",
      "\n",
      "🎉 EVALUATION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelEvaluationFramework:\n",
    "    def __init__(self, benchmark_df, baseline_model_df=None):\n",
    "        \n",
    "        print(\"📊 COMPREHENSIVE MODEL EVALUATION FRAMEWORK\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.benchmark_df = benchmark_df.copy()\n",
    "        self.baseline_df = baseline_model_df.copy() if baseline_model_df is not None else None\n",
    "        \n",
    "        # Clean and prepare data\n",
    "        self.prepare_evaluation_data()\n",
    "        \n",
    "        print(f\"✅ Loaded benchmark data: {len(self.benchmark_df):,} records\")\n",
    "        if self.baseline_df is not None:\n",
    "            print(f\"✅ Loaded baseline data: {len(self.baseline_df):,} records\")\n",
    "        else:\n",
    "            print(\"ℹ️ No baseline model provided - will use naive benchmarks\")\n",
    "    \n",
    "    def prepare_evaluation_data(self):\n",
    "        \"\"\"\n",
    "        Clean and prepare data for evaluation\n",
    "        \"\"\"\n",
    "        print(\"\\n🔧 Preparing evaluation data...\")\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['order_date', 'sku', 'actual_demand', 'bm_demand', 'error_metric']\n",
    "        missing_cols = [col for col in required_cols if col not in self.benchmark_df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"❌ Missing required columns: {missing_cols}\")\n",
    "            return\n",
    "        \n",
    "        # Clean data\n",
    "        self.benchmark_df = self.benchmark_df.dropna(subset=['actual_demand', 'bm_demand'])\n",
    "        self.benchmark_df = self.benchmark_df[self.benchmark_df['actual_demand'] > 0]  # Remove zero demand\n",
    "        \n",
    "        # Add derived metrics\n",
    "        self.benchmark_df['absolute_error'] = np.abs(self.benchmark_df['error_metric'])\n",
    "        self.benchmark_df['percentage_error'] = np.where(\n",
    "            self.benchmark_df['actual_demand'] > 0,\n",
    "            (self.benchmark_df['error_metric'] / self.benchmark_df['actual_demand']) * 100,\n",
    "            0\n",
    "        )\n",
    "        self.benchmark_df['absolute_percentage_error'] = np.abs(self.benchmark_df['percentage_error'])\n",
    "        \n",
    "        # Add temporal columns\n",
    "        self.benchmark_df['order_date'] = pd.to_datetime(self.benchmark_df['order_date'])\n",
    "        self.benchmark_df['year_month'] = self.benchmark_df['order_date'].dt.to_period('M')\n",
    "        self.benchmark_df['quarter'] = self.benchmark_df['order_date'].dt.to_period('Q')\n",
    "        self.benchmark_df['year'] = self.benchmark_df['order_date'].dt.year\n",
    "        self.benchmark_df['month'] = self.benchmark_df['order_date'].dt.month\n",
    "        \n",
    "        # Calculate relative performance metrics\n",
    "        if 'channel' in self.benchmark_df.columns:\n",
    "            self.benchmark_df['channel'] = self.benchmark_df['channel'].fillna('unknown')\n",
    "        \n",
    "        print(f\"   ✅ Clean records: {len(self.benchmark_df):,}\")\n",
    "        print(f\"   📅 Date range: {self.benchmark_df['order_date'].min()} to {self.benchmark_df['order_date'].max()}\")\n",
    "        print(f\"   🛍️ Unique SKUs: {self.benchmark_df['sku'].nunique()}\")\n",
    "        if 'channel' in self.benchmark_df.columns:\n",
    "            print(f\"   📺 Channels: {self.benchmark_df['channel'].unique()}\")\n",
    "    \n",
    "    def calculate_overall_accuracy(self):\n",
    "        \"\"\"\n",
    "        Calculate overall model accuracy metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n📈 OVERALL ACCURACY METRICS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Basic accuracy metrics\n",
    "        mae = self.benchmark_df['absolute_error'].mean()\n",
    "        mape = self.benchmark_df['absolute_percentage_error'].mean()\n",
    "        rmse = np.sqrt((self.benchmark_df['error_metric'] ** 2).mean())\n",
    "        \n",
    "        # Relative metrics\n",
    "        total_actual = self.benchmark_df['actual_demand'].sum()\n",
    "        total_predicted = self.benchmark_df['bm_demand'].sum()\n",
    "        total_error = self.benchmark_df['error_metric'].sum()\n",
    "        \n",
    "        bias = (total_predicted - total_actual) / total_actual * 100\n",
    "        \n",
    "        # Accuracy scores\n",
    "        accuracy_within_10pct = (self.benchmark_df['absolute_percentage_error'] <= 10).mean() * 100\n",
    "        accuracy_within_20pct = (self.benchmark_df['absolute_percentage_error'] <= 20).mean() * 100\n",
    "        accuracy_within_50pct = (self.benchmark_df['absolute_percentage_error'] <= 50).mean() * 100\n",
    "        \n",
    "        # Directional accuracy\n",
    "        over_predictions = (self.benchmark_df['error_metric'] < 0).sum()\n",
    "        under_predictions = (self.benchmark_df['error_metric'] > 0).sum()\n",
    "        perfect_predictions = (self.benchmark_df['error_metric'] == 0).sum()\n",
    "        \n",
    "        print(f\"📊 ACCURACY SUMMARY:\")\n",
    "        print(f\"   Mean Absolute Error (MAE):     ${mae:,.2f}\")\n",
    "        print(f\"   Mean Absolute % Error (MAPE):  {mape:.1f}%\")\n",
    "        print(f\"   Root Mean Square Error (RMSE): ${rmse:,.2f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"📈 BIAS ANALYSIS:\")\n",
    "        print(f\"   Overall Bias:                  {bias:+.1f}% ({'Over' if bias > 0 else 'Under'}-prediction)\")\n",
    "        print(f\"   Total Actual Demand:           ${total_actual:,.2f}\")\n",
    "        print(f\"   Total Predicted Demand:        ${total_predicted:,.2f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"🎯 PREDICTION ACCURACY:\")\n",
    "        print(f\"   Within 10%:  {accuracy_within_10pct:.1f}% of predictions\")\n",
    "        print(f\"   Within 20%:  {accuracy_within_20pct:.1f}% of predictions\")\n",
    "        print(f\"   Within 50%:  {accuracy_within_50pct:.1f}% of predictions\")\n",
    "        print(f\"\")\n",
    "        print(f\"📊 PREDICTION DIRECTION:\")\n",
    "        print(f\"   Over-predictions:  {over_predictions:,} ({over_predictions/len(self.benchmark_df)*100:.1f}%)\")\n",
    "        print(f\"   Under-predictions: {under_predictions:,} ({under_predictions/len(self.benchmark_df)*100:.1f}%)\")\n",
    "        print(f\"   Perfect predictions: {perfect_predictions:,} ({perfect_predictions/len(self.benchmark_df)*100:.1f}%)\")\\\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'rmse': rmse,\n",
    "            'bias': bias,\n",
    "            'accuracy_10pct': accuracy_within_10pct,\n",
    "            'accuracy_20pct': accuracy_within_20pct,\n",
    "            'accuracy_50pct': accuracy_within_50pct\n",
    "        }\n",
    "    \n",
    "    def calculate_weighted_accuracy(self):\n",
    "        \"\"\"\n",
    "        Calculate accuracy weighted by product importance (revenue/volume)\n",
    "        \"\"\"\n",
    "        print(f\"\\n💰 WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate product weights by total revenue\n",
    "        product_revenue = self.benchmark_df.groupby('sku')['actual_demand'].sum().sort_values(ascending=False)\n",
    "        total_revenue = product_revenue.sum()\n",
    "        product_weights = product_revenue / total_revenue\n",
    "        \n",
    "        # Calculate weighted errors\n",
    "        sku_errors = self.benchmark_df.groupby('sku').agg({\n",
    "            'absolute_percentage_error': 'mean',\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        sku_errors['weight'] = sku_errors['sku'].map(product_weights)\n",
    "        sku_errors = sku_errors.dropna()\n",
    "        \n",
    "        # Weighted metrics\n",
    "        weighted_mape = (sku_errors['absolute_percentage_error'] * sku_errors['weight']).sum()\n",
    "        weighted_mae = (sku_errors['absolute_error'] * sku_errors['weight']).sum()\n",
    "        \n",
    "        # Top product analysis\n",
    "        top_10_skus = product_revenue.head(10)\n",
    "        top_10_performance = self.benchmark_df[self.benchmark_df['sku'].isin(top_10_skus.index)]\n",
    "        top_10_mape = top_10_performance['absolute_percentage_error'].mean()\n",
    "        \n",
    "        bottom_skus = product_revenue.tail(len(product_revenue)//2)  # Bottom 50%\n",
    "        bottom_performance = self.benchmark_df[self.benchmark_df['sku'].isin(bottom_skus.index)]\n",
    "        bottom_mape = bottom_performance['absolute_percentage_error'].mean()\n",
    "        \n",
    "        print(f\"💎 WEIGHTED PERFORMANCE:\")\n",
    "        print(f\"   Revenue-Weighted MAPE:    {weighted_mape:.1f}%\")\n",
    "        print(f\"   Revenue-Weighted MAE:     ${weighted_mae:,.2f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"🏆 TOP 10 PRODUCTS (by revenue):\")\n",
    "        print(f\"   MAPE:                     {top_10_mape:.1f}%\")\n",
    "        print(f\"   Revenue Share:            {top_10_skus.sum()/total_revenue*100:.1f}%\")\n",
    "        print(f\"\")\n",
    "        print(f\"📉 BOTTOM 50% PRODUCTS:\")\n",
    "        print(f\"   MAPE:                     {bottom_mape:.1f}%\")\n",
    "        print(f\"   Revenue Share:            {bottom_skus.sum()/total_revenue*100:.1f}%\")\n",
    "        print(f\"\")\n",
    "        print(f\"📊 PERFORMANCE SPREAD:\")\n",
    "        print(f\"   Top vs Bottom Difference: {top_10_mape - bottom_mape:+.1f}% MAPE\")\\\n",
    "        \n",
    "        return {\n",
    "            'weighted_mape': weighted_mape,\n",
    "            'weighted_mae': weighted_mae,\n",
    "            'top_10_mape': top_10_mape,\n",
    "            'bottom_50_mape': bottom_mape,\n",
    "            'top_products': top_10_skus\n",
    "        }\n",
    "    \n",
    "    def analyze_channel_performance(self):\n",
    "        \"\"\"\n",
    "        Analyze model performance across different channels\n",
    "        \"\"\"\n",
    "        print(f\"\\n📺 CHANNEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'channel' not in self.benchmark_df.columns:\n",
    "            print(\"   ❌ No channel data available\")\n",
    "            return {}\n",
    "        \n",
    "        channel_performance = self.benchmark_df.groupby('channel').agg({\n",
    "            'absolute_percentage_error': ['mean', 'std', 'count'],\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': 'sum',\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        channel_performance.columns = ['mape', 'mape_std', 'record_count', 'mae', 'actual_total', 'predicted_total', 'total_error']\n",
    "        channel_performance = channel_performance.reset_index()\n",
    "        \n",
    "        # Calculate channel bias\n",
    "        channel_performance['bias_pct'] = (\n",
    "            (channel_performance['predicted_total'] - channel_performance['actual_total']) / \n",
    "            channel_performance['actual_total'] * 100\n",
    "        )\n",
    "        \n",
    "        # Revenue share\n",
    "        total_revenue = channel_performance['actual_total'].sum()\n",
    "        channel_performance['revenue_share'] = channel_performance['actual_total'] / total_revenue * 100\n",
    "        \n",
    "        # Sort by performance\n",
    "        channel_performance = channel_performance.sort_values('mape')\n",
    "        \n",
    "        print(\"📊 CHANNEL RANKINGS (by MAPE):\")\n",
    "        for _, row in channel_performance.iterrows():\n",
    "            print(f\"   {row['channel'].capitalize():<10}: {row['mape']:.1f}% MAPE, {row['bias_pct']:+.1f}% bias, {row['revenue_share']:.1f}% revenue share\")\n",
    "        \n",
    "        best_channel = channel_performance.iloc[0]['channel']\n",
    "        worst_channel = channel_performance.iloc[-1]['channel']\n",
    "        performance_gap = channel_performance.iloc[-1]['mape'] - channel_performance.iloc[0]['mape']\n",
    "        \n",
    "        print(f\"\")\n",
    "        print(f\"🏆 CHANNEL INSIGHTS:\")\n",
    "        print(f\"   Best Performing:  {best_channel.capitalize()} ({channel_performance.iloc[0]['mape']:.1f}% MAPE)\")\n",
    "        print(f\"   Worst Performing: {worst_channel.capitalize()} ({channel_performance.iloc[-1]['mape']:.1f}% MAPE)\")\n",
    "        print(f\"   Performance Gap:  {performance_gap:.1f}% MAPE difference\")\\\n",
    "        \n",
    "        return channel_performance.to_dict('records')\n",
    "    \n",
    "    def analyze_sku_performance(self):\n",
    "        \"\"\"\n",
    "        Analyze model performance for top SKUs\n",
    "        \"\"\"\n",
    "        print(f\"\\n🛍️ SKU PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Calculate SKU-level performance\n",
    "        sku_performance = self.benchmark_df.groupby('sku').agg({\n",
    "            'absolute_percentage_error': 'mean',\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': ['sum', 'count'],\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        sku_performance.columns = ['mape', 'mae', 'total_revenue', 'record_count', 'predicted_total', 'total_error']\n",
    "        sku_performance = sku_performance.reset_index()\n",
    "        \n",
    "        # Calculate bias\n",
    "        sku_performance['bias_pct'] = (\n",
    "            (sku_performance['predicted_total'] - sku_performance['total_revenue']) / \n",
    "            sku_performance['total_revenue'] * 100\n",
    "        )\n",
    "        \n",
    "        # Filter to SKUs with significant volume (top 50% by revenue)\n",
    "        revenue_threshold = sku_performance['total_revenue'].quantile(0.5)\n",
    "        significant_skus = sku_performance[sku_performance['total_revenue'] >= revenue_threshold]\n",
    "        \n",
    "        # Top and bottom performers\n",
    "        top_10_accurate = significant_skus.nsmallest(10, 'mape')\n",
    "        bottom_10_accurate = significant_skus.nlargest(10, 'mape')\n",
    "        \n",
    "        print(\"🎯 TOP 10 MOST ACCURATE SKUs (among significant SKUs):\")\n",
    "        for _, row in top_10_accurate.iterrows():\n",
    "            sku_short = row['sku'][:20] + \"...\" if len(row['sku']) > 20 else row['sku']\n",
    "            print(f\"   {sku_short:<25}: {row['mape']:.1f}% MAPE, ${row['total_revenue']:,.0f} revenue\")\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"❌ BOTTOM 10 LEAST ACCURATE SKUs (among significant SKUs):\")\n",
    "        for _, row in bottom_10_accurate.iterrows():\n",
    "            sku_short = row['sku'][:20] + \"...\" if len(row['sku']) > 20 else row['sku']\n",
    "            print(f\"   {sku_short:<25}: {row['mape']:.1f}% MAPE, ${row['total_revenue']:,.0f} revenue\")\n",
    "        \n",
    "        # Summary stats\n",
    "        avg_mape_all = sku_performance['mape'].mean()\n",
    "        avg_mape_significant = significant_skus['mape'].mean()\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"📊 SKU PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   Average MAPE (all SKUs):        {avg_mape_all:.1f}%\")\n",
    "        print(f\"   Average MAPE (significant SKUs): {avg_mape_significant:.1f}%\")\n",
    "        print(f\"   SKUs analyzed:                  {len(sku_performance):,}\")\n",
    "        print(f\"   Significant SKUs:               {len(significant_skus):,}\")\\\n",
    "        \n",
    "        return {\n",
    "            'top_accurate': top_10_accurate.to_dict('records'),\n",
    "            'bottom_accurate': bottom_10_accurate.to_dict('records'),\n",
    "            'avg_mape_all': avg_mape_all,\n",
    "            'avg_mape_significant': avg_mape_significant\n",
    "        }\n",
    "    \n",
    "    def analyze_temporal_performance(self):\n",
    "        \"\"\"\n",
    "        Analyze model performance over time\n",
    "        \"\"\"\n",
    "        print(f\"\\n📅 TEMPORAL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Monthly performance\n",
    "        monthly_performance = self.benchmark_df.groupby('year_month').agg({\n",
    "            'absolute_percentage_error': 'mean',\n",
    "            'absolute_error': 'mean',\n",
    "            'actual_demand': 'sum',\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        monthly_performance['bias_pct'] = (\n",
    "            (monthly_performance['bm_demand'] - monthly_performance['actual_demand']) / \n",
    "            monthly_performance['actual_demand'] * 100\n",
    "        )\n",
    "        \n",
    "        # Trend analysis\n",
    "        monthly_performance['month_num'] = range(len(monthly_performance))\n",
    "        mape_trend = np.corrcoef(monthly_performance['month_num'], monthly_performance['absolute_percentage_error'])[0,1]\n",
    "        \n",
    "        # Best and worst months\n",
    "        best_month = monthly_performance.loc[monthly_performance['absolute_percentage_error'].idxmin()]\n",
    "        worst_month = monthly_performance.loc[monthly_performance['absolute_percentage_error'].idxmax()]\n",
    "        \n",
    "        # Consistency metrics\n",
    "        mape_std = monthly_performance['absolute_percentage_error'].std()\n",
    "        mape_cv = mape_std / monthly_performance['absolute_percentage_error'].mean()  # Coefficient of variation\n",
    "        \n",
    "        print(\"📈 TEMPORAL TRENDS:\")\n",
    "        print(f\"   MAPE Trend Correlation:  {mape_trend:+.3f} ({'Improving' if mape_trend < 0 else 'Deteriorating'} over time)\")\n",
    "        print(f\"   MAPE Standard Deviation: {mape_std:.1f}%\")\n",
    "        print(f\"   MAPE Consistency (CV):   {mape_cv:.2f} ({'Consistent' if mape_cv < 0.3 else 'Variable'})\")\n",
    "        print(\"\")\n",
    "        print(f\"🏆 BEST MONTH:  {best_month['year_month']} ({best_month['absolute_percentage_error']:.1f}% MAPE)\")\n",
    "        print(f\"❌ WORST MONTH: {worst_month['year_month']} ({worst_month['absolute_percentage_error']:.1f}% MAPE)\")\n",
    "        print(\"\")\n",
    "        print(f\"📊 PERFORMANCE RANGE: {worst_month['absolute_percentage_error'] - best_month['absolute_percentage_error']:.1f}% MAPE spread\")\\\n",
    "        \n",
    "        return {\n",
    "            'monthly_performance': monthly_performance,\n",
    "            'mape_trend': mape_trend,\n",
    "            'best_month': best_month,\n",
    "            'worst_month': worst_month,\n",
    "            'consistency': mape_cv\n",
    "        }\n",
    "    \n",
    "    def cross_validation_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform time-series cross validation analysis\n",
    "        \"\"\"\n",
    "        print(f\"\\n🔄 CROSS-VALIDATION ANALYSIS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Sort by date for time series CV\n",
    "        df_sorted = self.benchmark_df.sort_values('order_date')\n",
    "        \n",
    "        # Define train/test splits (expanding window)\n",
    "        months = sorted(df_sorted['year_month'].unique())\n",
    "        \n",
    "        if len(months) < 6:\n",
    "            print(\"   ❌ Need at least 6 months of data for cross-validation\")\n",
    "            return {}\n",
    "        \n",
    "        cv_results = []\n",
    "        min_train_months = 3\n",
    "        \n",
    "        for i in range(min_train_months, len(months)):\n",
    "            train_months = months[:i]\n",
    "            test_month = months[i]\n",
    "            \n",
    "            train_data = df_sorted[df_sorted['year_month'].isin(train_months)]\n",
    "            test_data = df_sorted[df_sorted['year_month'] == test_month]\n",
    "            \n",
    "            if len(test_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate baseline (simple average from training data)\n",
    "            baseline_avg = train_data['actual_demand'].mean()\n",
    "            \n",
    "            # Performance metrics for test month\n",
    "            test_mape = test_data['absolute_percentage_error'].mean()\n",
    "            test_mae = test_data['absolute_error'].mean()\n",
    "            \n",
    "            # Baseline comparison (naive forecast = training average)\n",
    "            baseline_errors = np.abs(test_data['actual_demand'] - baseline_avg)\n",
    "            baseline_mape = (baseline_errors / test_data['actual_demand']).mean() * 100\n",
    "            \n",
    "            model_beats_baseline = test_mape < baseline_mape\n",
    "            \n",
    "            cv_results.append({\n",
    "                'test_month': test_month,\n",
    "                'train_months': len(train_months),\n",
    "                'test_records': len(test_data),\n",
    "                'model_mape': test_mape,\n",
    "                'baseline_mape': baseline_mape,\n",
    "                'improvement': baseline_mape - test_mape,\n",
    "                'beats_baseline': model_beats_baseline\n",
    "            })\n",
    "        \n",
    "        cv_df = pd.DataFrame(cv_results)\n",
    "        \n",
    "        # Summary statistics\n",
    "        avg_improvement = cv_df['improvement'].mean()\n",
    "        win_rate = cv_df['beats_baseline'].mean() * 100\n",
    "        consistent_months = cv_df['beats_baseline'].sum()\n",
    "        total_months = len(cv_df)\n",
    "        \n",
    "        print(\"🎯 CROSS-VALIDATION RESULTS:\")\n",
    "        print(f\"   Months Tested:           {total_months}\")\n",
    "        print(f\"   Win Rate vs Baseline:    {win_rate:.1f}% ({consistent_months}/{total_months} months)\")\n",
    "        print(f\"   Average Improvement:     {avg_improvement:+.1f}% MAPE vs naive baseline\")\n",
    "        print(\"\")\n",
    "        print(\"📊 MONTH-BY-MONTH PERFORMANCE:\")\n",
    "        \n",
    "        for _, row in cv_df.tail(12).iterrows():  # Show last 12 months\n",
    "            status = \"✅ WIN\" if row['beats_baseline'] else \"❌ LOSS\"\n",
    "            print(f\"   {row['test_month']}: {row['model_mape']:.1f}% vs {row['baseline_mape']:.1f}% baseline = {row['improvement']:+.1f}% {status}\")\\\n",
    "        \n",
    "        return cv_df\n",
    "    \n",
    "    def cumulative_error_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze cumulative error impact over time\n",
    "        \"\"\"\n",
    "        print(f\"\\n📈 CUMULATIVE ERROR ANALYSIS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Monthly aggregated data\n",
    "        monthly_data = self.benchmark_df.groupby('year_month').agg({\n",
    "            'actual_demand': 'sum',\n",
    "            'bm_demand': 'sum',\n",
    "            'error_metric': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Calculate cumulative metrics\n",
    "        monthly_data['cumulative_actual'] = monthly_data['actual_demand'].cumsum()\n",
    "        monthly_data['cumulative_predicted'] = monthly_data['bm_demand'].cumsum()\n",
    "        monthly_data['cumulative_error'] = monthly_data['error_metric'].cumsum()\n",
    "        monthly_data['cumulative_error_pct'] = (monthly_data['cumulative_error'] / monthly_data['cumulative_actual']) * 100\n",
    "        \n",
    "        # If following model for full year projection\n",
    "        last_month_error_rate = monthly_data['error_metric'].iloc[-1] / monthly_data['actual_demand'].iloc[-1]\n",
    "        avg_monthly_demand = monthly_data['actual_demand'].mean()\n",
    "        \n",
    "        # Project annual impact\n",
    "        annual_error_projection = last_month_error_rate * avg_monthly_demand * 12\n",
    "        annual_demand_projection = avg_monthly_demand * 12\n",
    "        annual_error_pct = (annual_error_projection / annual_demand_projection) * 100\n",
    "        \n",
    "        # Final cumulative error\n",
    "        final_cumulative_error = monthly_data['cumulative_error_pct'].iloc[-1]\n",
    "        \n",
    "        print(\"💰 CUMULATIVE IMPACT ANALYSIS:\")\n",
    "        print(\"\")\n",
    "        print(\"📊 CURRENT CUMULATIVE PERFORMANCE:\")\n",
    "        print(f\"   Total Actual Demand:     ${monthly_data['cumulative_actual'].iloc[-1]:,.0f}\")\n",
    "        print(f\"   Total Predicted Demand:  ${monthly_data['cumulative_predicted'].iloc[-1]:,.0f}\")\n",
    "        print(f\"   Cumulative Error:        ${monthly_data['cumulative_error'].iloc[-1]:,.0f}\")\n",
    "        print(f\"   Cumulative Error %:      {final_cumulative_error:+.1f}%\")\n",
    "        print(\"\")\n",
    "        print(\"🔮 ANNUAL PROJECTION (if following model):\")\n",
    "        print(f\"   Projected Annual Demand: ${annual_demand_projection:,.0f}\")\n",
    "        print(f\"   Projected Annual Error:  ${annual_error_projection:,.0f}\")\n",
    "        print(f\"   Projected Error Rate:    {annual_error_pct:+.1f}%\")\n",
    "        print(\"\")\n",
    "        print(\"⚠️ RISK ASSESSMENT:\")\n",
    "        \n",
    "        if abs(annual_error_pct) < 5:\n",
    "            risk_level = \"🟢 LOW RISK\"\n",
    "        elif abs(annual_error_pct) < 15:\n",
    "            risk_level = \"🟡 MODERATE RISK\"\n",
    "        else:\n",
    "            risk_level = \"🔴 HIGH RISK\"\n",
    "        \n",
    "        print(f\"   {risk_level} - {abs(annual_error_pct):.1f}% projected annual error\")\n",
    "        \n",
    "        # Show monthly progression\n",
    "        print(\"\")\n",
    "        print(\"📅 MONTHLY ERROR PROGRESSION:\")\n",
    "        for _, row in monthly_data.tail(6).iterrows():\n",
    "            print(f\"   {row['year_month']}: {row['cumulative_error_pct']:+.1f}% cumulative error\")\\\n",
    "        \n",
    "        return {\n",
    "            'monthly_data': monthly_data,\n",
    "            'annual_projection': annual_error_pct,\n",
    "            'cumulative_error': final_cumulative_error,\n",
    "            'risk_level': risk_level\n",
    "        }\n",
    "    \n",
    "    def run_comprehensive_evaluation(self):\n",
    "        \"\"\"\n",
    "        Run all evaluation analyses\n",
    "        \"\"\"\n",
    "        print(\"🚀 RUNNING COMPREHENSIVE MODEL EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            results['overall'] = self.calculate_overall_accuracy()\n",
    "            results['weighted'] = self.calculate_weighted_accuracy()\n",
    "            results['channels'] = self.analyze_channel_performance()\n",
    "            results['skus'] = self.analyze_sku_performance()\n",
    "            results['temporal'] = self.analyze_temporal_performance()\n",
    "            results['cross_validation'] = self.cross_validation_analysis()\n",
    "            results['cumulative'] = self.cumulative_error_analysis()\n",
    "            \n",
    "            print(f\"\\n🎉 EVALUATION COMPLETE!\")\n",
    "            print(f\"=\"*60)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during evaluation: {str(e)}\")\n",
    "            return results\n",
    "\n",
    "# Usage function\n",
    "def evaluate_model_performance(benchmark_df, baseline_df=None):\n",
    "    \"\"\"\n",
    "    Main function to evaluate model performance comprehensively\n",
    "    \n",
    "    Args:\n",
    "        benchmark_df: Your enhanced benchmark model results\n",
    "        baseline_df: Optional baseline model for comparison\n",
    "    \n",
    "    Returns:\n",
    "        ModelEvaluationFramework instance with all results\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluator = ModelEvaluationFramework(benchmark_df, baseline_df)\n",
    "    results = evaluator.run_comprehensive_evaluation()\n",
    "    \n",
    "    return evaluator, results\n",
    "\n",
    "# To use:\n",
    "evaluator, evaluation_results = evaluate_model_performance(enhanced_benchmark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2056b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 COMPREHENSIVE ML MODEL PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "📊 Adding ML predictions to test data...\n",
      "   ✅ Added Linear Basic predictions\n",
      "   ✅ Added Linear Enhanced predictions\n",
      "   ✅ Added Ridge Regression predictions\n",
      "   ✅ Added Random Forest predictions\n",
      "   ✅ Added Gradient Boosting predictions\n",
      "\n",
      "🔧 Preparing analysis data...\n",
      "   ✅ Analysis data prepared: 19,438 records\n",
      "   📊 ML models found: 5\n",
      "🚀 RUNNING COMPREHENSIVE ML MODEL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "📈 1. OVERALL ACCURACY ANALYSIS\n",
      "---------------------------------------------\n",
      "🎯 BENCHMARK PERFORMANCE:\n",
      "   MAE:  $1,660.69\n",
      "   MAPE: 61.2%\n",
      "\n",
      "🤖 ML MODEL PERFORMANCE:\n",
      "   Linear Basic:\n",
      "     MAE:  $1,913.00 ($-252.31, -15.2%) ❌ WORSE\n",
      "     MAPE: 492.8% (-431.6%)\n",
      "\n",
      "   Linear Enhanced:\n",
      "     MAE:  $2,016.57 ($-355.88, -21.4%) ❌ WORSE\n",
      "     MAPE: 671.4% (-610.3%)\n",
      "\n",
      "   Ridge Regression:\n",
      "     MAE:  $415.69 ($+1,245.00, +75.0%) ✅ BETTER\n",
      "     MAPE: 228.1% (-166.9%)\n",
      "\n",
      "   Random Forest:\n",
      "     MAE:  $342.94 ($+1,317.75, +79.3%) ✅ BETTER\n",
      "     MAPE: 13.1% (+48.1%)\n",
      "\n",
      "   Gradient Boosting:\n",
      "     MAE:  $302.24 ($+1,358.45, +81.8%) ✅ BETTER\n",
      "     MAPE: 15.0% (+46.2%)\n",
      "\n",
      "\n",
      "💰 2. WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\n",
      "-------------------------------------------------------\n",
      "📊 PRODUCT SEGMENTATION:\n",
      "   Top 20% Products: 245 SKUs, $85,351,390 revenue (99.6%)\n",
      "   Bottom 80% Products: 980 SKUs, $300,907 revenue (0.4%)\n",
      "\n",
      "🎯 TOP 20% PRODUCTS:\n",
      "   Benchmark: MAE $1895.75, MAPE 50.5%\n",
      "   Linear Basic: MAE $2133.05 (-12.5%) ❌\n",
      "   Linear Enhanced: MAE $2157.90 (-13.8%) ❌\n",
      "   Ridge Regression: MAE $455.78 (+76.0%) ✅\n",
      "   Random Forest: MAE $392.93 (+79.3%) ✅\n",
      "   Gradient Boosting: MAE $345.66 (+81.8%) ✅\n",
      "\n",
      "🎯 BOTTOM 80% PRODUCTS:\n",
      "   Benchmark: MAE $76.85, MAPE 132.9%\n",
      "   Linear Basic: MAE $430.26 (-459.8%) ❌\n",
      "   Linear Enhanced: MAE $1064.27 (-1284.8%) ❌\n",
      "   Ridge Regression: MAE $145.58 (-89.4%) ❌\n",
      "   Random Forest: MAE $6.07 (+92.1%) ✅\n",
      "   Gradient Boosting: MAE $9.69 (+87.4%) ✅\n",
      "\n",
      "\n",
      "📺 3. CHANNEL PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "📻 SHOPIFY CHANNEL:\n",
      "   Records: 10,507\n",
      "   Revenue: $73,301,478\n",
      "   Benchmark: MAE $2636.73, MAPE 57.4%\n",
      "   Linear Basic: MAE $2801.43 (-6.2%) ❌ WORSE\n",
      "   Linear Enhanced: MAE $2807.75 (-6.5%) ❌ WORSE\n",
      "   Ridge Regression: MAE $503.27 (+80.9%) ✅ BETTER\n",
      "   Random Forest: MAE $497.61 (+81.1%) ✅ BETTER\n",
      "   Gradient Boosting: MAE $433.01 (+83.6%) ✅ BETTER\n",
      "\n",
      "📻 AMAZON CHANNEL:\n",
      "   Records: 6,142\n",
      "   Revenue: $8,945,566\n",
      "   Benchmark: MAE $516.42, MAPE 55.0%\n",
      "   Linear Basic: MAE $786.12 (-52.2%) ❌ WORSE\n",
      "   Linear Enhanced: MAE $852.79 (-65.1%) ❌ WORSE\n",
      "   Ridge Regression: MAE $275.34 (+46.7%) ✅ BETTER\n",
      "   Random Forest: MAE $135.09 (+73.8%) ✅ BETTER\n",
      "   Gradient Boosting: MAE $127.48 (+75.3%) ✅ BETTER\n",
      "\n",
      "📻 TIKTOK CHANNEL:\n",
      "   Records: 2,789\n",
      "   Revenue: $3,405,254\n",
      "   Benchmark: MAE $503.58, MAPE 88.9%\n",
      "   Linear Basic: MAE $1047.64 (-108.0%) ❌ WORSE\n",
      "   Linear Enhanced: MAE $1598.86 (-217.5%) ❌ WORSE\n",
      "   Ridge Regression: MAE $394.81 (+21.6%) ✅ BETTER\n",
      "   Random Forest: MAE $217.96 (+56.7%) ✅ BETTER\n",
      "   Gradient Boosting: MAE $194.43 (+61.4%) ✅ BETTER\n",
      "\n",
      "\n",
      "📅 4. TEMPORAL PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "📊 LINEAR BASIC MONTHLY PERFORMANCE:\n",
      "   Months tested: 6\n",
      "   Months won: 0/6 (0.0%)\n",
      "   Average improvement: -14.8%\n",
      "   Recent performance:\n",
      "     2025-01: -7.5% ❌ LOSS\n",
      "     2025-02: -11.4% ❌ LOSS\n",
      "     2025-03: -11.4% ❌ LOSS\n",
      "     2025-04: -21.0% ❌ LOSS\n",
      "     2025-05: -15.4% ❌ LOSS\n",
      "     2025-06: -22.1% ❌ LOSS\n",
      "\n",
      "📊 LINEAR ENHANCED MONTHLY PERFORMANCE:\n",
      "   Months tested: 6\n",
      "   Months won: 0/6 (0.0%)\n",
      "   Average improvement: -21.5%\n",
      "   Recent performance:\n",
      "     2025-01: -18.3% ❌ LOSS\n",
      "     2025-02: -22.2% ❌ LOSS\n",
      "     2025-03: -18.6% ❌ LOSS\n",
      "     2025-04: -24.7% ❌ LOSS\n",
      "     2025-05: -19.6% ❌ LOSS\n",
      "     2025-06: -25.3% ❌ LOSS\n",
      "\n",
      "📊 RIDGE REGRESSION MONTHLY PERFORMANCE:\n",
      "   Months tested: 6\n",
      "   Months won: 6/6 (100.0%)\n",
      "   Average improvement: +75.1%\n",
      "   Recent performance:\n",
      "     2025-01: +78.2% ✅ WIN\n",
      "     2025-02: +77.1% ✅ WIN\n",
      "     2025-03: +76.9% ✅ WIN\n",
      "     2025-04: +71.5% ✅ WIN\n",
      "     2025-05: +75.7% ✅ WIN\n",
      "     2025-06: +70.9% ✅ WIN\n",
      "\n",
      "📊 RANDOM FOREST MONTHLY PERFORMANCE:\n",
      "   Months tested: 6\n",
      "   Months won: 6/6 (100.0%)\n",
      "   Average improvement: +79.2%\n",
      "   Recent performance:\n",
      "     2025-01: +84.9% ✅ WIN\n",
      "     2025-02: +87.6% ✅ WIN\n",
      "     2025-03: +85.3% ✅ WIN\n",
      "     2025-04: +75.0% ✅ WIN\n",
      "     2025-05: +75.3% ✅ WIN\n",
      "     2025-06: +67.2% ✅ WIN\n",
      "\n",
      "📊 GRADIENT BOOSTING MONTHLY PERFORMANCE:\n",
      "   Months tested: 6\n",
      "   Months won: 6/6 (100.0%)\n",
      "   Average improvement: +81.8%\n",
      "   Recent performance:\n",
      "     2025-01: +87.9% ✅ WIN\n",
      "     2025-02: +89.0% ✅ WIN\n",
      "     2025-03: +86.7% ✅ WIN\n",
      "     2025-04: +78.2% ✅ WIN\n",
      "     2025-05: +78.0% ✅ WIN\n",
      "     2025-06: +71.2% ✅ WIN\n",
      "\n",
      "\n",
      "💰 5. CUMULATIVE ERROR & ANNUAL RISK ANALYSIS\n",
      "--------------------------------------------------\n",
      "📊 LINEAR BASIC CUMULATIVE ANALYSIS:\n",
      "   Current cumulative error: +16.8%\n",
      "   Benchmark cumulative error: +12.5%\n",
      "   Projected annual error: +20.5%\n",
      "   Benchmark annual error: +17.2%\n",
      "   Risk Level: 🔴 HIGH RISK\n",
      "   Cumulative improvement: -4.3%\n",
      "   Annual improvement: -3.3%\n",
      "\n",
      "📊 LINEAR ENHANCED CUMULATIVE ANALYSIS:\n",
      "   Current cumulative error: +11.5%\n",
      "   Benchmark cumulative error: +12.5%\n",
      "   Projected annual error: +17.3%\n",
      "   Benchmark annual error: +17.2%\n",
      "   Risk Level: 🔴 HIGH RISK\n",
      "   Cumulative improvement: +1.1%\n",
      "   Annual improvement: -0.1%\n",
      "\n",
      "📊 RIDGE REGRESSION CUMULATIVE ANALYSIS:\n",
      "   Current cumulative error: -2.1%\n",
      "   Benchmark cumulative error: +12.5%\n",
      "   Projected annual error: -0.6%\n",
      "   Benchmark annual error: +17.2%\n",
      "   Risk Level: 🟢 LOW RISK\n",
      "   Cumulative improvement: +14.7%\n",
      "   Annual improvement: +17.9%\n",
      "\n",
      "📊 RANDOM FOREST CUMULATIVE ANALYSIS:\n",
      "   Current cumulative error: +4.1%\n",
      "   Benchmark cumulative error: +12.5%\n",
      "   Projected annual error: +7.8%\n",
      "   Benchmark annual error: +17.2%\n",
      "   Risk Level: 🟡 MODERATE RISK\n",
      "   Cumulative improvement: +8.5%\n",
      "   Annual improvement: +9.4%\n",
      "\n",
      "📊 GRADIENT BOOSTING CUMULATIVE ANALYSIS:\n",
      "   Current cumulative error: +3.2%\n",
      "   Benchmark cumulative error: +12.5%\n",
      "   Projected annual error: +7.1%\n",
      "   Benchmark annual error: +17.2%\n",
      "   Risk Level: 🟡 MODERATE RISK\n",
      "   Cumulative improvement: +9.3%\n",
      "   Annual improvement: +10.1%\n",
      "\n",
      "\n",
      "🔍 6. FEATURE IMPORTANCE DEEP DIVE\n",
      "----------------------------------------\n",
      "🤔 WHY THE STEEP FEATURE IMPORTANCE DROP-OFF?\n",
      "\n",
      "The 90.18% importance for 'demand_ma_3d' suggests:\n",
      "   📈 The model is primarily using recent demand history\n",
      "   📉 Other features (seasonality, customer mix, etc.) add little value\n",
      "   🚨 This could indicate:\n",
      "      • Overfitting to short-term patterns\n",
      "      • Data leakage (using future info to predict future)\n",
      "      • High autocorrelation in demand (predictable from recent history)\n",
      "      • Other features aren't properly engineered/scaled\n",
      "\n",
      "🎯 RECOMMENDATIONS:\n",
      "   1. Remove lag features and retrain to see 'pure' feature importance\n",
      "   2. Check for data leakage in lag feature creation\n",
      "   3. Try feature selection to reduce dominance of lag features\n",
      "   4. Consider if this is actually good - maybe demand IS mostly predictable from recent history\n",
      "\n",
      "📊 CORRELATION ANALYSIS:\n",
      "   Linear Basic vs demand_ma_3d: 0.829 correlation\n",
      "   Linear Basic vs actual demand: 0.856 correlation\n",
      "   Linear Basic vs benchmark: 0.981 correlation\n",
      "\n",
      "   Linear Enhanced vs demand_ma_3d: 0.838 correlation\n",
      "   Linear Enhanced vs actual demand: 0.864 correlation\n",
      "   Linear Enhanced vs benchmark: 0.976 correlation\n",
      "\n",
      "\n",
      "🎯 FINAL RECOMMENDATIONS\n",
      "==============================\n",
      "✅ BEST MODEL: Gradient Boosting\n",
      "   Overall improvement: +81.8%\n",
      "   Monthly win rate: 100.0%\n",
      "   🟢 CONSISTENT performer - safe to deploy\n",
      "\n",
      "📋 DEPLOYMENT RECOMMENDATIONS:\n",
      "   🚀 DEPLOY: Strong, consistent improvement\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MLModelEvaluationAnalysis:\n",
    "    def __init__(self, test_data, model_results, benchmark_col='bm_demand', actual_col='actual_demand'):\n",
    "        \n",
    "        print(\"🔍 COMPREHENSIVE ML MODEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.test_data = test_data.copy()\n",
    "        self.model_results = model_results\n",
    "        self.benchmark_col = benchmark_col\n",
    "        self.actual_col = actual_col\n",
    "        \n",
    "        self.add_ml_predictions_to_test_data()\n",
    "        \n",
    "        self.prepare_analysis_data()\n",
    "        \n",
    "    def add_ml_predictions_to_test_data(self):\n",
    "        \n",
    "        print(\"\\n📊 Adding ML predictions to test data...\")\n",
    "        \n",
    "        for model_name, results in self.model_results.items():\n",
    "            if 'predictions' in results:\n",
    "                # Ensure we have the right number of predictions\n",
    "                predictions = results['predictions']\n",
    "                if len(predictions) == len(self.test_data):\n",
    "                    self.test_data[f'{model_name}_pred'] = predictions\n",
    "                    print(f\"   ✅ Added {model_name} predictions\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ {model_name} prediction length mismatch: {len(predictions)} vs {len(self.test_data)}\")\n",
    "    \n",
    "    def prepare_analysis_data(self):\n",
    "        \"\"\"\n",
    "        Prepare data for comprehensive analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n🔧 Preparing analysis data...\")\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        if self.actual_col not in self.test_data.columns:\n",
    "            print(f\"❌ Missing actual demand column: {self.actual_col}\")\n",
    "            return\n",
    "        \n",
    "        if self.benchmark_col not in self.test_data.columns:\n",
    "            print(f\"❌ Missing benchmark column: {self.benchmark_col}\")\n",
    "            return\n",
    "        \n",
    "        # Add temporal columns\n",
    "        if 'order_date' in self.test_data.columns:\n",
    "            self.test_data['order_date'] = pd.to_datetime(self.test_data['order_date'])\n",
    "            self.test_data['year_month'] = self.test_data['order_date'].dt.to_period('M')\n",
    "            self.test_data['month'] = self.test_data['order_date'].dt.month\n",
    "            self.test_data['quarter'] = self.test_data['order_date'].dt.quarter\n",
    "        \n",
    "        # Calculate benchmark errors\n",
    "        self.test_data['benchmark_error'] = self.test_data[self.actual_col] - self.test_data[self.benchmark_col]\n",
    "        self.test_data['benchmark_abs_error'] = np.abs(self.test_data['benchmark_error'])\n",
    "        self.test_data['benchmark_pct_error'] = np.where(\n",
    "            self.test_data[self.actual_col] > 0,\n",
    "            (self.test_data['benchmark_error'] / self.test_data[self.actual_col]) * 100,\n",
    "            0\n",
    "        )\n",
    "        self.test_data['benchmark_abs_pct_error'] = np.abs(self.test_data['benchmark_pct_error'])\n",
    "        \n",
    "        # Calculate ML model errors\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            self.test_data[f'{model_name}_error'] = self.test_data[self.actual_col] - self.test_data[pred_col]\n",
    "            self.test_data[f'{model_name}_abs_error'] = np.abs(self.test_data[f'{model_name}_error'])\n",
    "            self.test_data[f'{model_name}_pct_error'] = np.where(\n",
    "                self.test_data[self.actual_col] > 0,\n",
    "                (self.test_data[f'{model_name}_error'] / self.test_data[self.actual_col]) * 100,\n",
    "                0\n",
    "            )\n",
    "            self.test_data[f'{model_name}_abs_pct_error'] = np.abs(self.test_data[f'{model_name}_pct_error'])\n",
    "        \n",
    "        print(f\"   ✅ Analysis data prepared: {len(self.test_data):,} records\")\n",
    "        print(f\"   📊 ML models found: {len(ml_pred_cols)}\")\n",
    "        \n",
    "    def analyze_overall_accuracy(self):\n",
    "        \"\"\"\n",
    "        1. Overall Accuracy Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n📈 1. OVERALL ACCURACY ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Get ML model prediction columns\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        if not ml_pred_cols:\n",
    "            print(\"❌ No ML model predictions found\")\n",
    "            return {}\n",
    "        \n",
    "        accuracy_results = {}\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        benchmark_mape = self.test_data['benchmark_abs_pct_error'].mean()\n",
    "        \n",
    "        print(f\"🎯 BENCHMARK PERFORMANCE:\")\n",
    "        print(f\"   MAE:  ${benchmark_mae:,.2f}\")\n",
    "        print(f\"   MAPE: {benchmark_mape:.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        accuracy_results['benchmark'] = {'mae': benchmark_mae, 'mape': benchmark_mape}\n",
    "        \n",
    "        # ML model performance\n",
    "        print(f\"🤖 ML MODEL PERFORMANCE:\")\n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            ml_mae = self.test_data[f'{model_name}_abs_error'].mean()\n",
    "            ml_mape = self.test_data[f'{model_name}_abs_pct_error'].mean()\n",
    "            \n",
    "            # Improvement calculations\n",
    "            mae_improvement = benchmark_mae - ml_mae\n",
    "            mae_improvement_pct = (mae_improvement / benchmark_mae) * 100\n",
    "            \n",
    "            mape_improvement = benchmark_mape - ml_mape\n",
    "            \n",
    "            status = \"✅ BETTER\" if mae_improvement > 0 else \"❌ WORSE\"\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            print(f\"     MAE:  ${ml_mae:,.2f} (${mae_improvement:+,.2f}, {mae_improvement_pct:+.1f}%) {status}\")\n",
    "            print(f\"     MAPE: {ml_mape:.1f}% ({mape_improvement:+.1f}%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            accuracy_results[model_name] = {\n",
    "                'mae': ml_mae,\n",
    "                'mape': ml_mape,\n",
    "                'mae_improvement': mae_improvement,\n",
    "                'mae_improvement_pct': mae_improvement_pct,\n",
    "                'mape_improvement': mape_improvement\n",
    "            }\n",
    "        \n",
    "        return accuracy_results\n",
    "    \n",
    "    def analyze_weighted_accuracy(self):\n",
    "        \"\"\"\n",
    "        2. Weighted Accuracy by Product Revenue\n",
    "        \"\"\"\n",
    "        print(\"\\n💰 2. WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        if 'sku' not in self.test_data.columns:\n",
    "            print(\"❌ No SKU column found for product analysis\")\n",
    "            return {}\n",
    "        \n",
    "        # Calculate product weights by revenue\n",
    "        product_revenue = self.test_data.groupby('sku')[self.actual_col].sum().sort_values(ascending=False)\n",
    "        total_revenue = product_revenue.sum()\n",
    "        product_weights = product_revenue / total_revenue\n",
    "        \n",
    "        # Top products analysis\n",
    "        top_20_pct_threshold = product_revenue.quantile(0.8)\n",
    "        top_products = product_revenue[product_revenue >= top_20_pct_threshold].index\n",
    "        \n",
    "        top_product_data = self.test_data[self.test_data['sku'].isin(top_products)]\n",
    "        bottom_product_data = self.test_data[~self.test_data['sku'].isin(top_products)]\n",
    "        \n",
    "        print(f\"📊 PRODUCT SEGMENTATION:\")\n",
    "        print(f\"   Top 20% Products: {len(top_products)} SKUs, ${product_revenue[product_revenue >= top_20_pct_threshold].sum():,.0f} revenue ({product_revenue[product_revenue >= top_20_pct_threshold].sum()/total_revenue*100:.1f}%)\")\n",
    "        print(f\"   Bottom 80% Products: {len(product_revenue) - len(top_products)} SKUs, ${product_revenue[product_revenue < top_20_pct_threshold].sum():,.0f} revenue ({product_revenue[product_revenue < top_20_pct_threshold].sum()/total_revenue*100:.1f}%)\")\n",
    "        print(\"\")\n",
    "        \n",
    "        weighted_results = {}\n",
    "        \n",
    "        # Analyze performance on top vs bottom products\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for segment_name, segment_data in [('Top 20% Products', top_product_data), ('Bottom 80% Products', bottom_product_data)]:\n",
    "            print(f\"🎯 {segment_name.upper()}:\")\n",
    "            \n",
    "            # Benchmark performance\n",
    "            bench_mae = segment_data['benchmark_abs_error'].mean()\n",
    "            bench_mape = segment_data['benchmark_abs_pct_error'].mean()\n",
    "            print(f\"   Benchmark: MAE ${bench_mae:.2f}, MAPE {bench_mape:.1f}%\")\n",
    "            \n",
    "            segment_results = {'benchmark': {'mae': bench_mae, 'mape': bench_mape}}\n",
    "            \n",
    "            # ML model performance\n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = segment_data[f'{model_name}_abs_error'].mean()\n",
    "                ml_mape = segment_data[f'{model_name}_abs_pct_error'].mean()\n",
    "                \n",
    "                mae_improvement = bench_mae - ml_mae\n",
    "                mae_improvement_pct = (mae_improvement / bench_mae) * 100\n",
    "                \n",
    "                status = \"✅\" if mae_improvement > 0 else \"❌\"\n",
    "                print(f\"   {model_name}: MAE ${ml_mae:.2f} ({mae_improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                segment_results[model_name] = {\n",
    "                    'mae': ml_mae,\n",
    "                    'mape': ml_mape,\n",
    "                    'improvement_pct': mae_improvement_pct\n",
    "                }\n",
    "            \n",
    "            weighted_results[segment_name] = segment_results\n",
    "            print(\"\")\n",
    "        \n",
    "        return weighted_results\n",
    "    \n",
    "    def analyze_channel_performance(self):\n",
    "        \"\"\"\n",
    "        3. Channel Performance Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n📺 3. CHANNEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'channel' not in self.test_data.columns:\n",
    "            print(\"❌ No channel column found\")\n",
    "            return {}\n",
    "        \n",
    "        channel_results = {}\n",
    "        channels = self.test_data['channel'].unique()\n",
    "        \n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for channel in channels:\n",
    "            channel_data = self.test_data[self.test_data['channel'] == channel]\n",
    "            \n",
    "            print(f\"📻 {channel.upper()} CHANNEL:\")\n",
    "            print(f\"   Records: {len(channel_data):,}\")\n",
    "            print(f\"   Revenue: ${channel_data[self.actual_col].sum():,.0f}\")\n",
    "            \n",
    "            # Benchmark performance\n",
    "            bench_mae = channel_data['benchmark_abs_error'].mean()\n",
    "            bench_mape = channel_data['benchmark_abs_pct_error'].mean()\n",
    "            print(f\"   Benchmark: MAE ${bench_mae:.2f}, MAPE {bench_mape:.1f}%\")\n",
    "            \n",
    "            channel_results[channel] = {'benchmark': {'mae': bench_mae, 'mape': bench_mape}}\n",
    "            \n",
    "            # ML model performance  \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = channel_data[f'{model_name}_abs_error'].mean()\n",
    "                ml_mape = channel_data[f'{model_name}_abs_pct_error'].mean()\n",
    "                \n",
    "                mae_improvement = bench_mae - ml_mae\n",
    "                mae_improvement_pct = (mae_improvement / bench_mae) * 100\n",
    "                \n",
    "                status = \"✅ BETTER\" if mae_improvement > 0 else \"❌ WORSE\"\n",
    "                print(f\"   {model_name}: MAE ${ml_mae:.2f} ({mae_improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                channel_results[channel][model_name] = {\n",
    "                    'mae': ml_mae,\n",
    "                    'mape': ml_mape,\n",
    "                    'improvement_pct': mae_improvement_pct\n",
    "                }\n",
    "            \n",
    "            print(\"\")\n",
    "        \n",
    "        return channel_results\n",
    "    \n",
    "    def analyze_temporal_performance(self):\n",
    "        \"\"\"\n",
    "        4. Temporal Performance Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n📅 4. TEMPORAL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'year_month' not in self.test_data.columns:\n",
    "            print(\"❌ No temporal data available\")\n",
    "            return {}\n",
    "        \n",
    "        # Monthly performance analysis\n",
    "        monthly_data = self.test_data.groupby('year_month').agg({\n",
    "            self.actual_col: 'sum',\n",
    "            self.benchmark_col: 'sum',\n",
    "            'benchmark_abs_error': 'mean',\n",
    "            'benchmark_abs_pct_error': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Add ML model monthly performance\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            monthly_ml = self.test_data.groupby('year_month').agg({\n",
    "                f'{model_name}_abs_error': 'mean',\n",
    "                f'{model_name}_abs_pct_error': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            monthly_data = monthly_data.join(monthly_ml)\n",
    "        \n",
    "        # Calculate month-by-month wins/losses\n",
    "        temporal_results = {}\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            # Count wins per month\n",
    "            monthly_comparison = []\n",
    "            \n",
    "            for month in monthly_data.index:\n",
    "                month_data = self.test_data[self.test_data['year_month'] == month]\n",
    "                \n",
    "                if len(month_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                bench_mae = month_data['benchmark_abs_error'].mean()\n",
    "                ml_mae = month_data[f'{model_name}_abs_error'].mean()\n",
    "                \n",
    "                wins_benchmark = ml_mae < bench_mae\n",
    "                improvement = bench_mae - ml_mae\n",
    "                improvement_pct = (improvement / bench_mae) * 100 if bench_mae > 0 else 0\n",
    "                \n",
    "                monthly_comparison.append({\n",
    "                    'month': month,\n",
    "                    'benchmark_mae': bench_mae,\n",
    "                    'ml_mae': ml_mae,\n",
    "                    'wins_benchmark': wins_benchmark,\n",
    "                    'improvement': improvement,\n",
    "                    'improvement_pct': improvement_pct,\n",
    "                    'records': len(month_data)\n",
    "                })\n",
    "            \n",
    "            comparison_df = pd.DataFrame(monthly_comparison)\n",
    "            \n",
    "            # Summary statistics\n",
    "            total_months = len(comparison_df)\n",
    "            months_won = comparison_df['wins_benchmark'].sum()\n",
    "            win_rate = (months_won / total_months) * 100 if total_months > 0 else 0\n",
    "            avg_improvement = comparison_df['improvement_pct'].mean()\n",
    "            \n",
    "            print(f\"📊 {model_name.upper()} MONTHLY PERFORMANCE:\")\n",
    "            print(f\"   Months tested: {total_months}\")\n",
    "            print(f\"   Months won: {months_won}/{total_months} ({win_rate:.1f}%)\")\n",
    "            print(f\"   Average improvement: {avg_improvement:+.1f}%\")\n",
    "            \n",
    "            # Show recent monthly performance\n",
    "            print(f\"   Recent performance:\")\n",
    "            for _, row in comparison_df.tail(6).iterrows():\n",
    "                status = \"✅ WIN\" if row['wins_benchmark'] else \"❌ LOSS\"\n",
    "                print(f\"     {row['month']}: {row['improvement_pct']:+.1f}% {status}\")\n",
    "            \n",
    "            temporal_results[model_name] = {\n",
    "                'total_months': total_months,\n",
    "                'months_won': months_won,\n",
    "                'win_rate': win_rate,\n",
    "                'avg_improvement': avg_improvement,\n",
    "                'monthly_details': comparison_df\n",
    "            }\n",
    "            print(\"\")\n",
    "        \n",
    "        return temporal_results\n",
    "    \n",
    "    def analyze_cumulative_error_impact(self):\n",
    "        \"\"\"\n",
    "        5. Cumulative Error & Annual Risk Analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n💰 5. CUMULATIVE ERROR & ANNUAL RISK ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if 'year_month' not in self.test_data.columns:\n",
    "            print(\"❌ No temporal data for cumulative analysis\")\n",
    "            return {}\n",
    "        \n",
    "        # Monthly aggregated data\n",
    "        monthly_data = self.test_data.groupby('year_month').agg({\n",
    "            self.actual_col: 'sum',\n",
    "            self.benchmark_col: 'sum',\n",
    "            'benchmark_error': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add ML model data\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        cumulative_results = {}\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            # Calculate monthly ML predictions and errors\n",
    "            monthly_ml = self.test_data.groupby('year_month').agg({\n",
    "                pred_col: 'sum',\n",
    "                f'{model_name}_error': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Merge with main monthly data\n",
    "            monthly_combined = monthly_data.merge(monthly_ml, on='year_month')\n",
    "            \n",
    "            # Calculate cumulative metrics\n",
    "            monthly_combined['cumulative_actual'] = monthly_combined[self.actual_col].cumsum()\n",
    "            monthly_combined['cumulative_benchmark'] = monthly_combined[self.benchmark_col].cumsum()\n",
    "            monthly_combined['cumulative_ml'] = monthly_combined[pred_col].cumsum()\n",
    "            \n",
    "            monthly_combined['cumulative_benchmark_error'] = monthly_combined['benchmark_error'].cumsum()\n",
    "            monthly_combined['cumulative_ml_error'] = monthly_combined[f'{model_name}_error'].cumsum()\n",
    "            \n",
    "            monthly_combined['cumulative_benchmark_error_pct'] = (monthly_combined['cumulative_benchmark_error'] / monthly_combined['cumulative_actual']) * 100\n",
    "            monthly_combined['cumulative_ml_error_pct'] = (monthly_combined['cumulative_ml_error'] / monthly_combined['cumulative_actual']) * 100\n",
    "            \n",
    "            # Annual projections\n",
    "            avg_monthly_demand = monthly_combined[self.actual_col].mean()\n",
    "            last_month_ml_error_rate = monthly_combined[f'{model_name}_error'].iloc[-1] / monthly_combined[self.actual_col].iloc[-1]\n",
    "            last_month_benchmark_error_rate = monthly_combined['benchmark_error'].iloc[-1] / monthly_combined[self.actual_col].iloc[-1]\n",
    "            \n",
    "            # Project full year impact\n",
    "            annual_demand_projection = avg_monthly_demand * 12\n",
    "            annual_ml_error_projection = last_month_ml_error_rate * avg_monthly_demand * 12\n",
    "            annual_benchmark_error_projection = last_month_benchmark_error_rate * avg_monthly_demand * 12\n",
    "            \n",
    "            annual_ml_error_pct = (annual_ml_error_projection / annual_demand_projection) * 100\n",
    "            annual_benchmark_error_pct = (annual_benchmark_error_projection / annual_demand_projection) * 100\n",
    "            \n",
    "            # Final cumulative error\n",
    "            final_ml_cumulative_error = monthly_combined['cumulative_ml_error_pct'].iloc[-1]\n",
    "            final_benchmark_cumulative_error = monthly_combined['cumulative_benchmark_error_pct'].iloc[-1]\n",
    "            \n",
    "            print(f\"📊 {model_name.upper()} CUMULATIVE ANALYSIS:\")\n",
    "            print(f\"   Current cumulative error: {final_ml_cumulative_error:+.1f}%\")\n",
    "            print(f\"   Benchmark cumulative error: {final_benchmark_cumulative_error:+.1f}%\")\n",
    "            print(f\"   Projected annual error: {annual_ml_error_pct:+.1f}%\")\n",
    "            print(f\"   Benchmark annual error: {annual_benchmark_error_pct:+.1f}%\")\n",
    "            \n",
    "            # Risk assessment\n",
    "            if abs(annual_ml_error_pct) < 5:\n",
    "                risk_level = \"🟢 LOW RISK\"\n",
    "            elif abs(annual_ml_error_pct) < 15:\n",
    "                risk_level = \"🟡 MODERATE RISK\"\n",
    "            else:\n",
    "                risk_level = \"🔴 HIGH RISK\"\n",
    "            \n",
    "            print(f\"   Risk Level: {risk_level}\")\n",
    "            \n",
    "            # Show if ML beats benchmark cumulatively\n",
    "            cumulative_improvement = final_benchmark_cumulative_error - final_ml_cumulative_error\n",
    "            annual_improvement = annual_benchmark_error_pct - annual_ml_error_pct\n",
    "            \n",
    "            print(f\"   Cumulative improvement: {cumulative_improvement:+.1f}%\")\n",
    "            print(f\"   Annual improvement: {annual_improvement:+.1f}%\")\n",
    "            \n",
    "            cumulative_results[model_name] = {\n",
    "                'final_cumulative_error': final_ml_cumulative_error,\n",
    "                'projected_annual_error': annual_ml_error_pct,\n",
    "                'risk_level': risk_level,\n",
    "                'cumulative_improvement': cumulative_improvement,\n",
    "                'annual_improvement': annual_improvement,\n",
    "                'monthly_data': monthly_combined\n",
    "            }\n",
    "            print(\"\")\n",
    "        \n",
    "        return cumulative_results\n",
    "    \n",
    "    def feature_importance_deep_dive(self):\n",
    "    \n",
    "        \n",
    "        # Analyze prediction patterns\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        if ml_pred_cols and 'demand_ma_3d' in self.test_data.columns:\n",
    "            print(\"📊 CORRELATION ANALYSIS:\")\n",
    "            \n",
    "            for pred_col in ml_pred_cols[:2]:  # Check top 2 models\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                \n",
    "                # Correlation between ML prediction and 3-day moving average\n",
    "                if 'demand_ma_3d' in self.test_data.columns:\n",
    "                    correlation = self.test_data[pred_col].corr(self.test_data['demand_ma_3d'])\n",
    "                    print(f\"   {model_name} vs demand_ma_3d: {correlation:.3f} correlation\")\n",
    "                \n",
    "                # Correlation with actual demand\n",
    "                correlation_actual = self.test_data[pred_col].corr(self.test_data[self.actual_col])\n",
    "                print(f\"   {model_name} vs actual demand: {correlation_actual:.3f} correlation\")\n",
    "                \n",
    "                # Correlation with benchmark\n",
    "                correlation_benchmark = self.test_data[pred_col].corr(self.test_data[self.benchmark_col])\n",
    "                print(f\"   {model_name} vs benchmark: {correlation_benchmark:.3f} correlation\")\n",
    "                print(\"\")\n",
    "    \n",
    "    def run_comprehensive_analysis(self):\n",
    "        \"\"\"\n",
    "        Run all analyses and provide summary\n",
    "        \"\"\"\n",
    "        print(\"🚀 RUNNING COMPREHENSIVE ML MODEL ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            results['overall_accuracy'] = self.analyze_overall_accuracy()\n",
    "            results['weighted_accuracy'] = self.analyze_weighted_accuracy()\n",
    "            results['channel_performance'] = self.analyze_channel_performance()\n",
    "            results['temporal_performance'] = self.analyze_temporal_performance()\n",
    "            results['cumulative_analysis'] = self.analyze_cumulative_error_impact()\n",
    "            self.feature_importance_deep_dive()\n",
    "            \n",
    "            # Summary conclusions\n",
    "            self.provide_final_recommendations(results)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during analysis: {str(e)}\")\n",
    "            return results\n",
    "    \n",
    "    def provide_final_recommendations(self, results):\n",
    "        \"\"\"\n",
    "        Provide final recommendations based on analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n🎯 FINAL RECOMMENDATIONS\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        # Find best performing ML model\n",
    "        if 'overall_accuracy' in results:\n",
    "            best_model = None\n",
    "            best_improvement = -float('inf')\n",
    "            \n",
    "            for model_name, metrics in results['overall_accuracy'].items():\n",
    "                if model_name != 'benchmark' and 'mae_improvement_pct' in metrics:\n",
    "                    if metrics['mae_improvement_pct'] > best_improvement:\n",
    "                        best_improvement = metrics['mae_improvement_pct']\n",
    "                        best_model = model_name\n",
    "            \n",
    "            if best_model and best_improvement > 0:\n",
    "                print(f\"✅ BEST MODEL: {best_model}\")\n",
    "                print(f\"   Overall improvement: {best_improvement:+.1f}%\")\n",
    "                \n",
    "                # Check consistency\n",
    "                if 'temporal_performance' in results and best_model in results['temporal_performance']:\n",
    "                    win_rate = results['temporal_performance'][best_model]['win_rate']\n",
    "                    print(f\"   Monthly win rate: {win_rate:.1f}%\")\n",
    "                    \n",
    "                    if win_rate >= 70:\n",
    "                        print(\"   🟢 CONSISTENT performer - safe to deploy\")\n",
    "                    elif win_rate >= 50:\n",
    "                        print(\"   🟡 MODERATELY consistent - monitor closely\")\n",
    "                    else:\n",
    "                        print(\"   🔴 INCONSISTENT - risky for production\")\n",
    "                \n",
    "                print(\"\")\n",
    "                print(\"📋 DEPLOYMENT RECOMMENDATIONS:\")\n",
    "                if best_improvement > 10 and win_rate >= 70:\n",
    "                    print(\"   🚀 DEPLOY: Strong, consistent improvement\")\n",
    "                elif best_improvement > 5:\n",
    "                    print(\"   🧪 PILOT: Test on subset before full deployment\")\n",
    "                else:\n",
    "                    print(\"   ⏸️  HOLD: Improvement too small, stick with benchmark\")\n",
    "                    \n",
    "            else:\n",
    "                print(\"❌ NO ML MODEL BEATS BENCHMARK\")\n",
    "                print(\"   💡 Stick with your benchmark model\")\n",
    "                print(\"   🔄 Consider different features or model types\")\n",
    "\n",
    "def analyze_ml_model_performance(test_data, model_results, benchmark_col='bm_demand', actual_col='actual_demand'):\n",
    "    \"\"\"\n",
    "    Main function to run comprehensive ML model analysis\n",
    "    \n",
    "    Args:\n",
    "        test_data: Test dataset with predictions\n",
    "        model_results: Dictionary of trained models and results\n",
    "        benchmark_col: Column name for benchmark predictions\n",
    "        actual_col: Column name for actual values\n",
    "    \n",
    "    Returns:\n",
    "        MLModelEvaluationAnalysis instance with all results\n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = MLModelEvaluationAnalysis(test_data, model_results, benchmark_col, actual_col)\n",
    "    analysis_results = analyzer.run_comprehensive_analysis()\n",
    "    \n",
    "    return analyzer, analysis_results\n",
    "\n",
    "# Usage:\n",
    "analyzer, analysis_results = analyze_ml_model_performance(test_data, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f7321b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 ADVANCED DEMAND PREDICTION MODELS\n",
      "🎯 Goal: Beat benchmark with sophisticated ML features\n",
      "============================================================\n",
      "   📊 Amazon order_items: 996,886 records\n",
      "   📊 Tiktok order_items: 232,267 records\n",
      "   📊 Shopify order_items: 10,773,655 records\n",
      "   ✅ Combined order_items: 12,002,808 total records\n",
      "   📊 Amazon daily_sku: 24,690 records\n",
      "   📊 Tiktok daily_sku: 6,836 records\n",
      "   📊 Shopify daily_sku: 70,585 records\n",
      "   ✅ Combined daily_sku: 102,111 total records\n",
      "\n",
      "🔧 Preparing base data...\n",
      "   ✅ Base data: 97,190 records\n",
      "   📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "\n",
      "🤖 MODEL TRAINING & EVALUATION\n",
      "---------------------------------------------\n",
      "\n",
      "🏗️ FEATURE ENGINEERING PIPELINE\n",
      "----------------------------------------\n",
      "   🗓️ Engineering seasonality features...\n",
      "   🛍️ Engineering product features...\n",
      "   👥 Engineering customer lifecycle features...\n",
      "   🔗 Engineering cross-SKU interaction features...\n",
      "   📈 Engineering lag features...\n",
      "   ✅ Feature engineering complete\n",
      "   📊 Total features: 76\n",
      "   📋 Records: 97,190\n",
      "📊 Training Data: 77,752 records\n",
      "📊 Test Data: 19,438 records\n",
      "🎯 Benchmark MAE: $1,660.69\n",
      "\n",
      "🔄 Training Linear Basic...\n",
      "   MAE: $1,913.00 | Improvement: $-252.31 (-15.2%) | ❌ UNDERPERFORMS\n",
      "🔄 Training Linear Enhanced...\n",
      "   MAE: $2,016.57 | Improvement: $-355.88 (-21.4%) | ❌ UNDERPERFORMS\n",
      "🔄 Training Ridge Regression...\n",
      "   MAE: $415.69 | Improvement: $+1,245.00 (+75.0%) | ✅ BEATS BENCHMARK\n",
      "🔄 Training Random Forest...\n",
      "   MAE: $342.94 | Improvement: $+1,317.75 (+79.3%) | ✅ BEATS BENCHMARK\n",
      "🔄 Training Gradient Boosting...\n",
      "   MAE: $302.24 | Improvement: $+1,358.45 (+81.8%) | ✅ BEATS BENCHMARK\n",
      "\n",
      "📋 MODEL FORMULAS & PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "            🚀 GRADIENT BOOSTING MODEL:\n",
      "            Demand = Σₘ₌₁ᴹ γₘ × hₘ(X)\n",
      "\n",
      "            Where:\n",
      "            - hₘ(X) = weak learner trained on residuals\n",
      "            - γₘ = learning rate\n",
      "            - X = comprehensive feature set\n",
      "\n",
      "            Sequential learning: Each tree learns from previous errors\n",
      "            Advanced feature interactions: Captures complex non-linear patterns\n",
      "            \n",
      "📊 PERFORMANCE:\n",
      "   MAE: $302.24\n",
      "   MAPE: 15.0%\n",
      "   Improvement vs Benchmark: $+1,358.45 (+81.8%)\n",
      "   Features Used: 76\n",
      "------------------------------------------------------------\n",
      "\n",
      "            🌳 RANDOM FOREST MODEL:\n",
      "            Demand = TreeEnsemble(Customer_Features, Product_Features, Market_Features, Time_Features)\n",
      "\n",
      "            Features include:\n",
      "            - All customer lifecycle variables\n",
      "            - Advanced seasonality patterns\n",
      "            - Product pricing and lifecycle\n",
      "            - Cross-SKU market effects\n",
      "            - Historical lag features\n",
      "\n",
      "            Non-linear: Captures feature interactions automatically\n",
      "            \n",
      "📊 PERFORMANCE:\n",
      "   MAE: $342.94\n",
      "   MAPE: 13.1%\n",
      "   Improvement vs Benchmark: $+1,317.75 (+79.3%)\n",
      "   Features Used: 76\n",
      "------------------------------------------------------------\n",
      "📊 PERFORMANCE:\n",
      "   MAE: $415.69\n",
      "   MAPE: 228.1%\n",
      "   Improvement vs Benchmark: $+1,245.00 (+75.0%)\n",
      "   Features Used: 76\n",
      "------------------------------------------------------------\n",
      "\n",
      "            🔢 LINEAR BASIC MODEL:\n",
      "            Demand = β₀ + β₁×new_customers + β₂×existing_customers + β₃×seasonality + ε\n",
      "\n",
      "            Features: Customer counts + basic seasonality\n",
      "            \n",
      "📊 PERFORMANCE:\n",
      "   MAE: $1,913.00\n",
      "   MAPE: 492.8%\n",
      "   Improvement vs Benchmark: $-252.31 (-15.2%)\n",
      "   Features Used: 5\n",
      "------------------------------------------------------------\n",
      "\n",
      "            🔢 LINEAR ENHANCED MODEL:\n",
      "            Demand = β₀ + β₁×new_customers + β₂×existing_customers + β₃×month_sin + β₄×month_cos + \n",
      "                     β₅×customer_loyalty_score + β₆×market_share + β₇×product_age + ε\n",
      "\n",
      "            Features: Customer counts + advanced seasonality + loyalty + market position + product lifecycle\n",
      "            \n",
      "📊 PERFORMANCE:\n",
      "   MAE: $2,016.57\n",
      "   MAPE: 671.4%\n",
      "   Improvement vs Benchmark: $-355.88 (-21.4%)\n",
      "   Features Used: 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 TOP 10 FEATURE IMPORTANCE (Random Forest):\n",
      "--------------------------------------------------\n",
      "   demand_ma_3d             : 0.9018\n",
      "   total_customers          : 0.0327\n",
      "   total_customers_calc     : 0.0279\n",
      "   error_percentage         : 0.0078\n",
      "   demand_vs_market_avg     : 0.0056\n",
      "   new_customer_demand      : 0.0028\n",
      "   new_customers            : 0.0027\n",
      "   market_share             : 0.0022\n",
      "   market_total_demand      : 0.0016\n",
      "   customers_lag_3d         : 0.0015\n"
     ]
    }
   ],
   "source": [
    "modeler, analyzer, results =run_advanced_demand_models(\n",
    "    benchmark_df=enhanced_benchmark_df,\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items, \n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cdae7c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 ADVANCED DEMAND PREDICTION MODELS\n",
      "🎯 Goal: Beat benchmark with sophisticated ML features\n",
      "============================================================\n",
      "   📊 Amazon order_items: 996,886 records\n",
      "   📊 Tiktok order_items: 232,267 records\n",
      "   📊 Shopify order_items: 10,773,655 records\n",
      "   ✅ Combined order_items: 12,002,808 total records\n",
      "   📊 Amazon daily_sku: 24,690 records\n",
      "   📊 Tiktok daily_sku: 6,836 records\n",
      "   📊 Shopify daily_sku: 70,585 records\n",
      "   ✅ Combined daily_sku: 102,111 total records\n",
      "\n",
      "🔧 Preparing base data...\n",
      "   ✅ Base data: 97,190 records\n",
      "   📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "\n",
      "🤖 MODEL TRAINING & EVALUATION\n",
      "---------------------------------------------\n",
      "\n",
      "🏗️ FEATURE ENGINEERING PIPELINE\n",
      "----------------------------------------\n",
      "   🗓️ Engineering seasonality features...\n",
      "   🛍️ Engineering product features...\n",
      "   👥 Engineering customer lifecycle features...\n",
      "   🔗 Engineering cross-SKU interaction features...\n",
      "   📈 Engineering lag features...\n",
      "   ✅ Feature engineering complete\n",
      "   📊 Total features: 68\n",
      "   📋 Records: 97,190\n",
      "📊 Training Data: 77,752 records\n",
      "📊 Test Data: 19,438 records\n",
      "🎯 Benchmark MAE: $1,660.69\n",
      "🎯 Benchmark MAPE: 61.2%\n",
      "\n",
      "🎯 TRAINING MODELS WITH DEFINED FORMULAS:\n",
      "--------------------------------------------------\n",
      "\n",
      "🔄 Training Customer-Centric Linear...\n",
      "📋 Formula: \n",
      "            📊 CUSTOMER-CENTRIC LINEAR MODEL:\n",
      "            Demand = β₀ + β₁×new_customers + β₂×existing_customers + β₃×customer_loyalty_score + \n",
      "                     β₄×new_customer_ratio + β₅×month_sin + β₆×month_cos + ε\n",
      "\n",
      "            Focus: Customer behavior drives demand\n",
      "            Weights: Customer features get 70% emphasis\n",
      "            \n",
      "   ✅ MAE: $1,894.95, MAPE: 393.1%\n",
      "   📊 Features used: 6\n",
      "\n",
      "🔄 Training Seasonality-Weighted Linear...\n",
      "📋 Formula: \n",
      "            📅 SEASONALITY-WEIGHTED LINEAR MODEL:\n",
      "            Demand = β₀ + β₁×demand_lag_7d + β₂×demand_ma_14d + β₃×month_sin + β₄×month_cos + \n",
      "                     β₅×dow_sin + β₆×dow_cos + β₇×is_q4×3 + β₈×is_weekend×2 + ε\n",
      "\n",
      "            Focus: Temporal patterns drive demand\n",
      "            Weights: Seasonality features get 60% emphasis, holidays 3x weight\n",
      "            \n",
      "   ✅ MAE: $1,284.75, MAPE: 542.5%\n",
      "   📊 Features used: 9\n",
      "\n",
      "🔄 Training Product-Market Linear...\n",
      "📋 Formula: \n",
      "            🛍️ PRODUCT-MARKET LINEAR MODEL:\n",
      "            Demand = β₀ + β₁×avg_price + β₂×product_age_days + β₃×market_share×5 + \n",
      "                     β₄×demand_vs_market_avg×3 + β₅×is_premium×2 + β₆×price_volatility + ε\n",
      "\n",
      "            Focus: Product characteristics and market position\n",
      "            Weights: Market features get 5x emphasis, premium products 2x\n",
      "            \n",
      "   ✅ MAE: $2,155.41, MAPE: 766.6%\n",
      "   📊 Features used: 7\n",
      "\n",
      "🔄 Training Hybrid Weighted Linear...\n",
      "📋 Formula: \n",
      "            🔄 HYBRID WEIGHTED LINEAR MODEL:\n",
      "            Demand = β₀ + 0.4×(Customer_Score) + 0.3×(Seasonality_Score) + 0.3×(Product_Score)\n",
      "\n",
      "            Where:\n",
      "            Customer_Score = new_customers + existing_customers×1.5 + loyalty_score×2\n",
      "            Seasonality_Score = month_sin + month_cos + lag_features\n",
      "            Product_Score = market_share×3 + price_features\n",
      "\n",
      "            Focus: Balanced approach with domain expertise weights\n",
      "            \n",
      "   ✅ MAE: $1,247.31, MAPE: 450.5%\n",
      "   📊 Features used: 20\n",
      "\n",
      "🔄 Training Elastic Net Regularized...\n",
      "📋 Formula: \n",
      "            📐 ELASTIC NET MODEL:\n",
      "            Demand = β₀ + Σᵢ(βᵢ×Xᵢ) + λ₁×Σᵢ|βᵢ| + λ₂×Σᵢ(βᵢ²)\n",
      "\n",
      "            Features: All features with L1 + L2 regularization\n",
      "            α = 0.5 (balanced L1/L2), automatic feature selection\n",
      "            \n",
      "   ✅ MAE: $1,112.40, MAPE: 659.4%\n",
      "   📊 Features used: 68\n",
      "\n",
      "🔄 Training Ridge Comprehensive...\n",
      "📋 Formula: \n",
      "            📊 RIDGE COMPREHENSIVE MODEL:\n",
      "            Demand = β₀ + Σᵢ(βᵢ×Xᵢ) + λ×Σᵢ(βᵢ²)\n",
      "\n",
      "            Features: All engineered features with L2 regularization\n",
      "            Prevents overfitting while keeping all features\n",
      "            \n",
      "   ✅ MAE: $1,114.00, MAPE: 634.2%\n",
      "   📊 Features used: 68\n",
      "\n",
      "🔄 Training Random Forest...\n",
      "📋 Formula: \n",
      "            🌳 RANDOM FOREST MODEL:\n",
      "            Demand = TreeEnsemble(All_Features)\n",
      "\n",
      "            Non-linear feature interactions, automatic importance ranking\n",
      "            Trees: 200, Max depth: 15, Min samples: 10\n",
      "            \n",
      "   ✅ MAE: $299.76, MAPE: 14.1%\n",
      "   📊 Features used: 68\n",
      "\n",
      "🔄 Training Gradient Boosting...\n",
      "📋 Formula: \n",
      "            🚀 GRADIENT BOOSTING MODEL:\n",
      "            Demand = Σₘ₌₁²⁰⁰ γₘ × hₘ(X)\n",
      "\n",
      "            Sequential learning from residuals\n",
      "            Learning rate: 0.1, Max depth: 6\n",
      "            \n",
      "   ✅ MAE: $239.08, MAPE: 19.8%\n",
      "   📊 Features used: 68\n",
      "\n",
      "📈 MODEL PERFORMANCE vs BENCHMARK:\n",
      "---------------------------------------------\n",
      "Customer-Centric Linear:\n",
      "   MAE: $1,894.95 ($-234.26, -14.1%) ❌ UNDERPERFORMS\n",
      "   MAPE: 393.1% (-331.9%)\n",
      "\n",
      "Seasonality-Weighted Linear:\n",
      "   MAE: $1,284.75 ($+375.94, +22.6%) ✅ BEATS BENCHMARK\n",
      "   MAPE: 542.5% (-481.3%)\n",
      "\n",
      "Product-Market Linear:\n",
      "   MAE: $2,155.41 ($-494.72, -29.8%) ❌ UNDERPERFORMS\n",
      "   MAPE: 766.6% (-705.4%)\n",
      "\n",
      "Hybrid Weighted Linear:\n",
      "   MAE: $1,247.31 ($+413.38, +24.9%) ✅ BEATS BENCHMARK\n",
      "   MAPE: 450.5% (-389.3%)\n",
      "\n",
      "Elastic Net Regularized:\n",
      "   MAE: $1,112.40 ($+548.29, +33.0%) ✅ BEATS BENCHMARK\n",
      "   MAPE: 659.4% (-598.2%)\n",
      "\n",
      "Ridge Comprehensive:\n",
      "   MAE: $1,114.00 ($+546.69, +32.9%) ✅ BEATS BENCHMARK\n",
      "   MAPE: 634.2% (-573.0%)\n",
      "\n",
      "Random Forest:\n",
      "   MAE: $299.76 ($+1,360.93, +81.9%) ✅ BEATS BENCHMARK\n",
      "   MAPE: 14.1% (+47.1%)\n",
      "\n",
      "Gradient Boosting:\n",
      "   MAE: $239.08 ($+1,421.61, +85.6%) ✅ BEATS BENCHMARK\n",
      "   MAPE: 19.8% (+41.3%)\n",
      "\n",
      "\n",
      "🔍 TOP 15 FEATURE IMPORTANCE (Random Forest):\n",
      "--------------------------------------------------\n",
      "    1. demand_lag_1d            : 0.7149 (71.5%)\n",
      "    2. total_customers          : 0.1009 (10.1%)\n",
      "    3. new_customer_demand      : 0.0686 (6.9%)\n",
      "    4. total_customers_calc     : 0.0252 (2.5%)\n",
      "    5. error_percentage         : 0.0219 (2.2%)\n",
      "    6. new_customers            : 0.0193 (1.9%)\n",
      "    7. demand_vs_market_avg     : 0.0188 (1.9%)\n",
      "    8. avg_price                : 0.0081 (0.8%)\n",
      "    9. market_avg_demand        : 0.0034 (0.3%)\n",
      "   10. existing_customers       : 0.0034 (0.3%)\n",
      "   11. market_demand_volatility : 0.0028 (0.3%)\n",
      "   12. market_total_demand      : 0.0027 (0.3%)\n",
      "   13. market_share             : 0.0024 (0.2%)\n",
      "   14. existing_customer_demand : 0.0017 (0.2%)\n",
      "   15. demand_ma_7d             : 0.0016 (0.2%)\n"
     ]
    }
   ],
   "source": [
    "modeler, analyzer, results = run_advanced_demand_models(\n",
    "    benchmark_df=enhanced_benchmark_df,\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items, \n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prophet library available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Run the enhanced analysis\\nenhanced_modeler, enhanced_analyzer, enhanced_results, marketing_insights = run_enhanced_demand_analysis_with_prophet_and_marketing(\\n    benchmark_df=your_benchmark_data,\\n    amazon_order_items=your_amazon_data,\\n    # ... other channel data\\n)\\n\\n# Extract specific insights\\nmarketing_effectiveness = enhanced_results['marketing_effectiveness']\\nq4_seasonality = enhanced_results['q4_seasonality']\\nprophet_performance = enhanced_results['prophet_effectiveness']\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prophet import with error handling\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "    print(\"✅ Prophet library available\")\n",
    "except ImportError:\n",
    "    PROPHET_AVAILABLE = False\n",
    "    print(\"⚠️ Prophet not available. Install with: pip install prophet\")\n",
    "\n",
    "class EnhancedDemandModels:\n",
    "    def __init__(self, benchmark_df, amazon_order_items=None, tiktok_order_items=None, shopify_order_items=None,\n",
    "                 amazon_daily_sku=None, tiktok_daily_sku=None, shopify_daily_sku=None):\n",
    "        \n",
    "        print(\"🚀 ENHANCED DEMAND PREDICTION MODELS\")\n",
    "        print(\"🎯 New Features: Prophet forecasting + Marketing transformations + Q4 seasonality\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.benchmark_df = benchmark_df.copy()\n",
    "        \n",
    "        # Combine channel-specific data\n",
    "        self.order_items_df = self.combine_channel_data([\n",
    "            (amazon_order_items, 'amazon'),\n",
    "            (tiktok_order_items, 'tiktok'), \n",
    "            (shopify_order_items, 'shopify')\n",
    "        ], 'order_items')\n",
    "        \n",
    "        self.daily_sku_df = self.combine_channel_data([\n",
    "            (amazon_daily_sku, 'amazon'),\n",
    "            (tiktok_daily_sku, 'tiktok'),\n",
    "            (shopify_daily_sku, 'shopify')\n",
    "        ], 'daily_sku')\n",
    "        \n",
    "        self.models = {}\n",
    "        self.feature_engineered_df = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.prophet_models = {}\n",
    "        \n",
    "        # Prepare base data\n",
    "        self.prepare_base_data()\n",
    "    \n",
    "    def combine_channel_data(self, channel_data_list, data_type):\n",
    "        \"\"\"Combine data from multiple channels into single dataframe\"\"\"\n",
    "        combined_data = []\n",
    "        \n",
    "        for data, channel_name in channel_data_list:\n",
    "            if data is not None and not data.empty:\n",
    "                df = data.copy()\n",
    "                df['channel'] = channel_name\n",
    "                combined_data.append(df)\n",
    "                print(f\"   📊 {channel_name.capitalize()} {data_type}: {len(df):,} records\")\n",
    "        \n",
    "        if combined_data:\n",
    "            result = pd.concat(combined_data, ignore_index=True)\n",
    "            print(f\"   ✅ Combined {data_type}: {len(result):,} total records\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"   ⚠️ No {data_type} data available\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_base_data(self):\n",
    "        \"\"\"Clean and prepare base data for feature engineering\"\"\"\n",
    "        print(\"\\n🔧 Preparing base data...\")\n",
    "        \n",
    "        # Clean benchmark data\n",
    "        self.benchmark_df = self.benchmark_df.dropna(subset=['actual_demand', 'bm_demand'])\n",
    "        self.benchmark_df = self.benchmark_df[self.benchmark_df['actual_demand'] > 0]\n",
    "        \n",
    "        # Ensure date column\n",
    "        self.benchmark_df['order_date'] = pd.to_datetime(self.benchmark_df['order_date'])\n",
    "        \n",
    "        print(f\"   ✅ Base data: {len(self.benchmark_df):,} records\")\n",
    "        print(f\"   📅 Date range: {self.benchmark_df['order_date'].min()} to {self.benchmark_df['order_date'].max()}\")\n",
    "    \n",
    "    def engineer_marketing_transformations(self, df):\n",
    "        \"\"\"Feature Set: Marketing Spend Transformations for Diminishing Returns\"\"\"\n",
    "        print(\"   💰 Engineering marketing spend transformations...\")\n",
    "        \n",
    "        # Identify marketing-related columns\n",
    "        marketing_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in \n",
    "                         ['marketing', 'spend', 'ad', 'advertising', 'campaign', 'promotion', 'cpc', 'cpm', 'roas'])]\n",
    "        \n",
    "        if not marketing_cols:\n",
    "            print(\"   ⚠️ No marketing columns found, creating synthetic marketing data\")\n",
    "            # Create synthetic marketing spend based on demand patterns\n",
    "            np.random.seed(42)\n",
    "            df['marketing_spend'] = np.random.gamma(2, df['actual_demand'] * 0.1) + np.random.normal(0, 50)\n",
    "            df['marketing_spend'] = np.maximum(df['marketing_spend'], 1)  # Ensure positive\n",
    "            marketing_cols = ['marketing_spend']\n",
    "        \n",
    "        print(f\"   📊 Found marketing columns: {marketing_cols}\")\n",
    "        \n",
    "        for col in marketing_cols:\n",
    "            # Ensure positive values for log transformations\n",
    "            df[f'{col}_positive'] = np.maximum(df[col], 1)\n",
    "            \n",
    "            # 1. Log transformation: Y ~ β₁*log(marketing)\n",
    "            # Captures diminishing returns - each additional dollar has decreasing impact\n",
    "            df[f'{col}_log'] = np.log(df[f'{col}_positive'])\n",
    "            \n",
    "            # 2. Quadratic transformation: Y ~ β₁*marketing + β₂*marketing²\n",
    "            # Captures non-linear relationship - can model both increasing and decreasing returns\n",
    "            df[f'{col}_squared'] = df[f'{col}_positive'] ** 2\n",
    "            \n",
    "            # 3. Square root transformation (alternative diminishing returns)\n",
    "            df[f'{col}_sqrt'] = np.sqrt(df[f'{col}_positive'])\n",
    "            \n",
    "            # 4. Marketing efficiency ratio (spend per unit of demand)\n",
    "            df[f'{col}_efficiency'] = df[f'{col}_positive'] / (df['actual_demand'] + 1)\n",
    "            \n",
    "            # 5. Marketing intensity buckets\n",
    "            marketing_quantiles = df[f'{col}_positive'].quantile([0.25, 0.5, 0.75])\n",
    "            df[f'{col}_intensity_low'] = (df[f'{col}_positive'] <= marketing_quantiles[0.25]).astype(int)\n",
    "            df[f'{col}_intensity_medium'] = ((df[f'{col}_positive'] > marketing_quantiles[0.25]) & \n",
    "                                           (df[f'{col}_positive'] <= marketing_quantiles[0.75])).astype(int)\n",
    "            df[f'{col}_intensity_high'] = (df[f'{col}_positive'] > marketing_quantiles[0.75]).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_q4_holiday_features(self, df):\n",
    "       \n",
    "        print(\"   🎄 Engineering Q4 and holiday features...\")\n",
    "        \n",
    "        # Basic Q4 features\n",
    "        df['is_q4'] = (df['order_date'].dt.quarter == 4).astype(int)\n",
    "        df['month'] = df['order_date'].dt.month\n",
    "        \n",
    "        # Detailed Q4 breakdown\n",
    "        df['is_october'] = (df['month'] == 10).astype(int)\n",
    "        df['is_november'] = (df['month'] == 11).astype(int) \n",
    "        df['is_december'] = (df['month'] == 12).astype(int)\n",
    "        \n",
    "        # Black Friday period (last Thursday of November + 4 days)\n",
    "        df['week_of_year'] = df['order_date'].dt.isocalendar().week\n",
    "        \n",
    "        # Approximate Black Friday weeks (weeks 47-48 typically)\n",
    "        df['is_black_friday_week'] = ((df['week_of_year'] >= 47) & (df['week_of_year'] <= 48) & \n",
    "                                     (df['month'] == 11)).astype(int)\n",
    "        \n",
    "        # Cyber Monday (Monday after Black Friday)\n",
    "        df['is_cyber_monday_week'] = df['is_black_friday_week']  # Same week typically\n",
    "        \n",
    "        # Pre-holiday shopping surge (first 3 weeks of December)\n",
    "        df['is_pre_christmas'] = ((df['month'] == 12) & (df['order_date'].dt.day <= 21)).astype(int)\n",
    "        \n",
    "        # Post-holiday period (last week of December)\n",
    "        df['is_post_christmas'] = ((df['month'] == 12) & (df['order_date'].dt.day >= 26)).astype(int)\n",
    "        \n",
    "        # Holiday shopping intensity score\n",
    "        df['holiday_intensity'] = (\n",
    "            df['is_black_friday_week'] * 3 +  # Highest intensity\n",
    "            df['is_pre_christmas'] * 2 +      # High intensity\n",
    "            df['is_q4'] * 1                   # Base Q4 lift\n",
    "        )\n",
    "        \n",
    "        # Days until/since major holidays\n",
    "        # This is approximate - in real implementation you'd use exact holiday dates\n",
    "        df['days_to_black_friday'] = np.where(\n",
    "            (df['month'] == 11) & (df['order_date'].dt.day < 25),\n",
    "            25 - df['order_date'].dt.day,  # Approximate\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        df['days_to_christmas'] = np.where(\n",
    "            df['month'] == 12,\n",
    "            25 - df['order_date'].dt.day,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Year-over-year Q4 growth (if multi-year data)\n",
    "        df['year'] = df['order_date'].dt.year\n",
    "        if df['year'].nunique() > 1:\n",
    "            # Create year-over-year comparison features\n",
    "            for year in sorted(df['year'].unique())[1:]:  # Skip first year\n",
    "                prev_year = year - 1\n",
    "                df[f'is_year_{year}'] = (df['year'] == year).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_prophet_features(self, df):\n",
    "        \"\"\"Feature Set: Prophet-derived features\"\"\"\n",
    "        print(\"   📈 Engineering Prophet-derived features...\")\n",
    "        \n",
    "        if not PROPHET_AVAILABLE:\n",
    "            print(\"   ⚠️ Prophet not available, skipping Prophet features\")\n",
    "            return df\n",
    "        \n",
    "        # Prepare data for Prophet (needs 'ds' and 'y' columns)\n",
    "        prophet_data = df[['order_date', 'actual_demand']].copy()\n",
    "        prophet_data.columns = ['ds', 'y']\n",
    "        prophet_data = prophet_data.groupby('ds')['y'].sum().reset_index()  # Aggregate by date\n",
    "        \n",
    "        try:\n",
    "            # Train Prophet model for trend decomposition\n",
    "            prophet_model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            \n",
    "            # Add custom seasonalities for Q4\n",
    "            prophet_model.add_seasonality(\n",
    "                name='quarterly',\n",
    "                period=91.25,  # ~3 months\n",
    "                fourier_order=4\n",
    "            )\n",
    "            \n",
    "            print(\"   🔄 Training Prophet model for feature extraction...\")\n",
    "            prophet_model.fit(prophet_data)\n",
    "            \n",
    "            # Generate components\n",
    "            future = prophet_model.make_future_dataframe(periods=0)\n",
    "            forecast = prophet_model.predict(future)\n",
    "            \n",
    "            # Extract Prophet components\n",
    "            prophet_components = forecast[['ds', 'trend', 'yearly', 'weekly', 'quarterly']].copy()\n",
    "            prophet_components.columns = ['order_date', 'prophet_trend', 'prophet_yearly', 'prophet_weekly', 'prophet_quarterly']\n",
    "            \n",
    "            # Merge Prophet features back to original data\n",
    "            df = df.merge(prophet_components, on='order_date', how='left')\n",
    "            \n",
    "            # Create additional Prophet-derived features\n",
    "            df['prophet_trend_change'] = df.groupby('sku')['prophet_trend'].pct_change()\n",
    "            df['prophet_seasonality_strength'] = np.abs(df['prophet_yearly']) + np.abs(df['prophet_weekly'])\n",
    "            \n",
    "            # Fill NaN values\n",
    "            prophet_cols = ['prophet_trend', 'prophet_yearly', 'prophet_weekly', 'prophet_quarterly', \n",
    "                          'prophet_trend_change', 'prophet_seasonality_strength']\n",
    "            for col in prophet_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "            \n",
    "            print(\"   ✅ Prophet features extracted successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error in Prophet feature engineering: {str(e)}\")\n",
    "            # Add dummy Prophet features\n",
    "            df['prophet_trend'] = df['actual_demand'].rolling(7).mean()\n",
    "            df['prophet_yearly'] = np.sin(2 * np.pi * df['order_date'].dt.dayofyear / 365.25)\n",
    "            df['prophet_weekly'] = np.sin(2 * np.pi * df['order_date'].dt.dayofweek / 7)\n",
    "            df['prophet_quarterly'] = 0\n",
    "            df = df.fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_log_log_transformations(self, df, target_col='actual_demand'):\n",
    "        \"\"\"Create log-log model transformations: log(Y) ~ β₁*log(X)\"\"\"\n",
    "        print(\"   📊 Creating log-log transformations...\")\n",
    "        \n",
    "        # Create log-transformed target\n",
    "        df[f'{target_col}_log'] = np.log(np.maximum(df[target_col], 1))\n",
    "        \n",
    "        # Find numeric columns for log-log transformation\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        marketing_cols = [col for col in numeric_cols if any(keyword in col.lower() for keyword in \n",
    "                         ['marketing', 'spend', 'ad', 'advertising', 'price', 'cost'])]\n",
    "        \n",
    "        # Create log-log features for marketing variables\n",
    "        for col in marketing_cols:\n",
    "            if col != target_col and not col.endswith('_log'):\n",
    "                # Ensure positive values\n",
    "                positive_col = f'{col}_positive'\n",
    "                if positive_col not in df.columns:\n",
    "                    df[positive_col] = np.maximum(df[col], 1)\n",
    "                \n",
    "                # Create log-log interaction\n",
    "                log_col = f'{col}_log'\n",
    "                if log_col in df.columns:\n",
    "                    df[f'loglog_{col}'] = df[f'{target_col}_log'] * df[log_col]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_enhanced_feature_dataset(self):\n",
    "        \"\"\"Create comprehensive feature-engineered dataset with new transformations\"\"\"\n",
    "        print(\"\\n🏗️ ENHANCED FEATURE ENGINEERING PIPELINE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        df = self.benchmark_df.copy()\n",
    "        \n",
    "        # Apply existing feature engineering\n",
    "        df = self.engineer_seasonality_features(df)\n",
    "        df = self.engineer_product_features(df)\n",
    "        df = self.engineer_customer_lifecycle_features(df)\n",
    "        df = self.engineer_cross_sku_features(df)\n",
    "        \n",
    "        # Apply new enhanced feature engineering\n",
    "        df = self.engineer_marketing_transformations(df)\n",
    "        df = self.engineer_q4_holiday_features(df)\n",
    "        df = self.engineer_prophet_features(df)\n",
    "        df = self.create_log_log_transformations(df)\n",
    "        df = self.engineer_lag_features(df)  # Apply lag features last\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        if 'channel' in df.columns:\n",
    "            le_channel = LabelEncoder()\n",
    "            df['channel_encoded'] = le_channel.fit_transform(df['channel'].fillna('unknown'))\n",
    "        else:\n",
    "            df['channel_encoded'] = 0\n",
    "        \n",
    "        # Remove non-feature columns for modeling\n",
    "        feature_cols = [col for col in df.columns if col not in [\n",
    "            'order_date', 'sku', 'actual_demand', 'bm_demand', 'error_metric', \n",
    "            'product_name', 'first_sale', 'last_sale', 'channel', 'actual_demand_log',\n",
    "            'marketing_spend_positive'  # Remove helper columns\n",
    "        ]]\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        for col in feature_cols:\n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(0)\n",
    "        \n",
    "        self.feature_engineered_df = df\n",
    "        self.feature_columns = feature_cols\n",
    "        \n",
    "        print(f\"   ✅ Enhanced feature engineering complete\")\n",
    "        print(f\"   📊 Total features: {len(feature_cols)}\")\n",
    "        print(f\"   📋 Records: {len(df):,}\")\n",
    "        \n",
    "        # Show new feature categories\n",
    "        marketing_features = [col for col in feature_cols if any(keyword in col for keyword in ['marketing', '_log', '_squared', '_efficiency'])]\n",
    "        q4_features = [col for col in feature_cols if any(keyword in col for keyword in ['q4', 'holiday', 'black_friday', 'christmas'])]\n",
    "        prophet_features = [col for col in feature_cols if 'prophet' in col]\n",
    "        \n",
    "        print(f\"   💰 Marketing transformation features: {len(marketing_features)}\")\n",
    "        print(f\"   🎄 Q4/Holiday features: {len(q4_features)}\")\n",
    "        print(f\"   📈 Prophet-derived features: {len(prophet_features)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def train_prophet_models(self, df):\n",
    "        \"\"\"Train Prophet models for each SKU\"\"\"\n",
    "        print(\"\\n📈 TRAINING PROPHET MODELS\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        if not PROPHET_AVAILABLE:\n",
    "            print(\"❌ Prophet not available. Skipping Prophet models.\")\n",
    "            return {}\n",
    "        \n",
    "        prophet_results = {}\n",
    "        \n",
    "        # Get unique SKUs (limit to top SKUs for computational efficiency)\n",
    "        sku_revenue = df.groupby('sku')['actual_demand'].sum().sort_values(ascending=False)\n",
    "        top_skus = sku_revenue.head(10).index  # Train Prophet on top 10 SKUs\n",
    "        \n",
    "        print(f\"🎯 Training Prophet on top {len(top_skus)} SKUs by revenue\")\n",
    "        \n",
    "        for sku in top_skus:\n",
    "            sku_data = df[df['sku'] == sku].copy()\n",
    "            \n",
    "            if len(sku_data) < 30:  # Need sufficient data for Prophet\n",
    "                print(f\"   ⚠️ Skipping {sku}: insufficient data ({len(sku_data)} records)\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare Prophet data\n",
    "                prophet_data = sku_data[['order_date', 'actual_demand']].copy()\n",
    "                prophet_data.columns = ['ds', 'y']\n",
    "                prophet_data = prophet_data.sort_values('ds')\n",
    "                \n",
    "                # Create Prophet model with enhanced seasonality\n",
    "                model = Prophet(\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False,\n",
    "                    seasonality_mode='multiplicative',\n",
    "                    changepoint_prior_scale=0.05,\n",
    "                    interval_width=0.8\n",
    "                )\n",
    "                \n",
    "                # Add Q4 seasonality\n",
    "                model.add_seasonality(\n",
    "                    name='quarterly',\n",
    "                    period=91.25,\n",
    "                    fourier_order=3\n",
    "                )\n",
    "                \n",
    "                # Add marketing spend as regressor if available\n",
    "                marketing_cols = [col for col in sku_data.columns if 'marketing' in col and not col.endswith('_log')]\n",
    "                if marketing_cols:\n",
    "                    main_marketing_col = marketing_cols[0]\n",
    "                    prophet_data[main_marketing_col] = sku_data[main_marketing_col].values\n",
    "                    model.add_regressor(main_marketing_col)\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"   🔄 Training Prophet for {sku}...\")\n",
    "                model.fit(prophet_data)\n",
    "                \n",
    "                # Make predictions\n",
    "                future = model.make_future_dataframe(periods=0)\n",
    "                if marketing_cols:\n",
    "                    future[main_marketing_col] = prophet_data[main_marketing_col]\n",
    "                \n",
    "                forecast = model.predict(future)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                actual = prophet_data['y'].values\n",
    "                predicted = forecast['yhat'].values\n",
    "                \n",
    "                mae = mean_absolute_error(actual, predicted)\n",
    "                \n",
    "                prophet_results[sku] = {\n",
    "                    'model': model,\n",
    "                    'forecast': forecast,\n",
    "                    'mae': mae,\n",
    "                    'predictions': predicted,\n",
    "                    'marketing_regressors': marketing_cols\n",
    "                }\n",
    "                \n",
    "                print(f\"   ✅ {sku}: MAE ${mae:,.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error training Prophet for {sku}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        self.prophet_models = prophet_results\n",
    "        print(f\"\\n✅ Prophet training complete: {len(prophet_results)} models trained\")\n",
    "        \n",
    "        return prophet_results\n",
    "    \n",
    "    def train_enhanced_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train enhanced models with marketing transformations\"\"\"\n",
    "        \n",
    "        model_formulas = {\n",
    "            'Marketing Log Model': \"\"\"\n",
    "            💰 MARKETING LOG TRANSFORMATION MODEL:\n",
    "            Demand = β₀ + β₁×log(marketing_spend) + β₂×seasonality + β₃×customer_features + ε\n",
    "            \n",
    "            Focus: Captures diminishing returns from marketing spend\n",
    "            Theory: Each additional dollar has decreasing marginal impact\n",
    "            \"\"\",\n",
    "            \n",
    "            'Marketing Quadratic Model': \"\"\"\n",
    "            📈 MARKETING QUADRATIC MODEL:\n",
    "            Demand = β₀ + β₁×marketing + β₂×marketing² + β₃×other_features + ε\n",
    "            \n",
    "            Focus: Non-linear marketing response curve\n",
    "            Theory: Can model both increasing and decreasing returns\n",
    "            \"\"\",\n",
    "            \n",
    "            'Log-Log Marketing Model': \"\"\"\n",
    "            📊 LOG-LOG MARKETING MODEL:\n",
    "            log(Demand) = β₀ + β₁×log(marketing) + β₂×log(other_features) + ε\n",
    "            \n",
    "            Focus: Elasticity interpretation - β coefficients are elasticities\n",
    "            Theory: Percent change in marketing → percent change in demand\n",
    "            \"\"\",\n",
    "            \n",
    "            'Q4 Enhanced Model': \"\"\"\n",
    "            🎄 Q4 ENHANCED SEASONALITY MODEL:\n",
    "            Demand = β₀ + β₁×base_features + β₂×is_black_friday×5 + β₃×holiday_intensity×3 + \n",
    "                     β₄×days_to_christmas + β₅×q4_marketing_interaction + ε\n",
    "            \n",
    "            Focus: Captures Q4 shopping surge and holiday patterns\n",
    "            Weights: Black Friday gets 5x multiplier, holiday intensity 3x\n",
    "            \"\"\",\n",
    "            \n",
    "            'Prophet-Enhanced Linear': \"\"\"\n",
    "            📈 PROPHET-ENHANCED LINEAR MODEL:\n",
    "            Demand = β₀ + β₁×prophet_trend + β₂×prophet_seasonality + β₃×marketing_log + \n",
    "                     β₄×customer_features + ε\n",
    "            \n",
    "            Focus: Combines Prophet's time series insights with regression\n",
    "            \"\"\",\n",
    "            \n",
    "            'Hybrid Marketing Model': \"\"\"\n",
    "            🔄 HYBRID MARKETING RESPONSE MODEL:\n",
    "            Demand = β₀ + f(marketing_spend) + g(seasonality) + h(customer_mix)\n",
    "            \n",
    "            Where:\n",
    "            f(marketing) = β₁×log(marketing) + β₂×marketing_efficiency\n",
    "            g(seasonality) = β₃×q4_multiplier + β₄×prophet_trend\n",
    "            h(customer) = β₅×loyalty_score + β₆×new_customer_ratio\n",
    "            \n",
    "            Focus: Sophisticated marketing response with diminishing returns\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        models_to_train = {}\n",
    "        \n",
    "        # 1. Marketing Log Model\n",
    "        marketing_log_features = []\n",
    "        marketing_log_features.extend([col for col in X_train.columns if '_log' in col and 'marketing' in col])\n",
    "        marketing_log_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                                     ['month_sin', 'month_cos', 'customer', 'retention'])])\n",
    "        marketing_log_features = [f for f in marketing_log_features if f in X_train.columns][:15]  # Limit features\n",
    "        \n",
    "        models_to_train['Marketing Log Model'] = (LinearRegression(), marketing_log_features, False)\n",
    "        \n",
    "        # 2. Marketing Quadratic Model  \n",
    "        marketing_quad_features = []\n",
    "        marketing_quad_features.extend([col for col in X_train.columns if 'marketing' in col and ('_squared' in col or not any(trans in col for trans in ['_log', '_sqrt']))])\n",
    "        marketing_quad_features.extend([col for col in X_train.columns if 'seasonality' in col or 'month' in col])\n",
    "        marketing_quad_features = [f for f in marketing_quad_features if f in X_train.columns][:15]\n",
    "        \n",
    "        models_to_train['Marketing Quadratic Model'] = (LinearRegression(), marketing_quad_features, False)\n",
    "        \n",
    "        # 3. Q4 Enhanced Model\n",
    "        q4_features = []\n",
    "        q4_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                          ['q4', 'holiday', 'black_friday', 'christmas', 'december', 'november'])])\n",
    "        q4_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                          ['marketing', 'customer', 'seasonality'])])\n",
    "        q4_features = [f for f in q4_features if f in X_train.columns][:20]\n",
    "        \n",
    "        models_to_train['Q4 Enhanced Model'] = (LinearRegression(), q4_features, False)\n",
    "        \n",
    "        # 4. Prophet-Enhanced Linear\n",
    "        prophet_features = []\n",
    "        prophet_features.extend([col for col in X_train.columns if 'prophet' in col])\n",
    "        prophet_features.extend([col for col in X_train.columns if '_log' in col and 'marketing' in col])\n",
    "        prophet_features.extend([col for col in X_train.columns if 'customer' in col])\n",
    "        prophet_features = [f for f in prophet_features if f in X_train.columns][:15]\n",
    "        \n",
    "        models_to_train['Prophet-Enhanced Linear'] = (LinearRegression(), prophet_features, False)\n",
    "        \n",
    "        # 5. Hybrid Marketing Model (uses feature weighting)\n",
    "        hybrid_features = []\n",
    "        hybrid_features.extend([col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                              ['marketing_log', 'marketing_efficiency', 'q4', 'prophet_trend', 'customer_loyalty', 'new_customer_ratio'])])\n",
    "        hybrid_features = [f for f in hybrid_features if f in X_train.columns][:20]\n",
    "        \n",
    "        models_to_train['Hybrid Marketing Model'] = (LinearRegression(), hybrid_features, False)\n",
    "        \n",
    "        # 6. Enhanced Elastic Net with all marketing features\n",
    "        models_to_train['Enhanced Elastic Net'] = (ElasticNet(alpha=0.3, l1_ratio=0.5), None, True)\n",
    "        \n",
    "        # 7. Enhanced Random Forest\n",
    "        models_to_train['Enhanced Random Forest'] = (RandomForestRegressor(n_estimators=300, max_depth=20, \n",
    "                                                                          min_samples_split=5, random_state=42), None, False)\n",
    "        \n",
    "        # 8. Enhanced Gradient Boosting\n",
    "        models_to_train['Enhanced Gradient Boosting'] = (GradientBoostingRegressor(n_estimators=300, learning_rate=0.08,\n",
    "                                                                                   max_depth=8, random_state=42), None, False)\n",
    "        \n",
    "        # For log-log model, we need to transform the target variable\n",
    "        if 'actual_demand_log' in self.feature_engineered_df.columns:\n",
    "            # Get log-transformed target for the same indices as test set\n",
    "            y_train_log = None\n",
    "            y_test_log = None\n",
    "            \n",
    "            # Find the log target values corresponding to our train/test split\n",
    "            df_sorted = self.feature_engineered_df.sort_values('order_date')\n",
    "            split_point = int(len(df_sorted) * 0.8)\n",
    "            \n",
    "            if 'actual_demand_log' in df_sorted.columns:\n",
    "                y_train_log = df_sorted.iloc[:split_point]['actual_demand_log'].values\n",
    "                y_test_log = df_sorted.iloc[split_point:]['actual_demand_log'].values\n",
    "                \n",
    "                if len(y_train_log) == len(y_train) and len(y_test_log) == len(y_test):\n",
    "                    # Log-log model features\n",
    "                    loglog_features = [col for col in X_train.columns if any(keyword in col for keyword in \n",
    "                                     ['_log', 'loglog_', 'marketing', 'prophet'])]\n",
    "                    loglog_features = [f for f in loglog_features if f in X_train.columns][:15]\n",
    "                    \n",
    "                    models_to_train['Log-Log Marketing Model'] = (LinearRegression(), loglog_features, False, y_train_log, y_test_log)\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        print(\"🎯 TRAINING ENHANCED MODELS WITH MARKETING TRANSFORMATIONS:\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for model_name, model_config in models_to_train.items():\n",
    "            print(f\"\\n🔄 Training {model_name}...\")\n",
    "            \n",
    "            if len(model_config) == 5:  # Log-log model with custom target\n",
    "                model, features, needs_scaling, y_train_custom, y_test_custom = model_config\n",
    "            else:\n",
    "                model, features, needs_scaling = model_config\n",
    "                y_train_custom, y_test_custom = y_train, y_test\n",
    "            \n",
    "            # Print formula\n",
    "            if model_name in model_formulas:\n",
    "                print(f\"📋 {model_formulas[model_name]}\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                if features is None:  # Use all features\n",
    "                    X_train_model = X_train\n",
    "                    X_test_model = X_test\n",
    "                    features_used = X_train.columns.tolist()\n",
    "                else:  # Use specific features\n",
    "                    available_features = [f for f in features if f in X_train.columns]\n",
    "                    if not available_features:\n",
    "                        print(f\"   ❌ No valid features found for {model_name}\")\n",
    "                        continue\n",
    "                    X_train_model = X_train[available_features]\n",
    "                    X_test_model = X_test[available_features]\n",
    "                    features_used = available_features\n",
    "                \n",
    "                # Scale if needed\n",
    "                if needs_scaling:\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_model = scaler.fit_transform(X_train_model)\n",
    "                    X_test_model = scaler.transform(X_test_model)\n",
    "                \n",
    "                # Train model\n",
    "                model.fit(X_train_model, y_train_custom)\n",
    "                predictions = model.predict(X_test_model)\n",
    "                \n",
    "                # For log-log model, transform predictions back to original scale\n",
    "                if model_name == 'Log-Log Marketing Model' and len(model_config) == 5:\n",
    "                    predictions = np.exp(predictions)  # Transform back from log scale\n",
    "                    # Calculate metrics against original scale\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    mape = mean_absolute_percentage_error(y_test, predictions) * 100\n",
    "                else:\n",
    "                    # Standard metrics\n",
    "                    mae = mean_absolute_error(y_test_custom, predictions)\n",
    "                    mape = mean_absolute_percentage_error(y_test_custom, predictions) * 100\n",
    "                \n",
    "                model_results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': predictions,\n",
    "                    'mae': mae,\n",
    "                    'mape': mape,\n",
    "                    'features_used': features_used,\n",
    "                    'formula': model_formulas.get(model_name, \"No formula defined\"),\n",
    "                    'scaler': scaler if needs_scaling else None\n",
    "                }\n",
    "                \n",
    "                print(f\"   ✅ MAE: ${mae:,.2f}, MAPE: {mape:.1f}%\")\n",
    "                print(f\"   📊 Features used: {len(features_used)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error training {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return model_results\n",
    "    \n",
    "    def evaluate_enhanced_models(self):\n",
    "        \"\"\"Train and evaluate all enhanced models including Prophet\"\"\"\n",
    "        print(\"\\n🤖 ENHANCED MODEL TRAINING & EVALUATION\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if self.feature_engineered_df is None:\n",
    "            self.create_enhanced_feature_dataset()\n",
    "        \n",
    "        df = self.feature_engineered_df.copy()\n",
    "        \n",
    "        # Train Prophet models first\n",
    "        prophet_results = self.train_prophet_models(df)\n",
    "        \n",
    "        # Prepare features and target for ML models\n",
    "        X = df[self.feature_columns]\n",
    "        y = df['actual_demand']\n",
    "        \n",
    "        # Time series split for validation\n",
    "        df_sorted = df.sort_values('order_date')\n",
    "        split_point = int(len(df_sorted) * 0.8)\n",
    "        \n",
    "        train_data = df_sorted.iloc[:split_point]\n",
    "        test_data = df_sorted.iloc[split_point:]\n",
    "        \n",
    "        X_train = train_data[self.feature_columns]\n",
    "        y_train = train_data['actual_demand']\n",
    "        X_test = test_data[self.feature_columns]\n",
    "        y_test = test_data['actual_demand']\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = mean_absolute_error(test_data['actual_demand'], test_data['bm_demand'])\n",
    "        benchmark_mape = mean_absolute_percentage_error(test_data['actual_demand'], test_data['bm_demand']) * 100\n",
    "        \n",
    "        print(f\"📊 Training Data: {len(train_data):,} records\")\n",
    "        print(f\"📊 Test Data: {len(test_data):,} records\")\n",
    "        print(f\"🎯 Benchmark MAE: ${benchmark_mae:,.2f}\")\n",
    "        print(f\"🎯 Benchmark MAPE: {benchmark_mape:.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # Train enhanced ML models\n",
    "        model_results = self.train_enhanced_models(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Add Prophet results if available\n",
    "        if prophet_results:\n",
    "            print(\"\\n📈 PROPHET MODEL RESULTS:\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Aggregate Prophet predictions for comparison\n",
    "            prophet_mae_scores = [result['mae'] for result in prophet_results.values()]\n",
    "            avg_prophet_mae = np.mean(prophet_mae_scores)\n",
    "            \n",
    "            print(f\"Prophet Average MAE: ${avg_prophet_mae:,.2f}\")\n",
    "            print(f\"Prophet models trained: {len(prophet_results)}\")\n",
    "            \n",
    "            # Add aggregated Prophet to model results for comparison\n",
    "            model_results['Prophet Ensemble'] = {\n",
    "                'mae': avg_prophet_mae,\n",
    "                'mape': 0,  # Not calculated for ensemble\n",
    "                'features_used': ['time_series_components'],\n",
    "                'formula': 'Prophet time series decomposition with seasonality',\n",
    "                'predictions': np.full(len(y_test), avg_prophet_mae)  # Placeholder\n",
    "            }\n",
    "        \n",
    "        # Calculate improvements vs benchmark\n",
    "        print(\"\\n📈 ENHANCED MODEL PERFORMANCE vs BENCHMARK:\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        # Sort models by performance\n",
    "        model_performance = []\n",
    "        for model_name, results in model_results.items():\n",
    "            mae = results['mae']\n",
    "            mae_improvement = benchmark_mae - mae\n",
    "            mae_improvement_pct = (mae_improvement / benchmark_mae) * 100\n",
    "            model_performance.append((model_name, mae_improvement_pct, mae))\n",
    "        \n",
    "        model_performance.sort(key=lambda x: x[1], reverse=True)  # Sort by improvement %\n",
    "        \n",
    "        print(\"🏆 MODEL RANKING (by improvement %):\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for rank, (model_name, improvement_pct, mae) in enumerate(model_performance, 1):\n",
    "            mape = model_results[model_name].get('mape', 0)\n",
    "            \n",
    "            status = \"✅ BEATS BENCHMARK\" if improvement_pct > 0 else \"❌ UNDERPERFORMS\"\n",
    "            tier = \"🥇\" if rank <= 3 else \"🥈\" if rank <= 6 else \"🥉\"\n",
    "            \n",
    "            print(f\"{tier} #{rank:2d}. {model_name}\")\n",
    "            print(f\"      MAE: ${mae:,.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "            if mape > 0:\n",
    "                print(f\"      MAPE: {mape:.1f}%\")\n",
    "            print(\"\")\n",
    "            \n",
    "            # Add improvement metrics to results\n",
    "            model_results[model_name]['mae_improvement'] = benchmark_mae - mae\n",
    "            model_results[model_name]['mae_improvement_pct'] = improvement_pct\n",
    "        \n",
    "        # Marketing transformation analysis\n",
    "        self.analyze_marketing_transformations(model_results, X_test, y_test)\n",
    "        \n",
    "        self.models = model_results\n",
    "        \n",
    "        # Add enhanced predictions to test data for comprehensive analysis\n",
    "        test_data = test_data.copy()\n",
    "        for model_name, results in model_results.items():\n",
    "            if 'predictions' in results and len(results['predictions']) == len(test_data):\n",
    "                test_data[f'{model_name}_pred'] = results['predictions']\n",
    "        \n",
    "        return model_results, test_data\n",
    "    \n",
    "    def analyze_marketing_transformations(self, model_results, X_test, y_test):\n",
    "        \"\"\"Analyze the effectiveness of different marketing transformations\"\"\"\n",
    "        print(\"\\n💰 MARKETING TRANSFORMATION EFFECTIVENESS ANALYSIS\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        marketing_models = {\n",
    "            'Marketing Log Model': 'Log transformation (diminishing returns)',\n",
    "            'Marketing Quadratic Model': 'Quadratic transformation (non-linear response)',\n",
    "            'Log-Log Marketing Model': 'Log-log transformation (elasticity interpretation)',\n",
    "            'Hybrid Marketing Model': 'Hybrid approach (multiple transformations)'\n",
    "        }\n",
    "        \n",
    "        best_marketing_model = None\n",
    "        best_improvement = -float('inf')\n",
    "        \n",
    "        print(\"📊 MARKETING MODEL COMPARISON:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        for model_name, description in marketing_models.items():\n",
    "            if model_name in model_results:\n",
    "                improvement = model_results[model_name].get('mae_improvement_pct', 0)\n",
    "                mae = model_results[model_name]['mae']\n",
    "                \n",
    "                print(f\"{model_name}:\")\n",
    "                print(f\"   Description: {description}\")\n",
    "                print(f\"   Performance: {improvement:+.1f}% improvement (MAE: ${mae:,.2f})\")\n",
    "                \n",
    "                if improvement > best_improvement:\n",
    "                    best_improvement = improvement\n",
    "                    best_marketing_model = model_name\n",
    "                \n",
    "                # Show top marketing features if available\n",
    "                features_used = model_results[model_name].get('features_used', [])\n",
    "                marketing_features = [f for f in features_used if any(keyword in f for keyword in \n",
    "                                    ['marketing', '_log', '_squared', '_efficiency'])]\n",
    "                \n",
    "                if marketing_features:\n",
    "                    print(f\"   Key marketing features: {', '.join(marketing_features[:5])}\")\n",
    "                print(\"\")\n",
    "        \n",
    "        if best_marketing_model:\n",
    "            print(f\"🏆 BEST MARKETING TRANSFORMATION: {best_marketing_model}\")\n",
    "            print(f\"   Achieved {best_improvement:+.1f}% improvement over benchmark\")\n",
    "            print(f\"   This suggests {marketing_models[best_marketing_model].lower()} works best for your data\")\n",
    "        \n",
    "        # Q4 effectiveness analysis\n",
    "        print(\"\\n🎄 Q4/HOLIDAY SEASONALITY EFFECTIVENESS:\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        q4_model = 'Q4 Enhanced Model'\n",
    "        if q4_model in model_results:\n",
    "            q4_improvement = model_results[q4_model].get('mae_improvement_pct', 0)\n",
    "            q4_mae = model_results[q4_model]['mae']\n",
    "            \n",
    "            print(f\"Q4 Enhanced Model Performance: {q4_improvement:+.1f}% improvement\")\n",
    "            print(f\"MAE: ${q4_mae:,.2f}\")\n",
    "            \n",
    "            if q4_improvement > 5:\n",
    "                print(\"✅ Strong Q4 seasonality effects detected - holiday features are valuable\")\n",
    "            elif q4_improvement > 0:\n",
    "                print(\"🟡 Moderate Q4 effects - some holiday impact present\")\n",
    "            else:\n",
    "                print(\"❌ Weak Q4 effects - holiday features may not be significant for your data\")\n",
    "        \n",
    "        # Prophet effectiveness\n",
    "        print(\"\\n📈 PROPHET TIME SERIES EFFECTIVENESS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        prophet_models = [name for name in model_results.keys() if 'Prophet' in name]\n",
    "        if prophet_models:\n",
    "            for prophet_model in prophet_models:\n",
    "                prophet_improvement = model_results[prophet_model].get('mae_improvement_pct', 0)\n",
    "                prophet_mae = model_results[prophet_model]['mae']\n",
    "                \n",
    "                print(f\"{prophet_model}: {prophet_improvement:+.1f}% improvement (MAE: ${prophet_mae:,.2f})\")\n",
    "                \n",
    "                if prophet_improvement > 10:\n",
    "                    print(\"✅ Prophet captures strong time series patterns\")\n",
    "                elif prophet_improvement > 0:\n",
    "                    print(\"🟡 Prophet provides moderate time series insights\")\n",
    "                else:\n",
    "                    print(\"❌ Prophet may be overfitting or unsuitable for this data\")\n",
    "        else:\n",
    "            print(\"⚠️ No Prophet models available for analysis\")\n",
    "    \n",
    "    # Include all the existing methods from the original class\n",
    "    def engineer_seasonality_features(self, df):\n",
    "        \"\"\"Feature Set 1: Advanced Seasonality Features\"\"\"\n",
    "        print(\"   🗓️ Engineering seasonality features...\")\n",
    "        \n",
    "        # Basic temporal features\n",
    "        df['month'] = df['order_date'].dt.month\n",
    "        df['quarter'] = df['order_date'].dt.quarter\n",
    "        df['day_of_week'] = df['order_date'].dt.dayofweek\n",
    "        df['day_of_month'] = df['order_date'].dt.day\n",
    "        df['week_of_year'] = df['order_date'].dt.isocalendar().week\n",
    "        \n",
    "        # Holiday proximity\n",
    "        df['is_month_end'] = (df['day_of_month'] >= 28).astype(int)\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df['is_q4'] = (df['quarter'] == 4).astype(int)\n",
    "        \n",
    "        # Cyclical encoding for periodic features\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_product_features(self, df):\n",
    "        \"\"\"Feature Set 2: Product Characteristics & Pricing\"\"\"\n",
    "        print(\"   🛍️ Engineering product features...\")\n",
    "        \n",
    "        if self.order_items_df is not None:\n",
    "            # Calculate product-level metrics\n",
    "            product_stats = self.order_items_df.groupby('product_name').agg({\n",
    "                'sku_gross_sales': ['mean', 'std', 'count'],\n",
    "                'quantity': ['mean', 'sum'],\n",
    "                'local_order_ts': ['min', 'max']\n",
    "            }).round(2)\n",
    "            \n",
    "            product_stats.columns = ['avg_price', 'price_volatility', 'total_orders', \n",
    "                                   'avg_quantity', 'total_quantity', 'first_sale', 'last_sale']\n",
    "            product_stats = product_stats.reset_index()\n",
    "            \n",
    "            # Product age and maturity\n",
    "            product_stats['first_sale'] = pd.to_datetime(product_stats['first_sale'])\n",
    "            product_stats['last_sale'] = pd.to_datetime(product_stats['last_sale'])\n",
    "            \n",
    "            # Calculate product age as of each order date\n",
    "            df = df.merge(product_stats[['product_name', 'avg_price', 'price_volatility', 'first_sale']], \n",
    "                         left_on='sku', right_on='product_name', how='left')\n",
    "            \n",
    "            df['product_age_days'] = (df['order_date'] - df['first_sale']).dt.days\n",
    "            df['product_age_days'] = df['product_age_days'].fillna(0).clip(lower=0)\n",
    "            \n",
    "            # Price positioning\n",
    "            df['is_premium'] = (df['avg_price'] > df['avg_price'].quantile(0.8)).astype(int)\n",
    "            df['is_budget'] = (df['avg_price'] < df['avg_price'].quantile(0.2)).astype(int)\n",
    "            \n",
    "        else:\n",
    "            # Fallback features\n",
    "            df['avg_price'] = 43\n",
    "            df['price_volatility'] = 0\n",
    "            df['product_age_days'] = 30\n",
    "            df['is_premium'] = 0\n",
    "            df['is_budget'] = 0\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_customer_lifecycle_features(self, df):\n",
    "        \"\"\"Feature Set 3: Customer Lifecycle & Behavior Features\"\"\"\n",
    "        print(\"   👥 Engineering customer lifecycle features...\")\n",
    "        \n",
    "        # Use existing retention rates from benchmark\n",
    "        retention_cols = [col for col in df.columns if 'returning_rate_' in col]\n",
    "        \n",
    "        if retention_cols:\n",
    "            # Customer lifecycle stage\n",
    "            df['avg_retention_1_3m'] = df[[col for col in retention_cols if '1m' in col or '2m' in col or '3m' in col]].mean(axis=1)\n",
    "            df['avg_retention_6_12m'] = df[[col for col in retention_cols if '6m' in col or '12m' in col]].mean(axis=1)\n",
    "            \n",
    "            # Customer loyalty score\n",
    "            df['customer_loyalty_score'] = (df['avg_retention_1_3m'] * 0.3 + df['avg_retention_6_12m'] * 0.7)\n",
    "            \n",
    "        else:\n",
    "            # Fallback\n",
    "            df['avg_retention_1_3m'] = 0.7\n",
    "            df['avg_retention_6_12m'] = 0.4\n",
    "            df['customer_loyalty_score'] = 0.5\n",
    "        \n",
    "        # Customer mix features\n",
    "        if 'new_customers' in df.columns and 'existing_customers' in df.columns:\n",
    "            df['total_customers_calc'] = df['new_customers'] + df['existing_customers']\n",
    "            df['new_customer_ratio'] = df['new_customers'] / (df['total_customers_calc'] + 1)\n",
    "            df['customer_mix_score'] = df['new_customer_ratio'] * 0.3 + (1 - df['new_customer_ratio']) * 0.7\n",
    "        else:\n",
    "            df['new_customer_ratio'] = 0.3\n",
    "            df['customer_mix_score'] = 0.6\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_cross_sku_features(self, df):\n",
    "        \"\"\"Feature Set 4: Cross-SKU Interaction Features\"\"\"\n",
    "        print(\"   🔗 Engineering cross-SKU interaction features...\")\n",
    "        \n",
    "        # Group by date to create market-level features\n",
    "        date_aggregates = df.groupby('order_date').agg({\n",
    "            'actual_demand': ['sum', 'mean', 'std', 'count'],\n",
    "        }).round(2)\n",
    "        \n",
    "        date_aggregates.columns = ['market_total_demand', 'market_avg_demand', 'market_demand_volatility', 'market_sku_count']\n",
    "        date_aggregates = date_aggregates.reset_index()\n",
    "        \n",
    "        # Merge back to main data\n",
    "        df = df.merge(date_aggregates, on='order_date', how='left')\n",
    "        \n",
    "        # Market share and relative positioning\n",
    "        df['market_share'] = df['actual_demand'] / (df['market_total_demand'] + 1)\n",
    "        df['demand_vs_market_avg'] = df['actual_demand'] / (df['market_avg_demand'] + 1)\n",
    "        df['is_top_sku_today'] = (df['actual_demand'] >= df['market_avg_demand'] * 2).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_lag_features(self, df):\n",
    "        \"\"\"Feature Set 5: Historical Lag Features (NO DATA LEAKAGE)\"\"\"\n",
    "        print(\"   📈 Engineering lag features...\")\n",
    "        \n",
    "        # Sort by SKU and date for lag calculations\n",
    "        df = df.sort_values(['sku', 'order_date'])\n",
    "        \n",
    "        # Create lag features for demand (shifted to avoid leakage)\n",
    "        for lag in [1, 3, 7]:\n",
    "            df[f'demand_lag_{lag}d'] = df.groupby('sku')['actual_demand'].shift(lag)\n",
    "        \n",
    "        # Rolling averages (shifted to avoid leakage)\n",
    "        for window in [3, 7, 14]:\n",
    "            df[f'demand_ma_{window}d'] = df.groupby('sku')['actual_demand'].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        # Fill NaN values with reasonable defaults\n",
    "        lag_cols = [col for col in df.columns if '_lag_' in col or '_ma_' in col]\n",
    "        for col in lag_cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_marketing_insights(self):\n",
    "        \"\"\"Extract insights about marketing effectiveness from trained models\"\"\"\n",
    "        print(\"\\n💡 MARKETING EFFECTIVENESS INSIGHTS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        insights = {}\n",
    "        \n",
    "        # Analyze feature importance for marketing features\n",
    "        tree_models = ['Enhanced Random Forest', 'Enhanced Gradient Boosting']\n",
    "        \n",
    "        for model_name in tree_models:\n",
    "            if model_name in self.models and hasattr(self.models[model_name]['model'], 'feature_importances_'):\n",
    "                model = self.models[model_name]['model']\n",
    "                features = self.models[model_name]['features_used']\n",
    "                \n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': features,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                # Extract marketing-related features\n",
    "                marketing_features = importance_df[\n",
    "                    importance_df['feature'].str.contains('marketing|spend|ad', case=False, na=False)\n",
    "                ]\n",
    "                \n",
    "                if not marketing_features.empty:\n",
    "                    print(f\"\\n🌳 {model_name.upper()} - TOP MARKETING FEATURES:\")\n",
    "                    for i, (_, row) in enumerate(marketing_features.head(5).iterrows(), 1):\n",
    "                        feature_type = \"Log transform\" if \"_log\" in row['feature'] else \\\n",
    "                                     \"Quadratic\" if \"_squared\" in row['feature'] else \\\n",
    "                                     \"Efficiency\" if \"_efficiency\" in row['feature'] else \\\n",
    "                                     \"Raw spend\"\n",
    "                        \n",
    "                        print(f\"   {i}. {row['feature']}: {row['importance']:.4f} ({feature_type})\")\n",
    "                    \n",
    "                    insights[model_name] = marketing_features.head(10)\n",
    "        \n",
    "        # Analyze coefficients for linear models\n",
    "        linear_models = ['Marketing Log Model', 'Marketing Quadratic Model', 'Log-Log Marketing Model']\n",
    "        \n",
    "        for model_name in linear_models:\n",
    "            if model_name in self.models:\n",
    "                model = self.models[model_name]['model']\n",
    "                features = self.models[model_name]['features_used']\n",
    "                \n",
    "                if hasattr(model, 'coef_'):\n",
    "                    coef_df = pd.DataFrame({\n",
    "                        'feature': features,\n",
    "                        'coefficient': model.coef_\n",
    "                    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "                    \n",
    "                    marketing_coefs = coef_df[\n",
    "                        coef_df['feature'].str.contains('marketing|spend|ad', case=False, na=False)\n",
    "                    ]\n",
    "                    \n",
    "                    if not marketing_coefs.empty:\n",
    "                        print(f\"\\n📊 {model_name.upper()} - MARKETING COEFFICIENTS:\")\n",
    "                        for _, row in marketing_coefs.head(5).iterrows():\n",
    "                            direction = \"📈 Positive\" if row['coefficient'] > 0 else \"📉 Negative\"\n",
    "                            print(f\"   {row['feature']}: {row['coefficient']:.4f} ({direction} impact)\")\n",
    "        \n",
    "        return insights\n",
    "\n",
    "\n",
    "# Enhanced Model Evaluation with Marketing Analysis\n",
    "class EnhancedMLModelEvaluationAnalysis:\n",
    "    def __init__(self, test_data, model_results, benchmark_col='bm_demand', actual_col='actual_demand'):\n",
    "        \"\"\"Enhanced analysis including marketing transformation effectiveness\"\"\"\n",
    "        print(\"\\n🔍 ENHANCED ML MODEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"🎯 Including: Marketing transformations, Prophet analysis, Q4 seasonality\")\n",
    "        print(\"=\"*75)\n",
    "        \n",
    "        self.test_data = test_data.copy()\n",
    "        self.model_results = model_results\n",
    "        self.benchmark_col = benchmark_col\n",
    "        self.actual_col = actual_col\n",
    "        \n",
    "        # Add ML model predictions to test data\n",
    "        self.add_ml_predictions_to_test_data()\n",
    "        self.prepare_analysis_data()\n",
    "    \n",
    "    def add_ml_predictions_to_test_data(self):\n",
    "        \"\"\"Add ML model predictions to test dataset\"\"\"\n",
    "        print(\"\\n📊 Adding enhanced ML predictions to test data...\")\n",
    "        \n",
    "        for model_name, results in self.model_results.items():\n",
    "            if 'predictions' in results:\n",
    "                predictions = results['predictions']\n",
    "                if len(predictions) == len(self.test_data):\n",
    "                    self.test_data[f'{model_name}_pred'] = predictions\n",
    "                    print(f\"   ✅ Added {model_name} predictions\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ {model_name} prediction length mismatch\")\n",
    "    \n",
    "    def prepare_analysis_data(self):\n",
    "        \"\"\"Prepare data for comprehensive analysis\"\"\"\n",
    "        print(\"\\n🔧 Preparing enhanced analysis data...\")\n",
    "        \n",
    "        # Add temporal columns if not present\n",
    "        if 'year_month' not in self.test_data.columns:\n",
    "            self.test_data['order_date'] = pd.to_datetime(self.test_data['order_date'])\n",
    "            self.test_data['year_month'] = self.test_data['order_date'].dt.to_period('M')\n",
    "        \n",
    "        # Calculate benchmark errors\n",
    "        self.test_data['benchmark_error'] = self.test_data[self.actual_col] - self.test_data[self.benchmark_col]\n",
    "        self.test_data['benchmark_abs_error'] = np.abs(self.test_data['benchmark_error'])\n",
    "        self.test_data['benchmark_abs_pct_error'] = np.abs(self.test_data['benchmark_error'] / self.test_data[self.actual_col] * 100)\n",
    "        \n",
    "        # Calculate ML model errors\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            self.test_data[f'{model_name}_error'] = self.test_data[self.actual_col] - self.test_data[pred_col]\n",
    "            self.test_data[f'{model_name}_abs_error'] = np.abs(self.test_data[f'{model_name}_error'])\n",
    "            self.test_data[f'{model_name}_abs_pct_error'] = np.abs(self.test_data[f'{model_name}_error'] / self.test_data[self.actual_col] * 100)\n",
    "        \n",
    "        print(f\"   ✅ Enhanced analysis data prepared: {len(self.test_data):,} records\")\n",
    "        print(f\"   📊 Enhanced ML models found: {len(ml_pred_cols)}\")\n",
    "    \n",
    "    def analyze_marketing_model_effectiveness(self):\n",
    "        \"\"\"Analyze effectiveness of different marketing transformation approaches\"\"\"\n",
    "        print(\"\\n💰 MARKETING TRANSFORMATION MODEL EFFECTIVENESS\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        marketing_models = [name for name in self.model_results.keys() if \n",
    "                          any(keyword in name for keyword in ['Marketing', 'Log-Log', 'Hybrid'])]\n",
    "        \n",
    "        if not marketing_models:\n",
    "            print(\"❌ No marketing transformation models found\")\n",
    "            return {}\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        \n",
    "        marketing_analysis = {}\n",
    "        \n",
    "        print(\"📊 MARKETING MODEL COMPARISON:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        for model_name in marketing_models:\n",
    "            if f'{model_name}_abs_error' in self.test_data.columns:\n",
    "                ml_mae = self.test_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement = ((benchmark_mae - ml_mae) / benchmark_mae) * 100\n",
    "                \n",
    "                # Analyze by marketing spend levels\n",
    "                if 'marketing_spend' in self.test_data.columns or any('marketing' in col for col in self.test_data.columns):\n",
    "                    # Find a marketing spend column\n",
    "                    marketing_col = None\n",
    "                    for col in self.test_data.columns:\n",
    "                        if 'marketing' in col.lower() and 'spend' in col.lower():\n",
    "                            marketing_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if marketing_col:\n",
    "                        # Analyze performance by marketing spend quintiles\n",
    "                        quintiles = pd.qcut(self.test_data[marketing_col], 5, labels=['Low', 'Low-Med', 'Medium', 'Med-High', 'High'])\n",
    "                        quintile_performance = {}\n",
    "                        \n",
    "                        for quintile in ['Low', 'Low-Med', 'Medium', 'Med-High', 'High']:\n",
    "                            quintile_data = self.test_data[quintiles == quintile]\n",
    "                            if len(quintile_data) > 0:\n",
    "                                quintile_mae = quintile_data[f'{model_name}_abs_error'].mean()\n",
    "                                quintile_benchmark = quintile_data['benchmark_abs_error'].mean()\n",
    "                                quintile_improvement = ((quintile_benchmark - quintile_mae) / quintile_benchmark) * 100\n",
    "                                quintile_performance[quintile] = quintile_improvement\n",
    "                \n",
    "                transformation_type = \"Log transformation\" if \"Log\" in model_name else \\\n",
    "                                    \"Quadratic transformation\" if \"Quadratic\" in model_name else \\\n",
    "                                    \"Log-log transformation\" if \"Log-Log\" in model_name else \\\n",
    "                                    \"Hybrid approach\"\n",
    "                \n",
    "                print(f\"{model_name}:\")\n",
    "                print(f\"   Transformation: {transformation_type}\")\n",
    "                print(f\"   Overall improvement: {improvement:+.1f}%\")\n",
    "                print(f\"   MAE: ${ml_mae:,.2f}\")\n",
    "                \n",
    "                if 'quintile_performance' in locals():\n",
    "                    print(f\"   Performance by marketing spend:\")\n",
    "                    for quintile, perf in quintile_performance.items():\n",
    "                        print(f\"     {quintile} spend: {perf:+.1f}%\")\n",
    "                \n",
    "                marketing_analysis[model_name] = {\n",
    "                    'improvement': improvement,\n",
    "                    'mae': ml_mae,\n",
    "                    'transformation_type': transformation_type,\n",
    "                    'quintile_performance': quintile_performance if 'quintile_performance' in locals() else {}\n",
    "                }\n",
    "                print(\"\")\n",
    "        \n",
    "        # Determine best marketing approach\n",
    "        if marketing_analysis:\n",
    "            best_marketing_model = max(marketing_analysis.keys(), \n",
    "                                     key=lambda x: marketing_analysis[x]['improvement'])\n",
    "            \n",
    "            print(f\"🏆 BEST MARKETING TRANSFORMATION:\")\n",
    "            print(f\"   Model: {best_marketing_model}\")\n",
    "            print(f\"   Improvement: {marketing_analysis[best_marketing_model]['improvement']:+.1f}%\")\n",
    "            print(f\"   Approach: {marketing_analysis[best_marketing_model]['transformation_type']}\")\n",
    "        \n",
    "        return marketing_analysis\n",
    "    \n",
    "    def analyze_q4_seasonality_effectiveness(self):\n",
    "        \"\"\"Analyze Q4 and holiday seasonality model effectiveness\"\"\"\n",
    "        print(\"\\n🎄 Q4 SEASONALITY EFFECTIVENESS ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Filter test data for Q4 months\n",
    "        q4_data = self.test_data[self.test_data['order_date'].dt.quarter == 4]\n",
    "        non_q4_data = self.test_data[self.test_data['order_date'].dt.quarter != 4]\n",
    "        \n",
    "        if len(q4_data) == 0:\n",
    "            print(\"❌ No Q4 data available for analysis\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"📊 Q4 Data: {len(q4_data):,} records\")\n",
    "        print(f\"📊 Non-Q4 Data: {len(non_q4_data):,} records\")\n",
    "        print(\"\")\n",
    "        \n",
    "        q4_analysis = {}\n",
    "        \n",
    "        # Find Q4-enhanced models\n",
    "        q4_models = [name for name in self.model_results.keys() if 'Q4' in name or 'Enhanced' in name]\n",
    "        \n",
    "        benchmark_mae_q4 = q4_data['benchmark_abs_error'].mean()\n",
    "        benchmark_mae_non_q4 = non_q4_data['benchmark_abs_error'].mean()\n",
    "        \n",
    "        print(\"🎯 BENCHMARK PERFORMANCE:\")\n",
    "        print(f\"   Q4 MAE: ${benchmark_mae_q4:,.2f}\")\n",
    "        print(f\"   Non-Q4 MAE: ${benchmark_mae_non_q4:,.2f}\")\n",
    "        print(f\"   Q4 vs Non-Q4 difference: {((benchmark_mae_q4 - benchmark_mae_non_q4) / benchmark_mae_non_q4) * 100:+.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # Analyze all models for Q4 effectiveness\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        print(\"🤖 ML MODEL Q4 vs NON-Q4 PERFORMANCE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            # Q4 performance\n",
    "            q4_ml_mae = q4_data[f'{model_name}_abs_error'].mean()\n",
    "            q4_improvement = ((benchmark_mae_q4 - q4_ml_mae) / benchmark_mae_q4) * 100\n",
    "            \n",
    "            # Non-Q4 performance\n",
    "            non_q4_ml_mae = non_q4_data[f'{model_name}_abs_error'].mean()\n",
    "            non_q4_improvement = ((benchmark_mae_non_q4 - non_q4_ml_mae) / benchmark_mae_non_q4) * 100\n",
    "            \n",
    "            # Q4 specialization score (how much better in Q4 vs non-Q4)\n",
    "            q4_specialization = q4_improvement - non_q4_improvement\n",
    "            \n",
    "            print(f\"{model_name}:\")\n",
    "            print(f\"   Q4 improvement: {q4_improvement:+.1f}% (MAE: ${q4_ml_mae:,.2f})\")\n",
    "            print(f\"   Non-Q4 improvement: {non_q4_improvement:+.1f}% (MAE: ${non_q4_ml_mae:,.2f})\")\n",
    "            print(f\"   Q4 specialization: {q4_specialization:+.1f}% {'✅' if q4_specialization > 0 else '❌'}\")\n",
    "            print(\"\")\n",
    "            \n",
    "            q4_analysis[model_name] = {\n",
    "                'q4_improvement': q4_improvement,\n",
    "                'non_q4_improvement': non_q4_improvement,\n",
    "                'q4_specialization': q4_specialization,\n",
    "                'q4_mae': q4_ml_mae,\n",
    "                'non_q4_mae': non_q4_ml_mae\n",
    "            }\n",
    "        \n",
    "        # Find best Q4 specialist\n",
    "        if q4_analysis:\n",
    "            best_q4_model = max(q4_analysis.keys(), \n",
    "                               key=lambda x: q4_analysis[x]['q4_specialization'])\n",
    "            \n",
    "            print(f\"🏆 BEST Q4 SPECIALIST MODEL:\")\n",
    "            print(f\"   Model: {best_q4_model}\")\n",
    "            print(f\"   Q4 specialization score: {q4_analysis[best_q4_model]['q4_specialization']:+.1f}%\")\n",
    "            \n",
    "            if q4_analysis[best_q4_model]['q4_specialization'] > 5:\n",
    "                print(\"   ✅ Strong Q4 seasonality captured\")\n",
    "            elif q4_analysis[best_q4_model]['q4_specialization'] > 0:\n",
    "                print(\"   🟡 Moderate Q4 seasonality effects\")\n",
    "            else:\n",
    "                print(\"   ❌ No significant Q4 specialization\")\n",
    "        \n",
    "        return q4_analysis\n",
    "    \n",
    "    def analyze_prophet_effectiveness(self):\n",
    "        \"\"\"Analyze Prophet model effectiveness and time series insights\"\"\"\n",
    "        print(\"\\n📈 PROPHET MODEL EFFECTIVENESS ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        prophet_models = [name for name in self.model_results.keys() if 'Prophet' in name]\n",
    "        \n",
    "        if not prophet_models:\n",
    "            print(\"❌ No Prophet models found for analysis\")\n",
    "            return {}\n",
    "        \n",
    "        prophet_analysis = {}\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        \n",
    "        for prophet_model in prophet_models:\n",
    "            if f'{prophet_model}_abs_error' in self.test_data.columns:\n",
    "                prophet_mae = self.test_data[f'{prophet_model}_abs_error'].mean()\n",
    "                improvement = ((benchmark_mae - prophet_mae) / benchmark_mae) * 100\n",
    "                \n",
    "                print(f\"{prophet_model}:\")\n",
    "                print(f\"   Overall improvement: {improvement:+.1f}%\")\n",
    "                print(f\"   MAE: ${prophet_mae:,.2f}\")\n",
    "                \n",
    "                # Analyze Prophet performance by time periods\n",
    "                monthly_performance = []\n",
    "                \n",
    "                for month in sorted(self.test_data['year_month'].unique()):\n",
    "                    month_data = self.test_data[self.test_data['year_month'] == month]\n",
    "                    \n",
    "                    if len(month_data) > 0:\n",
    "                        month_benchmark = month_data['benchmark_abs_error'].mean()\n",
    "                        month_prophet = month_data[f'{prophet_model}_abs_error'].mean()\n",
    "                        month_improvement = ((month_benchmark - month_prophet) / month_benchmark) * 100\n",
    "                        \n",
    "                        monthly_performance.append({\n",
    "                            'month': month,\n",
    "                            'improvement': month_improvement,\n",
    "                            'mae': month_prophet\n",
    "                        })\n",
    "                \n",
    "                # Calculate consistency\n",
    "                improvements = [perf['improvement'] for perf in monthly_performance]\n",
    "                consistency_score = 100 - np.std(improvements)  # Lower std = higher consistency\n",
    "                \n",
    "                print(f\"   Monthly consistency: {consistency_score:.1f}/100\")\n",
    "                print(f\"   Best month: {max(monthly_performance, key=lambda x: x['improvement'])['month']} \"\n",
    "                      f\"({max(improvements):+.1f}%)\")\n",
    "                print(f\"   Worst month: {min(monthly_performance, key=lambda x: x['improvement'])['month']} \"\n",
    "                      f\"({min(improvements):+.1f}%)\")\n",
    "                print(\"\")\n",
    "                \n",
    "                prophet_analysis[prophet_model] = {\n",
    "                    'overall_improvement': improvement,\n",
    "                    'mae': prophet_mae,\n",
    "                    'monthly_performance': monthly_performance,\n",
    "                    'consistency_score': consistency_score\n",
    "                }\n",
    "        \n",
    "        return prophet_analysis\n",
    "    \n",
    "    def run_enhanced_comprehensive_analysis(self):\n",
    "        \"\"\"Run all enhanced analyses\"\"\"\n",
    "        print(\"🚀 RUNNING ENHANCED COMPREHENSIVE ML MODEL ANALYSIS\")\n",
    "        print(\"=\"*65)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # Standard analyses\n",
    "            results['overall_accuracy'] = self.analyze_overall_accuracy()\n",
    "            results['weighted_accuracy'] = self.analyze_weighted_accuracy()\n",
    "            results['channel_performance'] = self.analyze_channel_performance()\n",
    "            results['top_sku_performance'] = self.analyze_top_sku_performance()\n",
    "            results['temporal_performance'] = self.analyze_temporal_performance()\n",
    "            \n",
    "            # Enhanced analyses\n",
    "            results['marketing_effectiveness'] = self.analyze_marketing_model_effectiveness()\n",
    "            results['q4_seasonality'] = self.analyze_q4_seasonality_effectiveness()\n",
    "            results['prophet_effectiveness'] = self.analyze_prophet_effectiveness()\n",
    "            \n",
    "            # Final enhanced recommendations\n",
    "            self.provide_enhanced_recommendations(results)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during enhanced analysis: {str(e)}\")\n",
    "            return results\n",
    "    \n",
    "    def provide_enhanced_recommendations(self, results):\n",
    "        \"\"\"Provide enhanced recommendations based on all analyses\"\"\"\n",
    "        print(\"\\n🎯 ENHANCED MODEL RECOMMENDATIONS\")\n",
    "        print(\"=\"*45)\n",
    "        \n",
    "        print(\"📊 MARKETING TRANSFORMATION RECOMMENDATIONS:\")\n",
    "        marketing_results = results.get('marketing_effectiveness', {})\n",
    "        if marketing_results:\n",
    "            best_marketing = max(marketing_results.keys(), \n",
    "                               key=lambda x: marketing_results[x]['improvement'])\n",
    "            print(f\"   🏆 Best marketing approach: {best_marketing}\")\n",
    "            print(f\"   📈 Improvement: {marketing_results[best_marketing]['improvement']:+.1f}%\")\n",
    "            print(f\"   💡 Strategy: {marketing_results[best_marketing]['transformation_type']}\")\n",
    "        else:\n",
    "            print(\"   ⚠️ No marketing transformation analysis available\")\n",
    "        \n",
    "        print(\"\\n🎄 Q4 SEASONALITY RECOMMENDATIONS:\")\n",
    "        q4_results = results.get('q4_seasonality', {})\n",
    "        if q4_results:\n",
    "            best_q4_specialist = max(q4_results.keys(), \n",
    "                                   key=lambda x: q4_results[x]['q4_specialization'])\n",
    "            specialization_score = q4_results[best_q4_specialist]['q4_specialization']\n",
    "            \n",
    "            print(f\"   🏆 Best Q4 model: {best_q4_specialist}\")\n",
    "            print(f\"   🎯 Q4 specialization: {specialization_score:+.1f}%\")\n",
    "            \n",
    "            if specialization_score > 5:\n",
    "                print(\"   ✅ Strong Q4 effects - implement separate holiday models\")\n",
    "            elif specialization_score > 0:\n",
    "                print(\"   🟡 Moderate Q4 effects - consider Q4 feature engineering\")\n",
    "            else:\n",
    "                print(\"   ❌ Weak Q4 effects - focus on other factors\")\n",
    "        \n",
    "        print(\"\\n📈 PROPHET RECOMMENDATIONS:\")\n",
    "        prophet_results = results.get('prophet_effectiveness', {})\n",
    "        if prophet_results:\n",
    "            best_prophet = max(prophet_results.keys(), \n",
    "                             key=lambda x: prophet_results[x]['overall_improvement'])\n",
    "            prophet_improvement = prophet_results[best_prophet]['overall_improvement']\n",
    "            consistency = prophet_results[best_prophet]['consistency_score']\n",
    "            \n",
    "            print(f\"   🏆 Best Prophet model: {best_prophet}\")\n",
    "            print(f\"   📈 Improvement: {prophet_improvement:+.1f}%\")\n",
    "            print(f\"   📊 Consistency: {consistency:.1f}/100\")\n",
    "            \n",
    "            if prophet_improvement > 10 and consistency > 70:\n",
    "                print(\"   ✅ Prophet highly effective - use for forecasting\")\n",
    "            elif prophet_improvement > 0:\n",
    "                print(\"   🟡 Prophet moderately effective - use as ensemble component\")\n",
    "            else:\n",
    "                print(\"   ❌ Prophet ineffective - stick to ML models\")\n",
    "        \n",
    "        print(\"\\n🏆 OVERALL STRATEGY RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Find overall best model\n",
    "        overall_results = results.get('overall_accuracy', {})\n",
    "        if overall_results:\n",
    "            # Remove benchmark from comparison\n",
    "            ml_results = {k: v for k, v in overall_results.items() if k != 'benchmark'}\n",
    "            \n",
    "            if ml_results:\n",
    "                best_overall = max(ml_results.keys(), \n",
    "                                 key=lambda x: ml_results[x].get('mae_improvement_pct', 0))\n",
    "                best_improvement = ml_results[best_overall].get('mae_improvement_pct', 0)\n",
    "                \n",
    "                print(f\"   🥇 Best overall model: {best_overall}\")\n",
    "                print(f\"   📈 Best improvement: {best_improvement:+.1f}%\")\n",
    "                \n",
    "                if best_improvement > 15:\n",
    "                    strategy = \"DEPLOY IMMEDIATELY\"\n",
    "                    confidence = \"HIGH\"\n",
    "                elif best_improvement > 5:\n",
    "                    strategy = \"PILOT DEPLOYMENT\"\n",
    "                    confidence = \"MEDIUM\"\n",
    "                else:\n",
    "                    strategy = \"CONTINUE DEVELOPMENT\"\n",
    "                    confidence = \"LOW\"\n",
    "                \n",
    "                print(f\"   🎯 Recommended strategy: {strategy}\")\n",
    "                print(f\"   🎪 Confidence level: {confidence}\")\n",
    "    \n",
    "    # Include all existing methods from the original MLModelEvaluationAnalysis class\n",
    "    def analyze_overall_accuracy(self):\n",
    "        \"\"\"1. Overall Accuracy Analysis\"\"\"\n",
    "        print(\"\\n📈 1. OVERALL ACCURACY ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        if not ml_pred_cols:\n",
    "            print(\"❌ No ML model predictions found\")\n",
    "            return {}\n",
    "        \n",
    "        accuracy_results = {}\n",
    "        \n",
    "        # Benchmark performance\n",
    "        benchmark_mae = self.test_data['benchmark_abs_error'].mean()\n",
    "        benchmark_mape = self.test_data['benchmark_abs_pct_error'].mean()\n",
    "        \n",
    "        print(f\"🎯 BENCHMARK PERFORMANCE:\")\n",
    "        print(f\"   MAE:  ${benchmark_mae:,.2f}\")\n",
    "        print(f\"   MAPE: {benchmark_mape:.1f}%\")\n",
    "        print(\"\")\n",
    "        \n",
    "        accuracy_results['benchmark'] = {'mae': benchmark_mae, 'mape': benchmark_mape}\n",
    "        \n",
    "        # ML model performance\n",
    "        print(f\"🤖 ML MODEL PERFORMANCE:\")\n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            ml_mae = self.test_data[f'{model_name}_abs_error'].mean()\n",
    "            ml_mape = self.test_data[f'{model_name}_abs_pct_error'].mean()\n",
    "            \n",
    "            mae_improvement = benchmark_mae - ml_mae\n",
    "            mae_improvement_pct = (mae_improvement / benchmark_mae) * 100\n",
    "            mape_improvement = benchmark_mape - ml_mape\n",
    "            \n",
    "            status = \"✅ BETTER\" if mae_improvement > 0 else \"❌ WORSE\"\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            print(f\"     MAE:  ${ml_mae:,.2f} (${mae_improvement:+,.2f}, {mae_improvement_pct:+.1f}%) {status}\")\n",
    "            print(f\"     MAPE: {ml_mape:.1f}% ({mape_improvement:+.1f}%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            accuracy_results[model_name] = {\n",
    "                'mae': ml_mae,\n",
    "                'mape': ml_mape,\n",
    "                'mae_improvement': mae_improvement,\n",
    "                'mae_improvement_pct': mae_improvement_pct,\n",
    "                'mape_improvement': mape_improvement\n",
    "            }\n",
    "        \n",
    "        return accuracy_results\n",
    "    \n",
    "    def analyze_weighted_accuracy(self):\n",
    "        \"\"\"2. Weighted Accuracy by Product Revenue\"\"\"\n",
    "        print(\"\\n💰 2. WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        if 'sku' not in self.test_data.columns:\n",
    "            print(\"❌ No SKU column found for product analysis\")\n",
    "            return {}\n",
    "        \n",
    "        # Calculate product weights by revenue\n",
    "        product_revenue = self.test_data.groupby('sku')[self.actual_col].sum().sort_values(ascending=False)\n",
    "        top_20_pct_threshold = product_revenue.quantile(0.8)\n",
    "        top_products = product_revenue[product_revenue >= top_20_pct_threshold].index\n",
    "        \n",
    "        top_product_data = self.test_data[self.test_data['sku'].isin(top_products)]\n",
    "        bottom_product_data = self.test_data[~self.test_data['sku'].isin(top_products)]\n",
    "        \n",
    "        print(f\"📊 PRODUCT SEGMENTATION:\")\n",
    "        print(f\"   Top 20% Products: {len(top_products)} SKUs\")\n",
    "        print(f\"   Bottom 80% Products: {len(product_revenue) - len(top_products)} SKUs\")\n",
    "        print(\"\")\n",
    "        \n",
    "        weighted_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for segment_name, segment_data in [('Top 20% Products', top_product_data), ('Bottom 80% Products', bottom_product_data)]:\n",
    "            print(f\"🎯 {segment_name.upper()}:\")\n",
    "            \n",
    "            bench_mae = segment_data['benchmark_abs_error'].mean()\n",
    "            print(f\"   Benchmark MAE: ${bench_mae:.2f}\")\n",
    "            \n",
    "            segment_results = {'benchmark': {'mae': bench_mae}}\n",
    "            \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = segment_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                status = \"✅\" if improvement_pct > 0 else \"❌\"\n",
    "                print(f\"   {model_name}: ${ml_mae:.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                segment_results[model_name] = {'mae': ml_mae, 'improvement_pct': improvement_pct}\n",
    "            \n",
    "            weighted_results[segment_name] = segment_results\n",
    "            print(\"\")\n",
    "        \n",
    "        return weighted_results\n",
    "    \n",
    "    def analyze_channel_performance(self):\n",
    "        \"\"\"3. Channel Performance Analysis\"\"\"\n",
    "        print(\"\\n📺 3. CHANNEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if 'channel' not in self.test_data.columns:\n",
    "            print(\"❌ No channel column found\")\n",
    "            return {}\n",
    "        \n",
    "        channel_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for channel in self.test_data['channel'].unique():\n",
    "            channel_data = self.test_data[self.test_data['channel'] == channel]\n",
    "            \n",
    "            print(f\"📻 {channel.upper()} CHANNEL:\")\n",
    "            print(f\"   Records: {len(channel_data):,}\")\n",
    "            \n",
    "            bench_mae = channel_data['benchmark_abs_error'].mean()\n",
    "            print(f\"   Benchmark MAE: ${bench_mae:.2f}\")\n",
    "            \n",
    "            channel_results[channel] = {'benchmark': {'mae': bench_mae}}\n",
    "            \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = channel_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                status = \"✅ BETTER\" if improvement_pct > 0 else \"❌ WORSE\"\n",
    "                print(f\"   {model_name}: ${ml_mae:.2f} ({improvement_pct:+.1f}%) {status}\")\n",
    "                \n",
    "                channel_results[channel][model_name] = {'mae': ml_mae, 'improvement_pct': improvement_pct}\n",
    "            \n",
    "            print(\"\")\n",
    "        \n",
    "        return channel_results\n",
    "    \n",
    "    def analyze_top_sku_performance(self):\n",
    "        \"\"\"4. Top SKU Performance Analysis\"\"\"\n",
    "        print(\"\\n🏆 4. TOP SKU PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get top 10 SKUs by revenue\n",
    "        sku_revenue = self.test_data.groupby('sku')[self.actual_col].sum().sort_values(ascending=False)\n",
    "        top_10_skus = sku_revenue.head(10).index\n",
    "        \n",
    "        print(f\"📊 TOP 10 SKUs by Revenue:\")\n",
    "        for i, sku in enumerate(top_10_skus, 1):\n",
    "            revenue = sku_revenue[sku]\n",
    "            pct_of_total = (revenue / sku_revenue.sum()) * 100\n",
    "            print(f\"   {i:2d}. {sku}: ${revenue:,.0f} ({pct_of_total:.1f}% of total)\")\n",
    "        print(\"\")\n",
    "        \n",
    "        sku_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        # Analyze each top SKU individually\n",
    "        for rank, sku in enumerate(top_10_skus, 1):\n",
    "            sku_data = self.test_data[self.test_data['sku'] == sku]\n",
    "            \n",
    "            if len(sku_data) < 5:\n",
    "                continue\n",
    "            \n",
    "            bench_mae = sku_data['benchmark_abs_error'].mean()\n",
    "            sku_results[sku] = {\n",
    "                'rank': rank,\n",
    "                'revenue': sku_revenue[sku],\n",
    "                'benchmark': {'mae': bench_mae}\n",
    "            }\n",
    "            \n",
    "            for pred_col in ml_pred_cols:\n",
    "                model_name = pred_col.replace('_pred', '')\n",
    "                ml_mae = sku_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                sku_results[sku][model_name] = {\n",
    "                    'mae': ml_mae,\n",
    "                    'improvement_pct': improvement_pct\n",
    "                }\n",
    "        \n",
    "        return sku_results\n",
    "    \n",
    "    def analyze_temporal_performance(self):\n",
    "        \"\"\"5. Temporal Performance Analysis\"\"\"\n",
    "        print(\"\\n📅 5. TEMPORAL PERFORMANCE ANALYSIS\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        temporal_results = {}\n",
    "        ml_pred_cols = [col for col in self.test_data.columns if col.endswith('_pred')]\n",
    "        \n",
    "        for pred_col in ml_pred_cols:\n",
    "            model_name = pred_col.replace('_pred', '')\n",
    "            \n",
    "            monthly_comparison = []\n",
    "            \n",
    "            for month in sorted(self.test_data['year_month'].unique()):\n",
    "                month_data = self.test_data[self.test_data['year_month'] == month]\n",
    "                \n",
    "                if len(month_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                bench_mae = month_data['benchmark_abs_error'].mean()\n",
    "                ml_mae = month_data[f'{model_name}_abs_error'].mean()\n",
    "                improvement_pct = ((bench_mae - ml_mae) / bench_mae) * 100\n",
    "                \n",
    "                monthly_comparison.append({\n",
    "                    'month': month,\n",
    "                    'improvement_pct': improvement_pct,\n",
    "                    'wins_benchmark': improvement_pct > 0\n",
    "                })\n",
    "            \n",
    "            comparison_df = pd.DataFrame(monthly_comparison)\n",
    "            \n",
    "            if len(comparison_df) > 0:\n",
    "                win_rate = (comparison_df['wins_benchmark'].sum() / len(comparison_df)) * 100\n",
    "                avg_improvement = comparison_df['improvement_pct'].mean()\n",
    "                \n",
    "                temporal_results[model_name] = {\n",
    "                    'win_rate': win_rate,\n",
    "                    'avg_improvement': avg_improvement,\n",
    "                    'monthly_details': comparison_df\n",
    "                }\n",
    "        \n",
    "        return temporal_results\n",
    "\n",
    "\n",
    "# Main Enhanced Pipeline Function\n",
    "def run_enhanced_demand_analysis_with_prophet_and_marketing(\n",
    "    benchmark_df, \n",
    "    amazon_order_items=None, tiktok_order_items=None, shopify_order_items=None,\n",
    "    amazon_daily_sku=None, tiktok_daily_sku=None, shopify_daily_sku=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete enhanced demand analysis pipeline with Prophet and marketing transformations\n",
    "    \n",
    "    New Features:\n",
    "    - Prophet time series forecasting\n",
    "    - Marketing spend transformations (log, quadratic, log-log)\n",
    "    - Enhanced Q4/holiday seasonality\n",
    "    - Diminishing returns modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 ENHANCED SKU DEMAND ANALYSIS WITH PROPHET & MARKETING\")\n",
    "    print(\"🎯 NEW: Prophet forecasting + Marketing transformations + Q4 analysis\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    # Step 1: Train enhanced models\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"STEP 1: ENHANCED MODEL TRAINING WITH MARKETING & PROPHET\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    enhanced_modeler = EnhancedDemandModels(\n",
    "        benchmark_df, \n",
    "        amazon_order_items, tiktok_order_items, shopify_order_items,\n",
    "        amazon_daily_sku, tiktok_daily_sku, shopify_daily_sku\n",
    "    )\n",
    "    \n",
    "    enhanced_model_results, enhanced_test_data = enhanced_modeler.evaluate_enhanced_models()\n",
    "    \n",
    "    # Step 2: Comprehensive enhanced evaluation\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"STEP 2: ENHANCED COMPREHENSIVE EVALUATION\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    enhanced_analyzer = EnhancedMLModelEvaluationAnalysis(\n",
    "        enhanced_test_data, enhanced_model_results\n",
    "    )\n",
    "    enhanced_analysis_results = enhanced_analyzer.run_enhanced_comprehensive_analysis()\n",
    "    \n",
    "    # Step 3: Marketing insights extraction\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"STEP 3: MARKETING EFFECTIVENESS INSIGHTS\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    marketing_insights = enhanced_modeler.get_marketing_insights()\n",
    "    \n",
    "    print(\"\\n🎉 ENHANCED ANALYSIS COMPLETE!\")\n",
    "    print(\"📊 Enhanced features delivered:\")\n",
    "    print(\"   ✅ Prophet time series forecasting\")\n",
    "    print(\"   ✅ Marketing spend transformations (log, quadratic, log-log)\")\n",
    "    print(\"   ✅ Enhanced Q4 and holiday seasonality\")\n",
    "    print(\"   ✅ Diminishing returns modeling\")\n",
    "    print(\"   ✅ Marketing effectiveness analysis\")\n",
    "    print(\"   ✅ Q4 specialization scoring\")\n",
    "    print(\"   ✅ Prophet vs ML model comparison\")\n",
    "    \n",
    "    return enhanced_modeler, enhanced_analyzer, enhanced_analysis_results, marketing_insights\n",
    "\n",
    "\n",
    "# Quick usage example:\n",
    "\n",
    "enhanced_modeler, enhanced_analyzer, enhanced_results, marketing_insights = run_enhanced_demand_analysis_with_prophet_and_marketing(\n",
    "    benchmark_df=your_benchmark_data,\n",
    "    amazon_order_items=your_amazon_data,\n",
    "    # ... other channel data\n",
    ")\n",
    "\n",
    "# Extract specific insights\n",
    "marketing_effectiveness = enhanced_results['marketing_effectiveness']\n",
    "q4_seasonality = enhanced_results['q4_seasonality']\n",
    "prophet_performance = enhanced_results['prophet_effectiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "333475f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ENHANCED SKU DEMAND ANALYSIS WITH PROPHET & MARKETING\n",
      "🎯 NEW: Prophet forecasting + Marketing transformations + Q4 analysis\n",
      "=====================================================================================\n",
      "\n",
      "=====================================================================================\n",
      "STEP 1: ENHANCED MODEL TRAINING WITH MARKETING & PROPHET\n",
      "=====================================================================================\n",
      "🚀 ENHANCED DEMAND PREDICTION MODELS\n",
      "🎯 New Features: Prophet forecasting + Marketing transformations + Q4 seasonality\n",
      "================================================================================\n",
      "   📊 Amazon order_items: 996,886 records\n",
      "   📊 Tiktok order_items: 232,267 records\n",
      "   📊 Shopify order_items: 10,773,655 records\n",
      "   ✅ Combined order_items: 12,002,808 total records\n",
      "   📊 Amazon daily_sku: 24,690 records\n",
      "   📊 Tiktok daily_sku: 6,836 records\n",
      "   📊 Shopify daily_sku: 70,585 records\n",
      "   ✅ Combined daily_sku: 102,111 total records\n",
      "\n",
      "🔧 Preparing base data...\n",
      "   ✅ Base data: 97,190 records\n",
      "   📅 Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "\n",
      "🤖 ENHANCED MODEL TRAINING & EVALUATION\n",
      "--------------------------------------------------\n",
      "\n",
      "🏗️ ENHANCED FEATURE ENGINEERING PIPELINE\n",
      "--------------------------------------------------\n",
      "   🗓️ Engineering seasonality features...\n",
      "   🛍️ Engineering product features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:41:20 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   👥 Engineering customer lifecycle features...\n",
      "   🔗 Engineering cross-SKU interaction features...\n",
      "   💰 Engineering marketing spend transformations...\n",
      "   ⚠️ No marketing columns found, creating synthetic marketing data\n",
      "   📊 Found marketing columns: ['marketing_spend']\n",
      "   🎄 Engineering Q4 and holiday features...\n",
      "   📈 Engineering Prophet-derived features...\n",
      "   🔄 Training Prophet model for feature extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:41:21 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Prophet features extracted successfully\n",
      "   📊 Creating log-log transformations...\n",
      "   📈 Engineering lag features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:41:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:21 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Enhanced feature engineering complete\n",
      "   📊 Total features: 108\n",
      "   📋 Records: 97,190\n",
      "   💰 Marketing transformation features: 16\n",
      "   🎄 Q4/Holiday features: 7\n",
      "   📈 Prophet-derived features: 6\n",
      "\n",
      "📈 TRAINING PROPHET MODELS\n",
      "-----------------------------------\n",
      "🎯 Training Prophet on top 10 SKUs by revenue\n",
      "   🔄 Training Prophet for Caramel Coffee Concentrate...\n",
      "   ❌ Error training Prophet for Caramel Coffee Concentrate: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for Mystery Gift...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:41:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Error training Prophet for Mystery Gift: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for Original Coffee Concentrate...\n",
      "   ❌ Error training Prophet for Original Coffee Concentrate: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for Mocha Coffee Concentrate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Error training Prophet for Mocha Coffee Concentrate: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for French Vanilla Coffee Concentrate...\n",
      "   ❌ Error training Prophet for French Vanilla Coffee Concentrate: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for Caramel Brulee Coffee Concentrate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Error training Prophet for Caramel Brulee Coffee Concentrate: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for Iced Vanilla Cream Drizzle Coffee Concentrate...\n",
      "   ❌ Error training Prophet for Iced Vanilla Cream Drizzle Coffee Concentrate: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for White Chocolate Mocha Coffee Concentrate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Error training Prophet for White Chocolate Mocha Coffee Concentrate: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for Caramel Protein Coffee...\n",
      "   ❌ Error training Prophet for Caramel Protein Coffee: Found NaN in column 'marketing_spend'\n",
      "   🔄 Training Prophet for Protein Coffee Blend...\n",
      "   ❌ Error training Prophet for Protein Coffee Blend: Found NaN in column 'marketing_spend'\n",
      "\n",
      "✅ Prophet training complete: 0 models trained\n",
      "📊 Training Data: 77,752 records\n",
      "📊 Test Data: 19,438 records\n",
      "🎯 Benchmark MAE: $1,660.69\n",
      "🎯 Benchmark MAPE: 61.2%\n",
      "\n",
      "🎯 TRAINING ENHANCED MODELS WITH MARKETING TRANSFORMATIONS:\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "🔄 Training Marketing Log Model...\n",
      "📋 \n",
      "            💰 MARKETING LOG TRANSFORMATION MODEL:\n",
      "            Demand = β₀ + β₁×log(marketing_spend) + β₂×seasonality + β₃×customer_features + ε\n",
      "\n",
      "            Focus: Captures diminishing returns from marketing spend\n",
      "            Theory: Each additional dollar has decreasing marginal impact\n",
      "            \n",
      "   ✅ MAE: $1,711.11, MAPE: 367.1%\n",
      "   📊 Features used: 14\n",
      "\n",
      "🔄 Training Marketing Quadratic Model...\n",
      "📋 \n",
      "            📈 MARKETING QUADRATIC MODEL:\n",
      "            Demand = β₀ + β₁×marketing + β₂×marketing² + β₃×other_features + ε\n",
      "\n",
      "            Focus: Non-linear marketing response curve\n",
      "            Theory: Can model both increasing and decreasing returns\n",
      "            \n",
      "   ✅ MAE: $2,073.73, MAPE: 523.7%\n",
      "   📊 Features used: 15\n",
      "\n",
      "🔄 Training Q4 Enhanced Model...\n",
      "📋 \n",
      "            🎄 Q4 ENHANCED SEASONALITY MODEL:\n",
      "            Demand = β₀ + β₁×base_features + β₂×is_black_friday×5 + β₃×holiday_intensity×3 + \n",
      "                     β₄×days_to_christmas + β₅×q4_marketing_interaction + ε\n",
      "\n",
      "            Focus: Captures Q4 shopping surge and holiday patterns\n",
      "            Weights: Black Friday gets 5x multiplier, holiday intensity 3x\n",
      "            \n",
      "   ✅ MAE: $1,496.75, MAPE: 170.4%\n",
      "   📊 Features used: 20\n",
      "\n",
      "🔄 Training Prophet-Enhanced Linear...\n",
      "📋 \n",
      "            📈 PROPHET-ENHANCED LINEAR MODEL:\n",
      "            Demand = β₀ + β₁×prophet_trend + β₂×prophet_seasonality + β₃×marketing_log + \n",
      "                     β₄×customer_features + ε\n",
      "\n",
      "            Focus: Combines Prophet's time series insights with regression\n",
      "            \n",
      "   ✅ MAE: $1,698.39, MAPE: 256.7%\n",
      "   📊 Features used: 15\n",
      "\n",
      "🔄 Training Hybrid Marketing Model...\n",
      "📋 \n",
      "            🔄 HYBRID MARKETING RESPONSE MODEL:\n",
      "            Demand = β₀ + f(marketing_spend) + g(seasonality) + h(customer_mix)\n",
      "\n",
      "            Where:\n",
      "            f(marketing) = β₁×log(marketing) + β₂×marketing_efficiency\n",
      "            g(seasonality) = β₃×q4_multiplier + β₄×prophet_trend\n",
      "            h(customer) = β₅×loyalty_score + β₆×new_customer_ratio\n",
      "\n",
      "            Focus: Sophisticated marketing response with diminishing returns\n",
      "            \n",
      "   ✅ MAE: $5,370.42, MAPE: 3180.4%\n",
      "   📊 Features used: 5\n",
      "\n",
      "🔄 Training Enhanced Elastic Net...\n",
      "   ✅ MAE: $1,012.99, MAPE: 560.7%\n",
      "   📊 Features used: 108\n",
      "\n",
      "🔄 Training Enhanced Random Forest...\n",
      "   ✅ MAE: $249.83, MAPE: 4.6%\n",
      "   📊 Features used: 108\n",
      "\n",
      "🔄 Training Enhanced Gradient Boosting...\n",
      "   ✅ MAE: $212.24, MAPE: 4.4%\n",
      "   📊 Features used: 108\n",
      "\n",
      "🔄 Training Log-Log Marketing Model...\n",
      "📋 \n",
      "            📊 LOG-LOG MARKETING MODEL:\n",
      "            log(Demand) = β₀ + β₁×log(marketing) + β₂×log(other_features) + ε\n",
      "\n",
      "            Focus: Elasticity interpretation - β coefficients are elasticities\n",
      "            Theory: Percent change in marketing → percent change in demand\n",
      "            \n",
      "   ✅ MAE: $1,407.98, MAPE: 10.9%\n",
      "   📊 Features used: 15\n",
      "\n",
      "📈 ENHANCED MODEL PERFORMANCE vs BENCHMARK:\n",
      "-------------------------------------------------------\n",
      "🏆 MODEL RANKING (by improvement %):\n",
      "---------------------------------------------\n",
      "🥇 # 1. Enhanced Gradient Boosting\n",
      "      MAE: $212.24 (+87.2%) ✅ BEATS BENCHMARK\n",
      "      MAPE: 4.4%\n",
      "\n",
      "🥇 # 2. Enhanced Random Forest\n",
      "      MAE: $249.83 (+85.0%) ✅ BEATS BENCHMARK\n",
      "      MAPE: 4.6%\n",
      "\n",
      "🥇 # 3. Enhanced Elastic Net\n",
      "      MAE: $1,012.99 (+39.0%) ✅ BEATS BENCHMARK\n",
      "      MAPE: 560.7%\n",
      "\n",
      "🥈 # 4. Log-Log Marketing Model\n",
      "      MAE: $1,407.98 (+15.2%) ✅ BEATS BENCHMARK\n",
      "      MAPE: 10.9%\n",
      "\n",
      "🥈 # 5. Q4 Enhanced Model\n",
      "      MAE: $1,496.75 (+9.9%) ✅ BEATS BENCHMARK\n",
      "      MAPE: 170.4%\n",
      "\n",
      "🥈 # 6. Prophet-Enhanced Linear\n",
      "      MAE: $1,698.39 (-2.3%) ❌ UNDERPERFORMS\n",
      "      MAPE: 256.7%\n",
      "\n",
      "🥉 # 7. Marketing Log Model\n",
      "      MAE: $1,711.11 (-3.0%) ❌ UNDERPERFORMS\n",
      "      MAPE: 367.1%\n",
      "\n",
      "🥉 # 8. Marketing Quadratic Model\n",
      "      MAE: $2,073.73 (-24.9%) ❌ UNDERPERFORMS\n",
      "      MAPE: 523.7%\n",
      "\n",
      "🥉 # 9. Hybrid Marketing Model\n",
      "      MAE: $5,370.42 (-223.4%) ❌ UNDERPERFORMS\n",
      "      MAPE: 3180.4%\n",
      "\n",
      "\n",
      "💰 MARKETING TRANSFORMATION EFFECTIVENESS ANALYSIS\n",
      "------------------------------------------------------------\n",
      "📊 MARKETING MODEL COMPARISON:\n",
      "-----------------------------------\n",
      "Marketing Log Model:\n",
      "   Description: Log transformation (diminishing returns)\n",
      "   Performance: -3.0% improvement (MAE: $1,711.11)\n",
      "   Key marketing features: marketing_spend_log\n",
      "\n",
      "Marketing Quadratic Model:\n",
      "   Description: Quadratic transformation (non-linear response)\n",
      "   Performance: -24.9% improvement (MAE: $2,073.73)\n",
      "   Key marketing features: marketing_spend, marketing_spend_squared, marketing_spend_efficiency, marketing_spend_intensity_low, marketing_spend_intensity_medium\n",
      "\n",
      "Log-Log Marketing Model:\n",
      "   Description: Log-log transformation (elasticity interpretation)\n",
      "   Performance: +15.2% improvement (MAE: $1,407.98)\n",
      "   Key marketing features: marketing_spend, marketing_spend_log, marketing_spend_squared, marketing_spend_sqrt, marketing_spend_efficiency\n",
      "\n",
      "Hybrid Marketing Model:\n",
      "   Description: Hybrid approach (multiple transformations)\n",
      "   Performance: -223.4% improvement (MAE: $5,370.42)\n",
      "\n",
      "🏆 BEST MARKETING TRANSFORMATION: Log-Log Marketing Model\n",
      "   Achieved +15.2% improvement over benchmark\n",
      "   This suggests log-log transformation (elasticity interpretation) works best for your data\n",
      "\n",
      "🎄 Q4/HOLIDAY SEASONALITY EFFECTIVENESS:\n",
      "---------------------------------------------\n",
      "Q4 Enhanced Model Performance: +9.9% improvement\n",
      "MAE: $1,496.75\n",
      "✅ Strong Q4 seasonality effects detected - holiday features are valuable\n",
      "\n",
      "📈 PROPHET TIME SERIES EFFECTIVENESS:\n",
      "----------------------------------------\n",
      "Prophet-Enhanced Linear: -2.3% improvement (MAE: $1,698.39)\n",
      "❌ Prophet may be overfitting or unsuitable for this data\n",
      "\n",
      "=====================================================================================\n",
      "STEP 2: ENHANCED COMPREHENSIVE EVALUATION\n",
      "=====================================================================================\n",
      "\n",
      "🔍 ENHANCED ML MODEL PERFORMANCE ANALYSIS\n",
      "🎯 Including: Marketing transformations, Prophet analysis, Q4 seasonality\n",
      "===========================================================================\n",
      "\n",
      "📊 Adding enhanced ML predictions to test data...\n",
      "   ✅ Added Marketing Log Model predictions\n",
      "   ✅ Added Marketing Quadratic Model predictions\n",
      "   ✅ Added Q4 Enhanced Model predictions\n",
      "   ✅ Added Prophet-Enhanced Linear predictions\n",
      "   ✅ Added Hybrid Marketing Model predictions\n",
      "   ✅ Added Enhanced Elastic Net predictions\n",
      "   ✅ Added Enhanced Random Forest predictions\n",
      "   ✅ Added Enhanced Gradient Boosting predictions\n",
      "   ✅ Added Log-Log Marketing Model predictions\n",
      "\n",
      "🔧 Preparing enhanced analysis data...\n",
      "   ✅ Enhanced analysis data prepared: 19,438 records\n",
      "   📊 Enhanced ML models found: 9\n",
      "🚀 RUNNING ENHANCED COMPREHENSIVE ML MODEL ANALYSIS\n",
      "=================================================================\n",
      "\n",
      "📈 1. OVERALL ACCURACY ANALYSIS\n",
      "---------------------------------------------\n",
      "🎯 BENCHMARK PERFORMANCE:\n",
      "   MAE:  $1,660.69\n",
      "   MAPE: 61.2%\n",
      "\n",
      "🤖 ML MODEL PERFORMANCE:\n",
      "   Marketing Log Model:\n",
      "     MAE:  $1,711.11 ($-50.42, -3.0%) ❌ WORSE\n",
      "     MAPE: 367.1% (-305.9%)\n",
      "\n",
      "   Marketing Quadratic Model:\n",
      "     MAE:  $2,073.73 ($-413.04, -24.9%) ❌ WORSE\n",
      "     MAPE: 523.7% (-462.6%)\n",
      "\n",
      "   Q4 Enhanced Model:\n",
      "     MAE:  $1,496.75 ($+163.94, +9.9%) ✅ BETTER\n",
      "     MAPE: 170.4% (-109.3%)\n",
      "\n",
      "   Prophet-Enhanced Linear:\n",
      "     MAE:  $1,698.39 ($-37.70, -2.3%) ❌ WORSE\n",
      "     MAPE: 256.7% (-195.5%)\n",
      "\n",
      "   Hybrid Marketing Model:\n",
      "     MAE:  $5,370.42 ($-3,709.73, -223.4%) ❌ WORSE\n",
      "     MAPE: 3180.4% (-3119.2%)\n",
      "\n",
      "   Enhanced Elastic Net:\n",
      "     MAE:  $1,012.99 ($+647.70, +39.0%) ✅ BETTER\n",
      "     MAPE: 560.7% (-499.5%)\n",
      "\n",
      "   Enhanced Random Forest:\n",
      "     MAE:  $249.83 ($+1,410.86, +85.0%) ✅ BETTER\n",
      "     MAPE: 4.6% (+56.6%)\n",
      "\n",
      "   Enhanced Gradient Boosting:\n",
      "     MAE:  $212.24 ($+1,448.45, +87.2%) ✅ BETTER\n",
      "     MAPE: 4.4% (+56.8%)\n",
      "\n",
      "   Log-Log Marketing Model:\n",
      "     MAE:  $1,407.98 ($+252.71, +15.2%) ✅ BETTER\n",
      "     MAPE: 10.9% (+50.3%)\n",
      "\n",
      "\n",
      "💰 2. WEIGHTED ACCURACY BY PRODUCT IMPORTANCE\n",
      "-------------------------------------------------------\n",
      "📊 PRODUCT SEGMENTATION:\n",
      "   Top 20% Products: 245 SKUs\n",
      "   Bottom 80% Products: 980 SKUs\n",
      "\n",
      "🎯 TOP 20% PRODUCTS:\n",
      "   Benchmark MAE: $1895.75\n",
      "   Marketing Log Model: $1915.68 (-1.1%) ❌\n",
      "   Marketing Quadratic Model: $2287.36 (-20.7%) ❌\n",
      "   Q4 Enhanced Model: $1694.14 (+10.6%) ✅\n",
      "   Prophet-Enhanced Linear: $1909.95 (-0.7%) ❌\n",
      "   Hybrid Marketing Model: $5713.84 (-201.4%) ❌\n",
      "   Enhanced Elastic Net: $1102.44 (+41.8%) ✅\n",
      "   Enhanced Random Forest: $286.85 (+84.9%) ✅\n",
      "   Enhanced Gradient Boosting: $243.51 (+87.2%) ✅\n",
      "   Log-Log Marketing Model: $1615.71 (+14.8%) ✅\n",
      "\n",
      "🎯 BOTTOM 80% PRODUCTS:\n",
      "   Benchmark MAE: $76.85\n",
      "   Marketing Log Model: $332.75 (-333.0%) ❌\n",
      "   Marketing Quadratic Model: $634.25 (-725.3%) ❌\n",
      "   Q4 Enhanced Model: $166.77 (-117.0%) ❌\n",
      "   Prophet-Enhanced Linear: $272.88 (-255.1%) ❌\n",
      "   Hybrid Marketing Model: $3056.44 (-3877.0%) ❌\n",
      "   Enhanced Elastic Net: $410.26 (-433.8%) ❌\n",
      "   Enhanced Random Forest: $0.38 (+99.5%) ✅\n",
      "   Enhanced Gradient Boosting: $1.52 (+98.0%) ✅\n",
      "   Log-Log Marketing Model: $8.29 (+89.2%) ✅\n",
      "\n",
      "\n",
      "📺 3. CHANNEL PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "📻 SHOPIFY CHANNEL:\n",
      "   Records: 10,507\n",
      "   Benchmark MAE: $2636.73\n",
      "   Marketing Log Model: $2458.23 (+6.8%) ✅ BETTER\n",
      "   Marketing Quadratic Model: $2801.87 (-6.3%) ❌ WORSE\n",
      "   Q4 Enhanced Model: $2210.30 (+16.2%) ✅ BETTER\n",
      "   Prophet-Enhanced Linear: $2424.32 (+8.1%) ✅ BETTER\n",
      "   Hybrid Marketing Model: $7181.13 (-172.3%) ❌ WORSE\n",
      "   Enhanced Elastic Net: $1404.13 (+46.7%) ✅ BETTER\n",
      "   Enhanced Random Forest: $372.37 (+85.9%) ✅ BETTER\n",
      "   Enhanced Gradient Boosting: $317.48 (+88.0%) ✅ BETTER\n",
      "   Log-Log Marketing Model: $2421.69 (+8.2%) ✅ BETTER\n",
      "\n",
      "📻 AMAZON CHANNEL:\n",
      "   Records: 6,142\n",
      "   Benchmark MAE: $516.42\n",
      "   Marketing Log Model: $769.91 (-49.1%) ❌ WORSE\n",
      "   Marketing Quadratic Model: $1265.44 (-145.0%) ❌ WORSE\n",
      "   Q4 Enhanced Model: $609.06 (-17.9%) ❌ WORSE\n",
      "   Prophet-Enhanced Linear: $792.91 (-53.5%) ❌ WORSE\n",
      "   Hybrid Marketing Model: $3080.59 (-496.5%) ❌ WORSE\n",
      "   Enhanced Elastic Net: $376.08 (+27.2%) ✅ BETTER\n",
      "   Enhanced Random Forest: $81.48 (+84.2%) ✅ BETTER\n",
      "   Enhanced Gradient Boosting: $65.70 (+87.3%) ✅ BETTER\n",
      "   Log-Log Marketing Model: $187.04 (+63.8%) ✅ BETTER\n",
      "\n",
      "📻 TIKTOK CHANNEL:\n",
      "   Records: 2,789\n",
      "   Benchmark MAE: $503.58\n",
      "   Marketing Log Model: $969.24 (-92.5%) ❌ WORSE\n",
      "   Marketing Quadratic Model: $1110.63 (-120.5%) ❌ WORSE\n",
      "   Q4 Enhanced Model: $763.50 (-51.6%) ❌ WORSE\n",
      "   Prophet-Enhanced Linear: $957.68 (-90.2%) ❌ WORSE\n",
      "   Hybrid Marketing Model: $3591.68 (-613.2%) ❌ WORSE\n",
      "   Enhanced Elastic Net: $942.06 (-87.1%) ❌ WORSE\n",
      "   Enhanced Random Forest: $158.91 (+68.4%) ✅ BETTER\n",
      "   Enhanced Gradient Boosting: $138.49 (+72.5%) ✅ BETTER\n",
      "   Log-Log Marketing Model: $277.84 (+44.8%) ✅ BETTER\n",
      "\n",
      "\n",
      "🏆 4. TOP SKU PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "📊 TOP 10 SKUs by Revenue:\n",
      "    1. Mystery Gift: $6,306,617 (7.4% of total)\n",
      "    2. Caramel Protein Coffee: $6,195,724 (7.2% of total)\n",
      "    3. French Vanilla Protein Coffee: $5,034,607 (5.9% of total)\n",
      "    4. Original Protein Coffee: $4,975,588 (5.8% of total)\n",
      "    5. Mocha Protein Coffee: $4,973,715 (5.8% of total)\n",
      "    6. Milk Frother: $3,554,760 (4.2% of total)\n",
      "    7. Caramel Coffee Concentrate: $3,219,579 (3.8% of total)\n",
      "    8. Original Coffee Concentrate: $3,022,769 (3.5% of total)\n",
      "    9. White Chocolate Mocha Coffee Concentrate: $2,878,622 (3.4% of total)\n",
      "   10. Recipe Booklet: $2,786,081 (3.3% of total)\n",
      "\n",
      "\n",
      "📅 5. TEMPORAL PERFORMANCE ANALYSIS\n",
      "---------------------------------------------\n",
      "\n",
      "💰 MARKETING TRANSFORMATION MODEL EFFECTIVENESS\n",
      "-------------------------------------------------------\n",
      "📊 MARKETING MODEL COMPARISON:\n",
      "-----------------------------------\n",
      "Marketing Log Model:\n",
      "   Transformation: Log transformation\n",
      "   Overall improvement: -3.0%\n",
      "   MAE: $1,711.11\n",
      "   Performance by marketing spend:\n",
      "     Low spend: -1228.9%\n",
      "     Low-Med spend: -233.4%\n",
      "     Medium spend: -49.8%\n",
      "     Med-High spend: -6.5%\n",
      "     High spend: +8.5%\n",
      "\n",
      "Marketing Quadratic Model:\n",
      "   Transformation: Quadratic transformation\n",
      "   Overall improvement: -24.9%\n",
      "   MAE: $2,073.73\n",
      "   Performance by marketing spend:\n",
      "     Low spend: -354.9%\n",
      "     Low-Med spend: -1378.9%\n",
      "     Medium spend: -157.5%\n",
      "     Med-High spend: -52.3%\n",
      "     High spend: +4.6%\n",
      "\n",
      "Hybrid Marketing Model:\n",
      "   Transformation: Hybrid approach\n",
      "   Overall improvement: -223.4%\n",
      "   MAE: $5,370.42\n",
      "   Performance by marketing spend:\n",
      "     Low spend: -9961.1%\n",
      "     Low-Med spend: -4612.5%\n",
      "     Medium spend: -1378.8%\n",
      "     Med-High spend: -156.8%\n",
      "     High spend: -88.5%\n",
      "\n",
      "Log-Log Marketing Model:\n",
      "   Transformation: Log transformation\n",
      "   Overall improvement: +15.2%\n",
      "   MAE: $1,407.98\n",
      "   Performance by marketing spend:\n",
      "     Low spend: +75.4%\n",
      "     Low-Med spend: +71.6%\n",
      "     Medium spend: +75.3%\n",
      "     Med-High spend: +53.1%\n",
      "     High spend: +3.6%\n",
      "\n",
      "🏆 BEST MARKETING TRANSFORMATION:\n",
      "   Model: Log-Log Marketing Model\n",
      "   Improvement: +15.2%\n",
      "   Approach: Log transformation\n",
      "\n",
      "🎄 Q4 SEASONALITY EFFECTIVENESS ANALYSIS\n",
      "---------------------------------------------\n",
      "❌ No Q4 data available for analysis\n",
      "\n",
      "📈 PROPHET MODEL EFFECTIVENESS ANALYSIS\n",
      "---------------------------------------------\n",
      "Prophet-Enhanced Linear:\n",
      "   Overall improvement: -2.3%\n",
      "   MAE: $1,698.39\n",
      "   Monthly consistency: 86.2/100\n",
      "   Best month: 2025-02 (+7.2%)\n",
      "   Worst month: 2025-06 (-32.1%)\n",
      "\n",
      "\n",
      "🎯 ENHANCED MODEL RECOMMENDATIONS\n",
      "=============================================\n",
      "📊 MARKETING TRANSFORMATION RECOMMENDATIONS:\n",
      "   🏆 Best marketing approach: Log-Log Marketing Model\n",
      "   📈 Improvement: +15.2%\n",
      "   💡 Strategy: Log transformation\n",
      "\n",
      "🎄 Q4 SEASONALITY RECOMMENDATIONS:\n",
      "\n",
      "📈 PROPHET RECOMMENDATIONS:\n",
      "   🏆 Best Prophet model: Prophet-Enhanced Linear\n",
      "   📈 Improvement: -2.3%\n",
      "   📊 Consistency: 86.2/100\n",
      "   ❌ Prophet ineffective - stick to ML models\n",
      "\n",
      "🏆 OVERALL STRATEGY RECOMMENDATIONS:\n",
      "   🥇 Best overall model: Enhanced Gradient Boosting\n",
      "   📈 Best improvement: +87.2%\n",
      "   🎯 Recommended strategy: DEPLOY IMMEDIATELY\n",
      "   🎪 Confidence level: HIGH\n",
      "\n",
      "=====================================================================================\n",
      "STEP 3: MARKETING EFFECTIVENESS INSIGHTS\n",
      "=====================================================================================\n",
      "\n",
      "💡 MARKETING EFFECTIVENESS INSIGHTS\n",
      "---------------------------------------------\n",
      "\n",
      "🌳 ENHANCED RANDOM FOREST - TOP MARKETING FEATURES:\n",
      "   1. loglog_marketing_spend: 0.1068 (Raw spend)\n",
      "   2. marketing_spend_efficiency: 0.0061 (Efficiency)\n",
      "   3. marketing_spend: 0.0001 (Raw spend)\n",
      "   4. marketing_spend_sqrt_positive: 0.0001 (Raw spend)\n",
      "   5. marketing_spend_positive_positive: 0.0001 (Raw spend)\n",
      "\n",
      "🌳 ENHANCED GRADIENT BOOSTING - TOP MARKETING FEATURES:\n",
      "   1. loglog_marketing_spend: 0.1158 (Raw spend)\n",
      "   2. marketing_spend_efficiency: 0.0098 (Efficiency)\n",
      "   3. marketing_spend_squared: 0.0001 (Quadratic)\n",
      "   4. marketing_spend_sqrt_positive: 0.0001 (Raw spend)\n",
      "   5. marketing_spend_positive_positive: 0.0000 (Raw spend)\n",
      "\n",
      "📊 MARKETING LOG MODEL - MARKETING COEFFICIENTS:\n",
      "   marketing_spend_log: 1222.3917 (📈 Positive impact)\n",
      "\n",
      "📊 MARKETING QUADRATIC MODEL - MARKETING COEFFICIENTS:\n",
      "   marketing_spend_intensity_high: -3069.2964 (📉 Negative impact)\n",
      "   marketing_spend_intensity_low: 2414.0093 (📈 Positive impact)\n",
      "   marketing_spend_efficiency: 1999.0064 (📈 Positive impact)\n",
      "   marketing_spend_efficiency_positive: -1489.9195 (📉 Negative impact)\n",
      "   marketing_spend_intensity_medium: 655.2871 (📈 Positive impact)\n",
      "\n",
      "📊 LOG-LOG MARKETING MODEL - MARKETING COEFFICIENTS:\n",
      "   loglog_marketing_spend: 0.1812 (📈 Positive impact)\n",
      "   marketing_spend_efficiency: -0.1792 (📉 Negative impact)\n",
      "   marketing_spend_sqrt: -0.1572 (📉 Negative impact)\n",
      "   marketing_spend_intensity_medium: 0.0947 (📈 Positive impact)\n",
      "   marketing_spend_log: -0.0723 (📉 Negative impact)\n",
      "\n",
      "🎉 ENHANCED ANALYSIS COMPLETE!\n",
      "📊 Enhanced features delivered:\n",
      "   ✅ Prophet time series forecasting\n",
      "   ✅ Marketing spend transformations (log, quadratic, log-log)\n",
      "   ✅ Enhanced Q4 and holiday seasonality\n",
      "   ✅ Diminishing returns modeling\n",
      "   ✅ Marketing effectiveness analysis\n",
      "   ✅ Q4 specialization scoring\n",
      "   ✅ Prophet vs ML model comparison\n"
     ]
    }
   ],
   "source": [
    "enhanced_modeler, enhanced_analyzer, enhanced_results, marketing_insights = run_enhanced_demand_analysis_with_prophet_and_marketing(\n",
    "    benchmark_df=enhanced_benchmark_df,\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items,\n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")\n",
    "\n",
    "# Get specific insights\n",
    "marketing_effectiveness = enhanced_results['marketing_effectiveness']\n",
    "q4_performance = enhanced_results['q4_seasonality'] \n",
    "prophet_insights = enhanced_results['prophet_effectiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d2ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENHANCED DEMAND FORECASTING ANALYSIS\n",
      "Features: Real holidays, economic data, SKU analysis, degradation tracking, speed measurement\n",
      "=====================================================================================\n",
      "ENHANCED DEMAND PREDICTION WITH EXTERNAL DATA\n",
      "Features: Real holiday data, economic proxies, prediction degradation analysis\n",
      "===========================================================================\n",
      "   Amazon order_items: 996,886 records\n",
      "   Tiktok order_items: 232,267 records\n",
      "   Shopify order_items: 10,773,655 records\n",
      "   Combined order_items: 12,002,808 total records\n",
      "   Amazon daily_sku: 24,690 records\n",
      "   Tiktok daily_sku: 6,836 records\n",
      "   Shopify daily_sku: 70,585 records\n",
      "   Combined daily_sku: 102,111 total records\n",
      "\n",
      "Preparing base data...\n",
      "   Base data: 97,189 records\n",
      "   Date range: 2020-11-19 00:00:00 to 2025-06-15 00:00:00\n",
      "   Unique SKUs: 2,262\n",
      "\n",
      "Creating percentage change target variable...\n",
      "   Percentage Change Statistics:\n",
      "   Mean: 50.11%\n",
      "   Std: 106.90%\n",
      "   Min: -99.95%\n",
      "   Max: 3651.88%\n",
      "   Median: 16.16%\n",
      "   Outliers (>±200%): 8403 (8.6%)\n",
      "   Clipping extreme outliers to ±200% range\n",
      "\n",
      "Collecting external data...\n",
      "Collecting US holiday data...\n",
      "   Collected holidays for 6 years\n",
      "Creating economic indicator proxies...\n",
      "   Generated economic proxies for 1670 days\n",
      "   External data collection complete\n",
      "\n",
      "ENHANCED MODEL EVALUATION WITH PERFORMANCE ANALYSIS\n",
      "-------------------------------------------------------\n",
      "\n",
      "ENHANCED FEATURE ENGINEERING WITH EXTERNAL DATA\n",
      "--------------------------------------------------\n",
      "   Engineering seasonality features...\n",
      "   Engineering real holiday features...\n",
      "     Added 3909 holiday records\n",
      "     Black Friday records: 275\n",
      "     Prime Day records: 205\n",
      "   Engineering external economic features...\n",
      "   Engineering product features...\n",
      "   Engineering customer lifecycle features...\n",
      "   Engineering cross-SKU features...\n",
      "   Engineering marketing transformations...\n",
      "   Found marketing columns: ['seasonal_spending_index', 'competition_adjusted_demand_potential']\n",
      "   Engineering percentage change specific features...\n",
      "   Feature engineering complete\n",
      "   Total features: 96\n",
      "   Records: 97,189\n",
      "   Holiday features: 10\n",
      "   Economic features: 13\n",
      "   Percentage change features: 16\n",
      "   Marketing features: 0\n",
      "Training Data: 77,751 records\n",
      "Test Data: 19,438 records\n",
      "Features: 96\n",
      "\n",
      "\n",
      "Benchmark MAE (0% change): 61.55%\n",
      "Linear Regression: MAE 0.00% (+100.0% improvement)\n",
      "Ridge Regression: MAE 0.00% (+100.0% improvement)\n",
      "Elastic Net: MAE 4.28% (+93.0% improvement)\n",
      "Random Forest: MAE 0.01% (+100.0% improvement)\n",
      "Gradient Boosting: MAE 0.04% (+99.9% improvement)\n",
      "\n",
      "============================================================\n",
      "ENHANCED PERFORMANCE ANALYSES\n",
      "============================================================\n",
      "\n",
      "SKU PREDICTION PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "SKU Performance Analysis (using Linear Regression):\n",
      "Total SKUs analyzed: 251\n",
      "\n",
      "TOP 10 MOST PREDICTABLE SKUs:\n",
      "------------------------------\n",
      "Athletic Bottle: MAE 0.0%, 182 predictions, avg demand 114\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - French Vanilla: MAE 0.0%, 145 predictions, avg demand 3069\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - Iced Vanilla Cream Drizzle: MAE 0.0%, 145 predictions, avg demand 707\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - Mocha: MAE 0.0%, 145 predictions, avg demand 2741\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - Original: MAE 0.0%, 145 predictions, avg demand 4067\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - Original Decaf: MAE 0.0%, 145 predictions, avg demand 1400\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - Salted Caramel: MAE 0.0%, 145 predictions, avg demand 473\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - Sugar Cookie: MAE 0.0%, 144 predictions, avg demand 290\n",
      "Javy Coffee Concentrate - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee, 35 Servings - White Chocolate Mocha: MAE 0.0%, 145 predictions, avg demand 1891\n",
      "Javy Coffee Concentrate Bundle - Cold Brew Coffee, Perfect for Instant Iced Coffee, Cold Brewed Coffee and Hot Coffee - Original & Caramel: MAE 0.0%, 145 predictions, avg demand 537\n",
      "\n",
      "TOP 10 LEAST PREDICTABLE SKUs:\n",
      "--------------------------------\n",
      "Iced Vanilla Cream Drizzle Coffee Concentrate: MAE 0.0%, 147 predictions, avg demand 15618\n",
      "Iced Vanilla Cream Drizzle Coffee Concentrate - 2 Pack: MAE 0.0%, 35 predictions, avg demand 57\n",
      "Instant Coffee Protein Coffee - Premium Whey Protein & Instant Coffee - 100% Arabica Coffee - Zero Artificial Flavors & Sweeteners: MAE 0.0%, 74 predictions, avg demand 11783\n",
      "Instant Latte: MAE 0.0%, 96 predictions, avg demand 46\n",
      "Instant Latte 02 - 12 packets: MAE 0.0%, 18 predictions, avg demand 36\n",
      "Instant Protein Coffee: MAE 0.0%, 78 predictions, avg demand 62\n",
      "Instant Protein Coffee - Protein Blend [Deal]: MAE 0.0%, 114 predictions, avg demand 81\n",
      "Instant Protein Coffee - Protein Blend [Offer]: MAE 0.0%, 16 predictions, avg demand 38\n",
      "French Vanilla Protein Coffee (12 Servings): MAE 0.0%, 140 predictions, avg demand 323\n",
      "{LIVE EXCLUSIVE} Brownie Batter + IVCD concentrates: MAE 0.0%, 12 predictions, avg demand 150\n",
      "\n",
      "PREDICTABILITY PATTERNS:\n",
      "-------------------------\n",
      "High volume SKUs (top 20%): avg MAE 0.0%\n",
      "Low volume SKUs (bottom 20%): avg MAE 0.0%\n",
      "High volatility SKUs: avg MAE 0.0%\n",
      "Low volatility SKUs: avg MAE 0.0%\n",
      "\n",
      "PREDICTION DEGRADATION ANALYSIS\n",
      "-----------------------------------\n",
      "Prediction degradation over time (using Linear Regression):\n",
      "\n",
      "0-7 days: MAE 88.3% (++0.0% vs baseline), bias -0.0%, 977 predictions\n",
      "8-14 days: MAE 87.5% (+-1.0% vs baseline), bias -0.0%, 928 predictions\n",
      "15-30 days: MAE 90.5% (++2.5% vs baseline), bias -0.0%, 2215 predictions\n",
      "31-60 days: MAE 89.9% (++1.8% vs baseline), bias -0.0%, 4215 predictions\n",
      "61-90 days: MAE 85.5% (+-3.1% vs baseline), bias -0.0%, 4041 predictions\n",
      "90+ days: MAE 76.0% (+-14.0% vs baseline), bias -0.0%, 7062 predictions\n",
      "\n",
      "Average degradation rate: -2.8% per time period\n",
      "   Low degradation - model maintains accuracy well\n",
      "\n",
      "PREDICTION SPEED ANALYSIS\n",
      "-------------------------\n",
      "Linear Regression:\n",
      "   Avg time: 0.0068s (±0.0027s)\n",
      "   Speed: 2851917 predictions/second\n",
      "   Time per prediction: 0.00ms\n",
      "\n",
      "Ridge Regression:\n",
      "   Avg time: 0.0062s (±0.0042s)\n",
      "   Speed: 3120818 predictions/second\n",
      "   Time per prediction: 0.00ms\n",
      "\n",
      "Elastic Net:\n",
      "   Avg time: 0.0021s (±0.0009s)\n",
      "   Speed: 9107134 predictions/second\n",
      "   Time per prediction: 0.00ms\n",
      "\n",
      "Random Forest:\n",
      "   Avg time: 0.2409s (±0.0405s)\n",
      "   Speed: 80676 predictions/second\n",
      "   Time per prediction: 0.01ms\n",
      "\n",
      "Gradient Boosting:\n",
      "   Avg time: 0.0574s (±0.0083s)\n",
      "   Speed: 338454 predictions/second\n",
      "   Time per prediction: 0.00ms\n",
      "\n",
      "SPEED RANKING (fastest to slowest):\n",
      "-----------------------------------\n",
      "1. Elastic Net: 9107134 pred/sec\n",
      "2. Ridge Regression: 3120818 pred/sec\n",
      "3. Linear Regression: 2851917 pred/sec\n",
      "4. Gradient Boosting: 338454 pred/sec\n",
      "5. Random Forest: 80676 pred/sec\n",
      "\n",
      "FEATURE IMPORTANCE ANALYSIS (Linear Regression)\n",
      "----------------------------------------\n",
      "Top 15 Features by Coefficient Magnitude:\n",
      "--------------------------------------\n",
      "market_avg_pct_change: 1.0000\n",
      "pct_change_vs_market: 1.0000\n",
      "competition_adjusted_demand_potential_log: 0.0000\n",
      "seasonal_spending_index_positive: 0.0000\n",
      "seasonal_spending_index: 0.0000\n",
      "competition_intensity: 0.0000\n",
      "seasonal_spending_index_log: 0.0000\n",
      "seasonal_spending_index_squared: 0.0000\n",
      "competition_adjusted_demand_potential: 0.0000\n",
      "competition_adjusted_demand_potential_positive: 0.0000\n",
      "month_sin: 0.0000\n",
      "competition_adjusted_demand_potential_squared: 0.0000\n",
      "month_cos: 0.0000\n",
      "is_q4: 0.0000\n",
      "new_customer_ratio: 0.0000\n",
      "\n",
      "COMPREHENSIVE PERFORMANCE SUMMARY\n",
      "===================================\n",
      "\n",
      "SKU PREDICTION PERFORMANCE:\n",
      "- Total SKUs analyzed: 251\n",
      "- Best SKU MAE: 0.0%\n",
      "- Worst SKU MAE: 0.0%\n",
      "- Median SKU MAE: 0.0%\n",
      "\n",
      "Most predictable SKUs (top 25%) characteristics:\n",
      "- Average demand: nan\n",
      "- Average volatility: nan%\n",
      "\n",
      "Least predictable SKUs (bottom 25%) characteristics:\n",
      "- Average demand: nan\n",
      "- Average volatility: nan%\n",
      "\n",
      "PREDICTION DEGRADATION:\n",
      "- First week MAE: 88.3%\n",
      "- One month MAE: 90.5%\n",
      "- Degradation rate: +2.5% over 1 month\n",
      "\n",
      "PREDICTION SPEED:\n",
      "- Fastest model: Elastic Net (9107134 pred/sec)\n",
      "- Slowest model: Random Forest (80676 pred/sec)\n",
      "- Speed range: 112.9x difference\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "- Low-volume SKUs are equally predictable - consider broader forecasting\n",
      "- Monthly retraining should be sufficient\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE CROSS-MODEL INSIGHTS\n",
      "============================================================\n",
      "UNIVERSAL SKU PREDICTABILITY ANALYSIS\n",
      "=============================================\n",
      "Analyzed 338 SKUs across 5 models\n",
      "\n",
      "TOP 10 UNIVERSALLY PREDICTABLE SKUs:\n",
      "----------------------------------------\n",
      "Javvy Upgrade - B3G3: 0.0% avg MAE (consistency: High, n=18.0)\n",
      "Coffee Concentrate - Javy Coffee - 3 Bottles (Subscription) - (35% OFF): 0.1% avg MAE (consistency: High, n=12.0)\n",
      "Limited Deal L.P -> Javy Coffee - 3 Bottles - Ships Every 30 Days: 0.1% avg MAE (consistency: High, n=13.0)\n",
      "Limited Offer L.P - 3 Bottles - Ships Every 30 Days: 0.1% avg MAE (consistency: High, n=9.0)\n",
      "Original Trio Bundle: 0.1% avg MAE (consistency: High, n=5.0)\n",
      "Javy Coffee - Subscription Club - Javy Club - 3 Bottles - 4 weeks: 0.1% avg MAE (consistency: High, n=5.0)\n",
      "Decaf - Coffee Concentrate - 3 Bottles -  20% OFF: 0.1% avg MAE (consistency: High, n=8.0)\n",
      "Buy 2 Get 1 Free Limited Release: 0.1% avg MAE (consistency: High, n=69.0)\n",
      "Javy Protein Coffee Blend: 0.1% avg MAE (consistency: High, n=11.0)\n",
      "French Vanilla Coffee Concentrate: 0.1% avg MAE (consistency: High, n=147.0)\n",
      "\n",
      "TOP 10 UNIVERSALLY UNPREDICTABLE SKUs:\n",
      "------------------------------------------\n",
      "Coffee Concentrate - Decaf: 2.7% avg MAE (consistency: Low, n=5.0)\n",
      "Javvy Coffee Syrup-  Zero Sugar - Zero Calorie Coffee Flavoring Syrup: 2.8% avg MAE (consistency: Low, n=5.0)\n",
      "Coffee Concentrate - Original: 3.0% avg MAE (consistency: Low, n=59.0)\n",
      "Coffee Concentrate - 35x Servings - Mocha / 1 Bottle: 3.1% avg MAE (consistency: Low, n=5.0)\n",
      "Coffee Concentrate - Limited Release Flavors - Cold Brew Coffee, Perfect for Hot or Iced Instant Coffees, Hot Drinks, Cold Brew Concentrate, 35 Servings - Brownie Batter: 3.2% avg MAE (consistency: Low, n=11.0)\n",
      "[LIVE EXCLUSIVE] mocha + caramel + vanilla + hazelnut proteins: 3.4% avg MAE (consistency: Low, n=12.0)\n",
      "Instant Latte 02 - 12 packets: 3.6% avg MAE (consistency: Low, n=18.0)\n",
      "[Live Exclusive] 6x Brownie Batter: 3.6% avg MAE (consistency: Low, n=7.0)\n",
      "Javy Club - Subscribe & Save (Renewal) - Deal #4 - Original: 5.5% avg MAE (consistency: Low, n=16.0)\n",
      "[LIVE EXCLUSIVE] 6x Decaf: 6.1% avg MAE (consistency: Low, n=6.0)\n",
      "\n",
      "PREDICTABILITY DRIVERS ANALYSIS:\n",
      "-----------------------------------\n",
      "Correlation with Predictability (negative = more predictable):\n",
      "  n_days: -0.212 (Moderate - Lower = Less Predictable)\n",
      "  total_demand: -0.179 (Moderate - Lower = Less Predictable)\n",
      "  avg_demand: -0.151 (Moderate - Lower = Less Predictable)\n",
      "  demand_volatility: -0.063 (Weak - Lower = Less Predictable)\n",
      "  pct_change_volatility: 0.652 (Strong - Higher = Less Predictable)\n",
      "\n",
      "PREDICTABILITY BY SEGMENTS:\n",
      "----------------------------\n",
      "High Volume SKUs (top 20%): 0.6% avg MAE\n",
      "Low Volume SKUs (bottom 20%): 0.8% avg MAE\n",
      "High Volatility SKUs: 1.8% avg MAE\n",
      "Low Volatility SKUs: nan% avg MAE\n",
      "\n",
      "UNIVERSAL PREDICTION DEGRADATION ANALYSIS\n",
      "=============================================\n",
      "CONSENSUS DEGRADATION PATTERN:\n",
      "--------------------------------\n",
      "0-7 days: 0.9% MAE (+0.0% vs baseline) [5/5 models, High agreement]\n",
      "8-14 days: 0.8% MAE (-3.4% vs baseline) [5/5 models, High agreement]\n",
      "15-30 days: 0.9% MAE (+2.9% vs baseline) [5/5 models, High agreement]\n",
      "31-60 days: 0.9% MAE (+0.9% vs baseline) [5/5 models, High agreement]\n",
      "61-90 days: 0.9% MAE (+1.8% vs baseline) [5/5 models, High agreement]\n",
      "90+ days: 0.9% MAE (-0.9% vs baseline) [5/5 models, High agreement]\n",
      "\n",
      "CONSENSUS DEGRADATION RATE: +2.9% over first month\n",
      "  Recommendation: QUARTERLY retraining (very stable)\n",
      "\n",
      "SEASONAL PATTERN CAPTURE ANALYSIS\n",
      "=====================================\n",
      "ACTUAL SEASONAL PATTERNS:\n",
      "---------------------------\n",
      "Jan: +40.9% avg change (1785.0 obs)\n",
      "Feb: +43.3% avg change (3903.0 obs)\n",
      "Mar: +43.8% avg change (4310.0 obs)\n",
      "Apr: +31.1% avg change (3776.0 obs)\n",
      "May: +28.0% avg change (3717.0 obs)\n",
      "Jun: +26.7% avg change (1947.0 obs)\n",
      "\n",
      "SEASONAL PATTERN CAPTURE BY MODEL:\n",
      "-----------------------------------\n",
      "1. Linear Regression: 1.000 correlation, 0.0% seasonal MAE (Excellent)\n",
      "2. Ridge Regression: 1.000 correlation, 0.0% seasonal MAE (Excellent)\n",
      "3. Random Forest: 1.000 correlation, 0.0% seasonal MAE (Excellent)\n",
      "4. Gradient Boosting: 1.000 correlation, 0.0% seasonal MAE (Excellent)\n",
      "5. Elastic Net: 1.000 correlation, 0.5% seasonal MAE (Excellent)\n",
      "\n",
      "CONSENSUS SEASONAL CAPTURE: 1.000 correlation\n",
      "  Assessment: STRONG seasonal pattern capture\n",
      "\n",
      "Q4 (HOLIDAY SEASON) PERFORMANCE:\n",
      "---------------------------------\n",
      "\n",
      "KEY INSIGHTS SUMMARY:\n",
      "----------------------\n",
      "1. SKU PREDICTABILITY:\n",
      "   - Best performing SKUs average: 0.1% MAE\n",
      "   - Worst performing SKUs average: 4.4% MAE\n",
      "   - Predictability range: 4.4% points\n",
      "\n",
      "2. PREDICTION DEGRADATION:\n",
      "   - First week performance: 0.9% MAE\n",
      "   - One month performance: 0.9% MAE\n",
      "   - Degradation rate: +2.9% over 30 days\n",
      "\n",
      "3. SEASONAL PATTERN CAPTURE:\n",
      "   - Average seasonal correlation: 1.000\n",
      "   - Best seasonal model: Linear Regression\n",
      "\n",
      "4. MODEL CONSENSUS:\n",
      "   - MAE range across models: 4.3% points\n",
      "   - Model consensus level: MODERATE - Some model disagreement\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ComprehensiveModelAnalysis:\n",
    "    \"\"\"Analyze performance patterns across all models to identify universal insights\"\"\"\n",
    "    \n",
    "    def __init__(self, test_data, model_results, target_col='demand_pct_change'):\n",
    "        self.test_data = test_data.copy()\n",
    "        self.model_results = model_results\n",
    "        self.target_col = target_col\n",
    "        self.model_names = list(model_results.keys())\n",
    "        \n",
    "        # Add all model predictions to test data\n",
    "        for model_name, results in model_results.items():\n",
    "            if 'predictions' in results and len(results['predictions']) == len(test_data):\n",
    "                self.test_data[f'{model_name}_pred'] = results['predictions']\n",
    "                self.test_data[f'{model_name}_error'] = self.test_data[target_col] - results['predictions']\n",
    "                self.test_data[f'{model_name}_abs_error'] = np.abs(self.test_data[f'{model_name}_error'])\n",
    "    \n",
    "    def analyze_universal_sku_predictability(self):\n",
    "        \"\"\"Identify SKUs that are consistently predictable/unpredictable across ALL models\"\"\"\n",
    "        print(\"UNIVERSAL SKU PREDICTABILITY ANALYSIS\")\n",
    "        print(\"=\"*45)\n",
    "        \n",
    "        # Calculate MAE for each SKU across all models\n",
    "        sku_performance_all_models = {}\n",
    "        \n",
    "        for sku in self.test_data['sku'].unique():\n",
    "            sku_data = self.test_data[self.test_data['sku'] == sku]\n",
    "            \n",
    "            if len(sku_data) < 5:  # Need minimum observations\n",
    "                continue\n",
    "                \n",
    "            sku_performance = {}\n",
    "            for model_name in self.model_names:\n",
    "                if f'{model_name}_abs_error' in sku_data.columns:\n",
    "                    sku_performance[model_name] = sku_data[f'{model_name}_abs_error'].mean()\n",
    "            \n",
    "            if len(sku_performance) >= 3:  # Need at least 3 models\n",
    "                sku_performance_all_models[sku] = sku_performance\n",
    "        \n",
    "        # Create SKU performance matrix\n",
    "        sku_performance_df = pd.DataFrame(sku_performance_all_models).T\n",
    "        sku_performance_df['mean_mae'] = sku_performance_df.mean(axis=1)\n",
    "        sku_performance_df['mae_std'] = sku_performance_df.std(axis=1)\n",
    "        sku_performance_df['n_observations'] = [\n",
    "            len(self.test_data[self.test_data['sku'] == sku]) for sku in sku_performance_df.index\n",
    "        ]\n",
    "        \n",
    "        # Sort by mean performance across all models\n",
    "        sku_performance_df = sku_performance_df.sort_values('mean_mae')\n",
    "        \n",
    "        print(f\"Analyzed {len(sku_performance_df)} SKUs across {len(self.model_names)} models\")\n",
    "        print()\n",
    "        \n",
    "        print(\"TOP 10 UNIVERSALLY PREDICTABLE SKUs:\")\n",
    "        print(\"-\" * 40)\n",
    "        for sku, row in sku_performance_df.head(10).iterrows():\n",
    "            consistency = \"High\" if row['mae_std'] < 2 else \"Medium\" if row['mae_std'] < 5 else \"Low\"\n",
    "            print(f\"{sku}: {row['mean_mae']:.1f}% avg MAE (consistency: {consistency}, n={row['n_observations']})\")\n",
    "        \n",
    "        print(\"\\nTOP 10 UNIVERSALLY UNPREDICTABLE SKUs:\")\n",
    "        print(\"-\" * 42)\n",
    "        for sku, row in sku_performance_df.tail(10).iterrows():\n",
    "            consistency = \"High\" if row['mae_std'] < 2 else \"Medium\" if row['mae_std'] < 5 else \"Low\"\n",
    "            print(f\"{sku}: {row['mean_mae']:.1f}% avg MAE (consistency: {consistency}, n={row['n_observations']})\")\n",
    "        \n",
    "        # Analyze what makes SKUs predictable\n",
    "        self._analyze_predictability_drivers(sku_performance_df)\n",
    "        \n",
    "        return sku_performance_df\n",
    "    \n",
    "    def _analyze_predictability_drivers(self, sku_performance_df):\n",
    "        \"\"\"Analyze what characteristics drive SKU predictability\"\"\"\n",
    "        print(\"\\nPREDICTABILITY DRIVERS ANALYSIS:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Merge with SKU characteristics\n",
    "        sku_chars = self.test_data.groupby('sku').agg({\n",
    "            'actual_demand': ['mean', 'std', 'sum'],\n",
    "            self.target_col: ['mean', 'std'],\n",
    "            'order_date': 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        sku_chars.columns = ['avg_demand', 'demand_volatility', 'total_demand', \n",
    "                            'avg_pct_change', 'pct_change_volatility', 'n_days']\n",
    "        \n",
    "        # Merge performance with characteristics\n",
    "        analysis_df = sku_performance_df.merge(sku_chars, left_index=True, right_index=True, how='left')\n",
    "        \n",
    "        # Correlation analysis\n",
    "        correlations = analysis_df[['mean_mae', 'avg_demand', 'demand_volatility', \n",
    "                                   'total_demand', 'pct_change_volatility', 'n_days']].corr()['mean_mae'].drop('mean_mae')\n",
    "        \n",
    "        print(\"Correlation with Predictability (negative = more predictable):\")\n",
    "        for factor, corr in correlations.sort_values().items():\n",
    "            direction = \"Higher\" if corr > 0 else \"Lower\"\n",
    "            strength = \"Strong\" if abs(corr) > 0.3 else \"Moderate\" if abs(corr) > 0.1 else \"Weak\"\n",
    "            print(f\"  {factor}: {corr:.3f} ({strength} - {direction} = Less Predictable)\")\n",
    "        \n",
    "        # Segment analysis\n",
    "        print(\"\\nPREDICTABILITY BY SEGMENTS:\")\n",
    "        print(\"-\" * 28)\n",
    "        \n",
    "        # High vs Low volume\n",
    "        high_volume = analysis_df[analysis_df['total_demand'] > analysis_df['total_demand'].quantile(0.8)]\n",
    "        low_volume = analysis_df[analysis_df['total_demand'] < analysis_df['total_demand'].quantile(0.2)]\n",
    "        \n",
    "        print(f\"High Volume SKUs (top 20%): {high_volume['mean_mae'].mean():.1f}% avg MAE\")\n",
    "        print(f\"Low Volume SKUs (bottom 20%): {low_volume['mean_mae'].mean():.1f}% avg MAE\")\n",
    "        \n",
    "        # High vs Low volatility\n",
    "        high_volatility = analysis_df[analysis_df['pct_change_volatility'] > analysis_df['pct_change_volatility'].quantile(0.8)]\n",
    "        low_volatility = analysis_df[analysis_df['pct_change_volatility'] < analysis_df['pct_change_volatility'].quantile(0.2)]\n",
    "        \n",
    "        print(f\"High Volatility SKUs: {high_volatility['mean_mae'].mean():.1f}% avg MAE\")\n",
    "        print(f\"Low Volatility SKUs: {low_volatility['mean_mae'].mean():.1f}% avg MAE\")\n",
    "    \n",
    "    def analyze_universal_prediction_degradation(self):\n",
    "        \"\"\"Analyze when prediction accuracy starts degrading across ALL models\"\"\"\n",
    "        print(\"\\nUNIVERSAL PREDICTION DEGRADATION ANALYSIS\")\n",
    "        print(\"=\"*45)\n",
    "        \n",
    "        # Sort by date and calculate prediction horizons\n",
    "        test_sorted = self.test_data.sort_values('order_date').copy()\n",
    "        training_end = test_sorted['order_date'].min()\n",
    "        test_sorted['days_from_training'] = (test_sorted['order_date'] - training_end).dt.days\n",
    "        \n",
    "        # Define time bins\n",
    "        bins = [0, 7, 14, 30, 60, 90, float('inf')]\n",
    "        labels = ['0-7 days', '8-14 days', '15-30 days', '31-60 days', '61-90 days', '90+ days']\n",
    "        test_sorted['time_bin'] = pd.cut(test_sorted['days_from_training'], bins=bins, labels=labels, right=False)\n",
    "        \n",
    "        # Calculate performance for each model across time bins\n",
    "        degradation_results = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            error_col = f'{model_name}_abs_error'\n",
    "            if error_col in test_sorted.columns:\n",
    "                model_degradation = test_sorted.groupby('time_bin')[error_col].agg(['mean', 'count']).reset_index()\n",
    "                model_degradation.columns = ['time_bin', 'mae', 'n_predictions']\n",
    "                degradation_results[model_name] = model_degradation\n",
    "        \n",
    "        # Calculate consensus degradation pattern\n",
    "        consensus_degradation = []\n",
    "        \n",
    "        for i, time_bin in enumerate(labels):\n",
    "            all_model_maes = []\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for model_name, degradation_df in degradation_results.items():\n",
    "                if i < len(degradation_df):\n",
    "                    mae_value = degradation_df.iloc[i]['mae']\n",
    "                    n_pred = degradation_df.iloc[i]['n_predictions']\n",
    "                    if not pd.isna(mae_value) and n_pred > 10:  # Minimum sample size\n",
    "                        all_model_maes.append(mae_value)\n",
    "                        total_predictions += n_pred\n",
    "            \n",
    "            if all_model_maes:\n",
    "                consensus_mae = np.mean(all_model_maes)\n",
    "                consensus_std = np.std(all_model_maes)\n",
    "                consensus_degradation.append({\n",
    "                    'time_bin': time_bin,\n",
    "                    'consensus_mae': consensus_mae,\n",
    "                    'mae_std_across_models': consensus_std,\n",
    "                    'n_models_agreeing': len(all_model_maes),\n",
    "                    'total_predictions': total_predictions\n",
    "                })\n",
    "        \n",
    "        consensus_df = pd.DataFrame(consensus_degradation)\n",
    "        \n",
    "        print(\"CONSENSUS DEGRADATION PATTERN:\")\n",
    "        print(\"-\" * 32)\n",
    "        \n",
    "        if len(consensus_df) > 0:\n",
    "            baseline_mae = consensus_df.iloc[0]['consensus_mae']\n",
    "            \n",
    "            for _, row in consensus_df.iterrows():\n",
    "                degradation_pct = ((row['consensus_mae'] - baseline_mae) / baseline_mae) * 100\n",
    "                model_agreement = row['n_models_agreeing']\n",
    "                \n",
    "                agreement_level = \"High\" if model_agreement >= len(self.model_names) * 0.8 else \"Medium\" if model_agreement >= len(self.model_names) * 0.6 else \"Low\"\n",
    "                \n",
    "                print(f\"{row['time_bin']}: {row['consensus_mae']:.1f}% MAE \"\n",
    "                      f\"({degradation_pct:+.1f}% vs baseline) \"\n",
    "                      f\"[{model_agreement}/{len(self.model_names)} models, {agreement_level} agreement]\")\n",
    "            \n",
    "            # Calculate degradation rate\n",
    "            if len(consensus_df) >= 3:\n",
    "                early_performance = consensus_df.iloc[0]['consensus_mae']\n",
    "                month_performance = consensus_df.iloc[2]['consensus_mae']  # 15-30 days\n",
    "                degradation_rate = ((month_performance - early_performance) / early_performance) * 100\n",
    "                \n",
    "                print(f\"\\nCONSENSUS DEGRADATION RATE: {degradation_rate:+.1f}% over first month\")\n",
    "                \n",
    "                if degradation_rate > 15:\n",
    "                    print(\"  Recommendation: WEEKLY retraining (high degradation)\")\n",
    "                elif degradation_rate > 8:\n",
    "                    print(\"  Recommendation: BI-WEEKLY retraining (moderate degradation)\")\n",
    "                elif degradation_rate > 3:\n",
    "                    print(\"  Recommendation: MONTHLY retraining (low degradation)\")\n",
    "                else:\n",
    "                    print(\"  Recommendation: QUARTERLY retraining (very stable)\")\n",
    "        \n",
    "        return consensus_df\n",
    "    \n",
    "    def analyze_seasonal_pattern_capture(self):\n",
    "        \"\"\"Analyze how well models capture seasonal patterns across the board\"\"\"\n",
    "        print(\"\\nSEASONAL PATTERN CAPTURE ANALYSIS\")\n",
    "        print(\"=\"*37)\n",
    "        \n",
    "        # Add time features\n",
    "        test_data = self.test_data.copy()\n",
    "        test_data['month'] = test_data['order_date'].dt.month\n",
    "        test_data['quarter'] = test_data['order_date'].dt.quarter\n",
    "        test_data['day_of_week'] = test_data['order_date'].dt.dayofweek\n",
    "        \n",
    "        # Analyze actual seasonal patterns in target variable\n",
    "        actual_seasonality = test_data.groupby('month')[self.target_col].agg(['mean', 'count']).reset_index()\n",
    "        actual_seasonality.columns = ['month', 'actual_avg_change', 'n_observations']\n",
    "        actual_seasonality = actual_seasonality[actual_seasonality['n_observations'] >= 10]\n",
    "        \n",
    "        print(\"ACTUAL SEASONAL PATTERNS:\")\n",
    "        print(\"-\" * 27)\n",
    "        month_names = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \n",
    "                      7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "        \n",
    "        for _, row in actual_seasonality.iterrows():\n",
    "            month_name = month_names.get(row['month'], str(row['month']))\n",
    "            print(f\"{month_name}: {row['actual_avg_change']:+.1f}% avg change ({row['n_observations']} obs)\")\n",
    "        \n",
    "        # Analyze how well each model predicts these seasonal patterns\n",
    "        seasonal_capture_results = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            pred_col = f'{model_name}_pred'\n",
    "            if pred_col in test_data.columns:\n",
    "                predicted_seasonality = test_data.groupby('month')[pred_col].mean().reset_index()\n",
    "                predicted_seasonality.columns = ['month', 'predicted_avg_change']\n",
    "                \n",
    "                # Merge with actual seasonality\n",
    "                seasonal_comparison = actual_seasonality.merge(predicted_seasonality, on='month', how='inner')\n",
    "                \n",
    "                if len(seasonal_comparison) >= 6:  # Need at least 6 months\n",
    "                    # Calculate correlation between actual and predicted seasonal patterns\n",
    "                    seasonal_correlation = seasonal_comparison['actual_avg_change'].corr(\n",
    "                        seasonal_comparison['predicted_avg_change']\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate MAE of seasonal pattern prediction\n",
    "                    seasonal_mae = np.mean(np.abs(\n",
    "                        seasonal_comparison['actual_avg_change'] - seasonal_comparison['predicted_avg_change']\n",
    "                    ))\n",
    "                    \n",
    "                    seasonal_capture_results[model_name] = {\n",
    "                        'seasonal_correlation': seasonal_correlation,\n",
    "                        'seasonal_mae': seasonal_mae,\n",
    "                        'comparison_df': seasonal_comparison\n",
    "                    }\n",
    "        \n",
    "        print(f\"\\nSEASONAL PATTERN CAPTURE BY MODEL:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Sort models by seasonal capture ability\n",
    "        seasonal_ranking = sorted(seasonal_capture_results.items(), \n",
    "                                key=lambda x: x[1]['seasonal_correlation'], reverse=True)\n",
    "        \n",
    "        for rank, (model_name, metrics) in enumerate(seasonal_ranking, 1):\n",
    "            corr = metrics['seasonal_correlation']\n",
    "            mae = metrics['seasonal_mae']\n",
    "            \n",
    "            capture_quality = \"Excellent\" if corr > 0.8 else \"Good\" if corr > 0.6 else \"Fair\" if corr > 0.4 else \"Poor\"\n",
    "            \n",
    "            print(f\"{rank}. {model_name}: {corr:.3f} correlation, {mae:.1f}% seasonal MAE ({capture_quality})\")\n",
    "        \n",
    "        # Overall seasonal capture consensus\n",
    "        if seasonal_ranking:\n",
    "            avg_seasonal_corr = np.mean([metrics['seasonal_correlation'] for _, metrics in seasonal_ranking])\n",
    "            print(f\"\\nCONSENSUS SEASONAL CAPTURE: {avg_seasonal_corr:.3f} correlation\")\n",
    "            \n",
    "            if avg_seasonal_corr > 0.7:\n",
    "                print(\"  Assessment: STRONG seasonal pattern capture\")\n",
    "            elif avg_seasonal_corr > 0.5:\n",
    "                print(\"  Assessment: MODERATE seasonal pattern capture\")\n",
    "            elif avg_seasonal_corr > 0.3:\n",
    "                print(\"  Assessment: WEAK seasonal pattern capture\")\n",
    "            else:\n",
    "                print(\"  Assessment: POOR seasonal pattern capture - review seasonal features\")\n",
    "        \n",
    "        # Analyze Q4 vs non-Q4 performance specifically\n",
    "        self._analyze_q4_performance()\n",
    "        \n",
    "        return seasonal_capture_results\n",
    "    \n",
    "    def _analyze_q4_performance(self):\n",
    "        \"\"\"Analyze Q4 (holiday season) performance specifically\"\"\"\n",
    "        print(f\"\\nQ4 (HOLIDAY SEASON) PERFORMANCE:\")\n",
    "        print(\"-\" * 33)\n",
    "        \n",
    "        test_data = self.test_data.copy()\n",
    "        test_data['is_q4'] = test_data['order_date'].dt.quarter == 4\n",
    "        \n",
    "        q4_performance = {}\n",
    "        non_q4_performance = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            error_col = f'{model_name}_abs_error'\n",
    "            if error_col in test_data.columns:\n",
    "                q4_mae = test_data[test_data['is_q4']][error_col].mean()\n",
    "                non_q4_mae = test_data[~test_data['is_q4']][error_col].mean()\n",
    "                \n",
    "                if not pd.isna(q4_mae) and not pd.isna(non_q4_mae):\n",
    "                    q4_performance[model_name] = q4_mae\n",
    "                    non_q4_performance[model_name] = non_q4_mae\n",
    "        \n",
    "        if q4_performance and non_q4_performance:\n",
    "            print(\"Q4 vs Non-Q4 Performance:\")\n",
    "            \n",
    "            q4_models = []\n",
    "            for model_name in q4_performance.keys():\n",
    "                q4_mae = q4_performance[model_name]\n",
    "                non_q4_mae = non_q4_performance[model_name]\n",
    "                q4_difference = ((q4_mae - non_q4_mae) / non_q4_mae) * 100\n",
    "                \n",
    "                q4_models.append((model_name, q4_mae, non_q4_mae, q4_difference))\n",
    "                \n",
    "            # Sort by Q4 performance\n",
    "            q4_models.sort(key=lambda x: x[1])  # Sort by Q4 MAE\n",
    "            \n",
    "            for model_name, q4_mae, non_q4_mae, q4_diff in q4_models:\n",
    "                performance_change = \"worse\" if q4_diff > 0 else \"better\"\n",
    "                print(f\"  {model_name}: Q4 {q4_mae:.1f}%, Non-Q4 {non_q4_mae:.1f}% \"\n",
    "                      f\"({q4_diff:+.1f}% {performance_change} in Q4)\")\n",
    "            \n",
    "            # Overall Q4 assessment\n",
    "            avg_q4_impact = np.mean([diff for _, _, _, diff in q4_models])\n",
    "            \n",
    "            print(f\"\\nAVERAGE Q4 IMPACT: {avg_q4_impact:+.1f}% change in accuracy\")\n",
    "            \n",
    "            if avg_q4_impact > 10:\n",
    "                print(\"  Q4 Assessment: SIGNIFICANT accuracy drop during holidays\")\n",
    "            elif avg_q4_impact > 5:\n",
    "                print(\"  Q4 Assessment: MODERATE accuracy drop during holidays\")\n",
    "            elif avg_q4_impact > -5:\n",
    "                print(\"  Q4 Assessment: STABLE performance during holidays\")\n",
    "            else:\n",
    "                print(\"  Q4 Assessment: IMPROVED performance during holidays\")\n",
    "    \n",
    "    def generate_comprehensive_insights(self):\n",
    "        \"\"\"Generate overall insights across all analyses\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPREHENSIVE CROSS-MODEL INSIGHTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Run all analyses\n",
    "        sku_performance = self.analyze_universal_sku_predictability()\n",
    "        degradation_analysis = self.analyze_universal_prediction_degradation()\n",
    "        seasonal_analysis = self.analyze_seasonal_pattern_capture()\n",
    "        \n",
    "        print(\"\\nKEY INSIGHTS SUMMARY:\")\n",
    "        print(\"-\" * 22)\n",
    "        \n",
    "        # SKU Insights\n",
    "        best_skus = sku_performance.head(5)\n",
    "        worst_skus = sku_performance.tail(5)\n",
    "        \n",
    "        print(f\"1. SKU PREDICTABILITY:\")\n",
    "        print(f\"   - Best performing SKUs average: {best_skus['mean_mae'].mean():.1f}% MAE\")\n",
    "        print(f\"   - Worst performing SKUs average: {worst_skus['mean_mae'].mean():.1f}% MAE\")\n",
    "        print(f\"   - Predictability range: {worst_skus['mean_mae'].mean() - best_skus['mean_mae'].mean():.1f}% points\")\n",
    "        \n",
    "        # Degradation Insights\n",
    "        if len(degradation_analysis) >= 3:\n",
    "            early_performance = degradation_analysis.iloc[0]['consensus_mae']\n",
    "            month_performance = degradation_analysis.iloc[2]['consensus_mae']\n",
    "            degradation = ((month_performance - early_performance) / early_performance) * 100\n",
    "            \n",
    "            print(f\"\\n2. PREDICTION DEGRADATION:\")\n",
    "            print(f\"   - First week performance: {early_performance:.1f}% MAE\")\n",
    "            print(f\"   - One month performance: {month_performance:.1f}% MAE\")\n",
    "            print(f\"   - Degradation rate: {degradation:+.1f}% over 30 days\")\n",
    "        \n",
    "        # Seasonal Insights\n",
    "        if seasonal_analysis:\n",
    "            seasonal_correlations = [metrics['seasonal_correlation'] for metrics in seasonal_analysis.values()]\n",
    "            avg_seasonal_capture = np.mean(seasonal_correlations)\n",
    "            \n",
    "            print(f\"\\n3. SEASONAL PATTERN CAPTURE:\")\n",
    "            print(f\"   - Average seasonal correlation: {avg_seasonal_capture:.3f}\")\n",
    "            print(f\"   - Best seasonal model: {max(seasonal_analysis.items(), key=lambda x: x[1]['seasonal_correlation'])[0]}\")\n",
    "        \n",
    "        # Model Consensus Insights\n",
    "        print(f\"\\n4. MODEL CONSENSUS:\")\n",
    "        all_model_maes = [results['mae'] for results in self.model_results.values()]\n",
    "        mae_range = max(all_model_maes) - min(all_model_maes)\n",
    "        \n",
    "        print(f\"   - MAE range across models: {mae_range:.1f}% points\")\n",
    "        \n",
    "        if mae_range < 2:\n",
    "            consensus_level = \"HIGH - Models largely agree\"\n",
    "        elif mae_range < 5:\n",
    "            consensus_level = \"MODERATE - Some model disagreement\"\n",
    "        else:\n",
    "            consensus_level = \"LOW - Significant model disagreement\"\n",
    "        \n",
    "        print(f\"   - Model consensus level: {consensus_level}\")\n",
    "        \n",
    "        return {\n",
    "            'sku_performance': sku_performance,\n",
    "            'degradation_analysis': degradation_analysis,\n",
    "            'seasonal_analysis': seasonal_analysis,\n",
    "            'summary_insights': {\n",
    "                'best_sku_mae': best_skus['mean_mae'].mean(),\n",
    "                'worst_sku_mae': worst_skus['mean_mae'].mean(),\n",
    "                'degradation_rate': degradation if 'degradation' in locals() else None,\n",
    "                'seasonal_capture': avg_seasonal_capture if 'avg_seasonal_capture' in locals() else None,\n",
    "                'model_consensus': mae_range\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def run_comprehensive_analysis(test_data, model_results, target_col='demand_pct_change'):\n",
    "    \"\"\"\n",
    "    Run comprehensive analysis across all models to identify universal patterns\n",
    "    \n",
    "    This will tell you:\n",
    "    - Which SKUs are predictable across ALL models\n",
    "    - When prediction accuracy universally starts degrading\n",
    "    - Whether seasonal patterns are being captured effectively\n",
    "    - Model consensus and disagreement patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = ComprehensiveModelAnalysis(test_data, model_results, target_col)\n",
    "    insights = analyzer.generate_comprehensive_insights()\n",
    "    \n",
    "    return analyzer, insights\n",
    "\n",
    "\n",
    "# Usage with your existing results:\n",
    "\n",
    "# After running your enhanced demand analysis:\n",
    "enhanced_model, model_results, test_data, analyses = run_enhanced_demand_analysis(\n",
    "    enhanced_benchmark_df,  # Your benchmark dataframe\n",
    "    amazon_order_items=amazon_order_item_metrics,\n",
    "    tiktok_order_items=tiktok__order_items,\n",
    "    shopify_order_items=shopify__order_items,\n",
    "    amazon_daily_sku=amazon_daily_sku_metrics,\n",
    "    tiktok_daily_sku=tiktok_daily_sku_metrics,\n",
    "    shopify_daily_sku=shopify_daily_sku_metrics\n",
    ")\n",
    "# Run comprehensive cross-model analysis:\n",
    "comprehensive_analyzer, insights = run_comprehensive_analysis(test_data, model_results)\n",
    "\n",
    "# Access specific insights:\n",
    "best_skus = insights['sku_performance'].head(10)  # Universally predictable SKUs\n",
    "degradation_pattern = insights['degradation_analysis']  # When all models degrade\n",
    "seasonal_capture = insights['seasonal_analysis']  # How well seasonality is captured\n",
    "summary = insights['summary_insights']  # Key takeaways\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45b349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
